<!DOCTYPE html>

<html lang="en"><head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Mind the Gap (Under the Curve)</title>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script async="" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<style>
        @import url('https://fonts.googleapis.com/css2?family=Playfair+Display:wght@400;700&family=Lato:wght@300;400;700&display=swap');
        
        :root {
            --primary: #2c3e50;
            --secondary: #8e44ad;
            --accent: #3498db;
            --light: #ecf0f1;
            --dark: #2c3e50;
            --text: #333;
            --border: #ddd;
            --formal-bg: #f8f9fa;
            --layman-bg: #fffaf5;
            --formal-border: #3498db;
            --layman-border: #ff9f43;
        }
        
        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }
        
        body {
            font-family: 'Lato', sans-serif;
            color: var(--text);
            line-height: 1.6;
            background-color: #f9f9f9;
            padding: 20px;
            counter-reset: section;
        }
        
        .magazine-container {
            max-width: 800px;
            margin: 0 auto;
            background-color: white;
            box-shadow: 0 0 30px rgba(0, 0, 0, 0.1);
            position: relative;
            padding: 40px 50px;
        }
        
        @media print {
            .magazine-container {
                box-shadow: none;
                padding: 0;
                max-width: 100%;
            }
            body {
                padding: 0;
                background: white;
            }
        }
        
        .header {
            text-align: center;
            padding: 30px 0;
            border-bottom: 2px solid var(--border);
            margin-bottom: 30px;
        }
        
        h1 {
            font-family: 'Playfair Display', serif;
            font-size: 2.8rem;
            color: var(--primary);
            margin-bottom: 10px;
            letter-spacing: 1px;
            font-weight: 700;
        }
        
        .subtitle {
            font-size: 1.4rem;
            color: var(--secondary);
            font-weight: 300;
            margin-bottom: 20px;
        }
        
        .author {
            font-style: italic;
            color: #777;
            margin-bottom: 20px;
        }
        
        .disclaimer {
            background-color: #f8f8f8;
            border-left: 4px solid #e74c3c;
            padding: 15px;
            margin: 20px 0;
            font-size: 0.9rem;
        }
        
        h2 {
            font-family: 'Playfair Display', serif;
            color: var(--primary);
            margin: 40px 0 20px;
            padding-bottom: 10px;
            border-bottom: 1px solid var(--border);
            counter-increment: section;
            position: relative;
        }
        
        h2:before {
            content: counter(section) ".";
            position: absolute;
            left: -30px;
            color: var(--secondary);
            font-weight: bold;
        }
        
        h3 {
            font-family: 'Playfair Display', serif;
            color: var(--secondary);
            margin: 25px 0 15px;
            font-size: 1.3rem;
        }
        
        p {
            margin-bottom: 15px;
            text-align: justify;
        }
        
        .formal {
            background-color: var(--formal-bg);
            padding: 15px;
            border-radius: 5px;
            margin: 15px 0;
            border-left: 3px solid var(--formal-border);
            font-family: 'Courier New', monospace;
        }
        
        .layman {
            background-color: var(--layman-bg);
            padding: 15px;
            border-radius: 5px;
            margin: 15px 0;
            border-left: 3px solid var(--layman-border);
			  display: block;              /* forces it onto its own line */
			  margin-top: 0.3em;            /* space above */
			  margin-bottom: 0.6em;         /* space below */
			  line-height: 1.4;             /* taller line box for wrapped lines */
			  font-style: italic;
			  color: #555;			
        }
        
        .equation {
            text-align: center;
            margin: 20px 0;
            padding: 10px;
            font-family: monospace;
            background-color: #f8f9fa;
            border-radius: 4px;
            overflow-x: auto;
        }
        
        /* ---- NEW STYLING FOR PSEUDOCODE ---- */
        .pseudocode {
            background: #2b2b2b; /* Dark background for contrast */
            color: #f8f8f2;      /* Light text color */
            border: 1px solid #444;
            border-radius: 8px;
            padding: 1.2em;
            margin: 1.5em 0;
            font-family: 'Fira Code', 'Monaco', 'Cascadia Code', 'Consolas', monospace;
            white-space: pre-wrap;
            overflow-x: auto;
            line-height: 1.5;
            counter-reset: line;
            box-shadow: 0 6px 16px rgba(0, 0, 0, 0.2);
        }
        .pseudocode .comment { color: #75715e; font-style: italic; } /* Comments are grey/italic */
        .pseudocode .keyword { color: #f92672; font-weight: bold; }   /* Keywords like 'function', 'if' are pink/bold */
        .pseudocode .function { color: #a6e22e; }                     /* Function names are green */
        .pseudocode .string { color: #e6db74; }                       /* Strings are yellow */
        .pseudocode .number { color: #ae81ff; }                       /* Numbers are purple */
        .pseudocode .operator { color: #f92672; }                     /* Operators are pink */

        /* Optional: Line numbers */
        .pseudocode.code-with-lines {
            padding-left: 3.5em;
        }
        .pseudocode.code-with-lines > div:before {
            counter-increment: line;
            content: counter(line);
            display: inline-block;
            width: 2.5em;
            margin-left: -3.5em;
            margin-right: 1em;
            color: #6c6783;
            text-align: right;
            user-select: none;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            font-size: 0.95rem;
        }
        
        th, td {
            padding: 12px 15px;
            text-align: left;
            border: 1px solid var(--border);
        }
        
        th {
            background-color: var(--primary);
            color: white;
            font-weight: bold;
        }
        
        tr:nth-child(even) {
            background-color: #f8f9fa;
        }
        
        ul, ol {
            margin-left: 30px;
            margin-bottom: 20px;
        }
        
        li {
            margin-bottom: 8px;
        }
        
        .key-point {
            background-color: rgba(52, 152, 219, 0.1);
            padding: 15px;
            border-radius: 5px;
            margin: 15px 0;
            border-left: 3px solid var(--accent);
        }
        
        .section-divider {
            height: 1px;
            background: linear-gradient(to right, transparent, var(--secondary), transparent);
            margin: 40px 0;
        }
        
        .toc {
            background-color: #f8f9fa;
            padding: 20px;
            border-radius: 5px;
            margin: 20px 0;
        }
        
        .toc ul {
            list-style-type: none;
            margin-left: 0;
        }
        
        .toc li {
            margin-bottom: 10px;
        }
        
        .toc a {
            text-decoration: none;
            color: var(--secondary);
            transition: color 0.3s;
        }
        
        .toc a:hover {
            color: var(--accent);
            text-decoration: underline;
        }
        
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid var(--border);
            font-size: 0.9rem;
            color: #777;
        }


          figure.inline-svg { margin: 1rem auto; max-width: var(--content-max); }
          figure.inline-svg svg { width: 100%; height: auto; display:block; }
          figure.inline-svg figcaption { font-size: .9rem; color:#555; margin-top:.35rem; }

        
        .svg-container {
            text-align: center;
            margin: 20px 0;
        }
        
        .svg-container svg {
            max-width: 100%;
            height: auto;
        }
        
        .caption {
            font-style: italic;
            text-align: center;
            margin-top: 5px;
            color: #777;
            font-size: 0.9rem;
        }
        
        @media (max-width: 768px) {
            .magazine-container {
                padding: 20px;
            }
            
            h1 {
                font-size: 2.2rem;
            }
            
            h2:before {
                left: -20px;
            }
        }
		
		.integral-table {
		  width: 100%;
		  border-collapse: collapse;
		  margin: 25px 0;
		  font-size: 0.95rem;
		  background: white;
		  box-shadow: 0 4px 20px rgba(0,0,0,0.05);
		}

		.integral-table thead tr {
		  background: linear-gradient(90deg, #3498db, #8e44ad);
		  color: #fff;
		  text-align: left;
		}

		.integral-table th,
		.integral-table td {
		  padding: 12px 15px;
		  border: 1px solid #ddd;
		  vertical-align: top;
		}

		.integral-table tbody tr:nth-child(even) {
		  background-color: #f9f9f9;
		}

		.integral-table tbody tr:hover {
		  background-color: #f1f8ff;
		}

		.integral-table td:nth-child(1) {
		  font-weight: bold;
		  text-align: center;
		  color: #8e44ad;
		}

		.integral-table td:nth-child(3) {
		  font-family: 'Courier New', monospace;
		  background-color: #f8f9fa;
		  border-radius: 4px;
		}

		@media screen and (max-width: 768px) {
		  .integral-table thead {
			display: none;
		  }
		  .integral-table, .integral-table tbody, .integral-table tr, .integral-table td {
			display: block;
			width: 100%;
		  }
		  .integral-table tr {
			margin-bottom: 15px;
			border-bottom: 2px solid #eee;
		  }
		  .integral-table td {
			text-align: right;
			padding-left: 50%;
			position: relative;
		  }
		  .integral-table td::before {
			content: attr(data-label);
			position: absolute;
			left: 15px;
			width: 45%;
			padding-right: 10px;
			font-weight: bold;
			text-align: left;
			color: #555;
		  }
              .print-footer { display: none; }
		}

		

          /* Chapter page breaks (print only) */
          .chapter { /* a wrapper you can add to each top-level section */
            page-break-before: always;
            break-before: page;
          }
          .chapter:first-of-type { page-break-before: auto; break-before: auto; }
        
        
		@media print {
		  * {
			-webkit-print-color-adjust: exact !important; /* Chrome, Edge, Safari */
			color-adjust: exact !important;              /* Firefox */
			print-color-adjust: exact !important;        /* Modern spec */
		  }
		  body {
			background: white;
			font-size: 11pt;
		  }
		  .integral-table {
			width: 100%;
			max-width: 100%;
			border: 1pt solid #000;
			border-collapse: collapse;
			page-break-inside: avoid;
		  }
		  .integral-table td {
			padding: 4pt 6pt;
			border: 0.5pt solid #000;
		  }
          


		}
		
    </style>
<style id="print-a4-fixes">
  @page { size: A4; margin: 12mm; }
  @media print {
    html, body { -webkit-print-color-adjust: exact; print-color-adjust: exact; }
    .magazine-container { width: auto !important; margin: 0 !important; padding: 0 !important; }
    figure, .figure, svg { page-break-inside: avoid !important; break-inside: avoid !important; overflow: visible !important; }
    .print-footer {
  position: fixed; bottom: 0; left: 0; right: 0; text-align: center;
  font-size: 10pt; color: #666;
    }
    .print-footer::after {
      /* Most browsers support counter(page) when printing */
      content: counter(page) " of Gene Boo - Mind the Gap!";
}

    svg { max-width: 100% !important; height: auto !important; display: block !important; }
  }
</style>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script><script src="https://cdn.jsdelivr.net/npm/highlight.js/lib/common/highlight.min.js"></script><link href="https://cdn.jsdelivr.net/npm/highlight.js/styles/default.min.css" rel="stylesheet"/><script>hljs.highlightAll();</script>
<style>
body {
  margin: 0;
  font-family: 'Segoe UI', sans-serif;
  scroll-behavior: smooth;
}
#sidebar {
  position: fixed;
  top: 0;
  left: -250px;
  width: 250px;
  height: 100%;
  background: linear-gradient(to bottom right, #0f2027, #203a43, #2c5364);
  color: white;
  overflow-y: auto;
  transition: left 0.3s ease;
  z-index: 1000;
}
#sidebar-indicator {
  position: fixed;
  top: 50%;
  left: 0;
  width: 20px;
  height: 60px;
  background: #00ffff;
  color: black;
  font-size: 24px;
  text-align: center;
  line-height: 60px;
  cursor: pointer;
  z-index: 1001;
}
#sidebar-content {
  padding: 20px;
}
.nav-link {
  display: block;
  color: white;
  text-decoration: none;
  margin: 8px 0;
}
.nav-link:hover {
  text-decoration: underline;
}
.nav-link.h3 {
  margin-left: 20px;
  font-size: 0.9em;
}
</style>
<script>
document.addEventListener("DOMContentLoaded", function() {
  const sidebar = document.getElementById("sidebar");
  const indicator = document.getElementById("sidebar-indicator");

  document.body.addEventListener("mousemove", function(e) {
    if (e.clientX < 30) {
      sidebar.style.left = "0";
    } else if (e.clientX > 250) {
      sidebar.style.left = "-250px";
    }
  });

  indicator.addEventListener("click", function() {
    sidebar.style.left = sidebar.style.left === "0px" ? "-250px" : "0";
  });
});
</script>
</head>
<body>
<div id="sidebar">
<div id="sidebar-indicator">☰</div>
<div id="sidebar-content">
<a class="nav-link h2" href="#section-1">Introduction</a>
<a class="nav-link h2" href="#section-2">Fundamental Quadrature Methods</a>
<a class="nav-link h3" href="#section-2-1">Trapezoid Rule</a>
<a class="nav-link h3" href="#section-2-2">Simpson's Rule</a>
<a class="nav-link h3" href="#section-2-3">Romberg Integration</a>
<a class="nav-link h2" href="#section-3">Advanced Quadrature Techniques</a>
<a class="nav-link h3" href="#section-3-1">Gaussian Quadrature</a>
<a class="nav-link h3" href="#section-3-2">Adaptive Quadrature</a>
<a class="nav-link h3" href="#section-3-3">Clenshaw–Curtis and Tanh–Sinh Quadrature</a>
<a class="nav-link h3" href="#section-3-4">Filon and Levin Quadrature for Oscillatory Integrals</a>
<a class="nav-link h2" href="#section-4">Building Custom Quadrature Rules</a>
<a class="nav-link h3" href="#section-4-1">Generalized Gaussian Quadrature (Golub–Welsch)</a>
<a class="nav-link h3" href="#section-4-2">Generalized (Non‑Gaussian) Quadrature Creation</a>
<a class="nav-link h2" href="#section-5">Modern Quadrature Architectures</a>
<a class="nav-link h2" href="#section-6">Past infinity, past certainty — into the (purely conceptual) quantum fold of Quadrature</a>
<a class="nav-link h2" href="#section-7">Option Pricing with Quadrature Methods</a>
<a class="nav-link h3" href="#section-7-1">Black-Scholes Model</a>
<a class="nav-link h3" href="#section-7-2">Gauss-Hermite for Option Pricing</a>
<a class="nav-link h3" href="#section-7-3">Strike-Aware Formulation</a>
<a class="nav-link h3" href="#section-7-4">Carr-Madan Quadrature Formulation</a>
<a class="nav-link h3" href="#section-7-5">Stop-Loss Premium Formulation</a>
<a class="nav-link h3" href="#section-7-6">Practical guidance and examples</a>
<a class="nav-link h2" href="#section-8">Beyond Black-Scholes &amp; Finance</a>
<a class="nav-link h3" href="#section-8-1">Multidimensional Integrals</a>
<a class="nav-link h3" href="#section-8-2">Integrals over a Circle and Arbitrary Shapes</a>
<a class="nav-link h2" href="#section-9">Quadrature in the Wild: From Physics to Finance</a>
<a class="nav-link h3" href="#section-9-1">Physics: The Path Integral</a>
<a class="nav-link h3" href="#section-9-2">Chemistry: The Electronic Structure Problem</a>
<a class="nav-link h3" href="#section-9-3">Biology: Pharmacokinetics</a>
<a class="nav-link h3" href="#section-9-4">Maps &amp; Geography: Geospatial Quadrature</a>
<a class="nav-link h3" href="#section-9-5">DSP: Audio</a>
<a class="nav-link h3" href="#section-9-6">Graphics: Rendering</a>
<a class="nav-link h3" href="#section-9-7">Graphics: Quadrature for Animation and Quaternions</a>
<a class="nav-link h3" href="#section-9-8">Video Games: Physics</a>
<a class="nav-link h3" href="#section-9-9">Real‑Estate: Architecture</a>
<a class="nav-link h3" href="#section-9-10">Shipbuilding: Naval</a>
<a class="nav-link h3" href="#section-9-11">Navigation / Logistics</a>
<a class="nav-link h3" href="#section-9-12">Remote Sensing: Crude Oil Detection</a>
<a class="nav-link h3" href="#section-9-13">Crystal Science: Diffraction</a>
<a class="nav-link h3" href="#section-9-14">Lasers: Mode Overlap</a>
<a class="nav-link h3" href="#section-9-15">Astrophysics</a>
<a class="nav-link h3" href="#section-9-16">Finance: Local Volatility and Beyond</a>
<a class="nav-link h2" href="#section-10">FFT: The Quadrature in Disguise</a>
<a class="nav-link h3" href="#section-10-1">Chebyshev Nodes and Clenshaw-Curtis</a>
<a class="nav-link h3" href="#section-10-2">FFT as a Quadrature Rule</a>
<a class="nav-link h3" href="#section-10-3">Convolution: The Integral that FFT Loves</a>
<a class="nav-link h3" href="#section-10-4">Quadrature for Convolution</a>
<a class="nav-link h2" href="#section-11">Time is Money: Quadrature for Bermudan Options</a>
<a class="nav-link h3" href="#section-11-1">The Dynamic Programming Equation</a>
<a class="nav-link h3" href="#section-11-2">Quadrature Methods</a>
<a class="nav-link h3" href="#section-11-3">FFT Acceleration</a>
<a class="nav-link h2" href="#section-12">The Grand Reduction: Taming a High-Dimensional Integral</a>
<a class="nav-link h3" href="#section-12-1">The Problem</a>
<a class="nav-link h3" href="#section-12-2">Step 1: Change of Variables - Cholesky Decomposition</a>
<a class="nav-link h3" href="#section-12-3">Step 2: Dimension Reduction via Conditioning</a>
<a class="nav-link h3" href="#section-12-4">Step 3: PCA Reduction</a>
<a class="nav-link h3" href="#section-12-5">Step 4: Numerical Integration</a>
<a class="nav-link h3" href="#section-12-6">Step 5: Validation with Monte Carlo</a>
<a class="nav-link h3" href="#section-12-7">Step 6: Compare Results</a>
<a class="nav-link h2" href="#section-13">Conclusion</a>
<a class="nav-link h3" href="#section-13-1">References &amp; Further Reading</a>
</div>
</div>

<div class="magazine-container">
<div class="header">
<h1>Mind the Gap (Under the Curve)</h1>
<div class="subtitle">A Hobbyist's Summary on Quadrature with Applications to Option Pricing</div>
<div class="author">Author: Gene Boo | Date: Jun 2023, Sep 2025</div>
</div>
<div class="disclaimer">
<strong>Disclaimer:</strong> This article is intended as an educational guide aimed at other practitioners, hobbyist coders, and curious students. The methods and interpretations presented herein are aimed with the sole purpose of documenting various interesting quantitative methods as a personal archive and memorandum. While effort has been made to ensure clarity, readers should consult formal sources for rigorous applications (please refer to the biblio at the end of this document). The author is neither a mathematician nor a physicist nor computer scientist but merely a lowly risk practitioner and an avid hobbyist fanboy of algorithms. If the reader wishes to implement any of the material given, he/she is urged to verify and validate each step of the way.
        </div>
<h2 id="section-1">Introduction</h2>
<p>
		Numerical integration, or quadrature, is a cornerstone of computational mathematics. It’s the art of approximating definite integrals—those elegant expressions of accumulated change—using finite, well-chosen sums. In finance, especially in the realm of option pricing, quadrature methods offer a compelling alternative to closed-form formulas (when they exist) and Monte Carlo simulations (when brute force meets high dimensions). Quadrature is where mathematical elegance meets computational pragmatism.
        </p>
<p>
          The term <strong>“quadrature”</strong> comes from the Latin <em>quadratus</em>, meaning “square.” Historically, it referred to the classical geometric problem of <em>squaring a figure</em>—constructing a square with the same area as a given circle or other shape using only a straightedge and compass. This was a central challenge in ancient Greek mathematics, famously leading to the proof that exact squaring of the circle is impossible (since \(\pi\) is transcendental).

          <h4>Extension to Integration
          Over time, the meaning of quadrature broadened to denote <strong>area computation</strong> in general. Since integration in calculus is fundamentally about finding the area under a curve, the term “quadrature” became synonymous with <strong>numerical integration</strong>:
          \[
            \text{Quadrature: } \int_a^b f(x)\,dx \approx \sum_{i=1}^n w_i\,f(x_i),
          \]
          where \(w_i\) and \(x_i\) are weights and nodes in a quadrature rule (e.g., Simpson’s rule, Gaussian quadrature).

		</h4></p>
<div class="formal">
		Quadrature transforms integrals into weighted sums:
		\[ I = \int_a^b f(x)\,dx \approx \sum_{i=1}^n w_i f(x_i) \]
		Different rules pick nodes \(x_i\) and weights \(w_i\) to exploit smoothness, singularities, symmetry, or infinite domains. The smarter the rule, the fewer points you need—and the more accurate your result.
		</div>
<div class="layman">
<strong>Think of quadrature</strong> as placing a few clever sensors across a landscape to estimate its total brightness. You don’t need to measure every pixel—just the right ones. It’s like tasting a dish and knowing the recipe: strategic sampling beats brute force. Below are some simple to complex integration problems that many practitioners use quadrature for:
		</div>
<table class="integral-table">
<thead>
<tr>
<th>Rank</th>
<th>Domain</th>
<th>Integral</th>
<th>Name / Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Finance</td>
<td>
				\( C = e^{-rT} \int_{0}^{\infty} \max(s-K,0)\,f_{S_T}(s)\,ds \)
			  </td>
<td>Risk‑neutral expectation for a European call</td>
</tr>
<tr>
<td>2</td>
<td>Logistics</td>
<td>
				\(\mathbb{E}[C(Q)]=\int_{0}^{\infty}\!\big(c_h(x-Q)_+ + c_b(Q-x)_+\big)f_D(x)\,dx\)
			  </td>
<td>Newsvendor expected cost integral</td>
</tr>
<tr>
<td>3</td>
<td>Astrophysics</td>
<td>
				\( D_L(z)=(1+z)\frac{c}{H_0}\int_{0}^{z}\!\frac{dz'}{E(z')} \)
			  </td>
<td>Luminosity distance in FLRW cosmology</td>
</tr>
<tr>
<td>4</td>
<td>Physics</td>
<td>
				\( S(\alpha)=\int_{0}^{\infty}\!\cos(\alpha x^2)\,dx \)
			  </td>
<td>Fresnel cosine integral</td>
</tr>
<tr>
<td>5</td>
<td>Chemistry</td>
<td>
				\(\displaystyle \iint \frac{\phi_a(\mathbf r_1)\phi_b(\mathbf r_1)\,\phi_c(\mathbf r_2)\phi_d(\mathbf r_2)}{\lVert \mathbf r_1-\mathbf r_2\rVert}\,d\mathbf r_1\,d\mathbf r_2\)
			  </td>
<td>Two‑electron repulsion integral (ERI)</td>
</tr>
<tr>
<td>6</td>
<td>Machine Learning</td>
<td>
				\( Z(\theta)=\int_{\mathbb R^d}\!e^{-E(x;\theta)}\,dx \)
			  </td>
<td>Partition function of an energy‑based model</td>
</tr>
<tr>
<td>7</td>
<td>Neural Networks</td>
<td>
				\( p(y^*\!\mid x^*,\mathcal D)=\int p(y^*\!\mid x^*,w)\,p(w\mid \mathcal D)\,dw \)
			  </td>
<td>Bayesian neural network predictive distribution</td>
</tr>
</tbody>
</table>
<p>
		But quadrature isn’t just a numerical trick—it’s a philosophy. It asks: “What do I know about this function? Where does it change? Where does it hide its secrets?” And then it answers with precision, elegance, and sometimes, a bit of magic. Most importantly, with well implemented classes or helper functions, it preserves the original outlook of the integral within the code itself and this is the case for most computer languages (apart from say, assembly language).
		</p>
<p>
		In this guide, we’ll journey from the basics of integration to the cutting edge of financial modeling. We’ll explore trapezoids and Chebyshev nodes, adaptive zoom-ins and Gaussian elegance. We’ll see how quadrature powers option pricing—from Black-Scholes to Carr-Madan, from stop-loss premiums to strike-aware formulations. Whether you’re a curious teen, a hobbyist coder, or a budding quant, this article is your launchpad into the world where math meets money.
		</p>
<div class="key-point">
<strong>Why it matters:</strong> Quadrature lets us solve problems that are too messy for formulas and too slow for brute force. It’s the bridge between theory and practice, between elegance and efficiency.
		</div>
<nav id="toc"></nav>
<style>
  /* Optional, nicer jump alignment & simple styling */
  h1, h2, h3 { scroll-margin-top: 80px; }
  #toc { background:#fafafa; border:1px solid #e5e5e5; border-radius:8px; padding:1rem; }
  #toc ul { list-style: disc; margin: .3rem 0 .3rem 1.2rem; }
  #toc a { text-decoration: none; }
  #toc a:hover { text-decoration: underline; }
</style>
<script>
document.addEventListener('DOMContentLoaded', function () {
  const tocContainer = document.getElementById('toc');
  if (!tocContainer) return;

  // Collect only H2 and H3
  const headers = Array.from(document.querySelectorAll('h2, h3'));

  // Utilities
  const slugify = (txt) => txt
    .toLowerCase()
    .replace(/[–—]/g, '-')                // en/em dashes -> hyphen
    .replace(/[^a-z0-9\s-]/g, '')         // strip punctuation
    .replace(/[\s-]+/g, '-')              // spaces -> hyphen
    .replace(/^-+|-+$/g, '') || 'section';

  // Build nested structure: H2 -> [H3...]
  const toc = document.createElement('ul');
  tocContainer.appendChild(toc);

  let lastH2Li = null;

  headers.forEach(h => {
    const text = (h.textContent || '').trim();
    if (!text) return;
    if (text.toLowerCase() === 'table of contents') return; // skip the TOC heading itself

    // Ensure a stable id
    if (!h.id) h.id = slugify(text);

    const li = document.createElement('li');
    const a  = document.createElement('a');
    a.href = '#' + h.id;
    a.textContent = text;
    li.appendChild(a);

    if (h.tagName.toLowerCase() === 'h2') {
      toc.appendChild(li);
      lastH2Li = li; // remember parent for following H3s
    } else { // h3
      // attach as a child of the most recent H2; if none, place at root
      let parent = lastH2Li;
      if (!parent) { parent = document.createElement('li'); toc.appendChild(parent); lastH2Li = parent; }
      let sub = parent.querySelector(':scope > ul');
      if (!sub) { sub = document.createElement('ul'); parent.appendChild(sub); }
      sub.appendChild(li);
    }
  });
});
</script>
<h2 id="section-2">Fundamental Quadrature Methods</h2>
<h3 id="section-2-1">Trapezoid Rule</h3>
<div class="formal">
			The simplest of them all, but wait - don't throw it out! It may not serve well compared to other methods for direct integration, but it is most useful for post‑processing Fast Fourier Transform (FFT) results. We present to you the trapezoid rule, which approximates integrals using linear interpolation to link the nodes:
			

			\[
				\int_a^b f(x)\,dx \approx \frac{b-a}{2} \,[f(a) + f(b)]
			\]


			<br/><br/>
			In the context of FFT‑based methods, the trapezoid rule has a unique advantage: the FFT naturally produces function samples at equally spaced points over a periodic domain. 
			The trapezoid rule is <em>exact</em> for integrating any trigonometric polynomial whose highest frequency is below the Nyquist limit — precisely the type of output the FFT delivers. 
			This means that for smooth, periodic functions reconstructed from FFT coefficients, the trapezoid rule achieves spectral (exponentially fast) convergence without the complexity of higher‑order schemes.
			<br/><br/>
			Other quadrature rules, such as Simpson’s or Gaussian quadrature, require either non‑uniform nodes or additional function evaluations at midpoints, which are not directly available from the FFT grid without interpolation. 
			Interpolation would add computational cost and potential aliasing errors, negating their theoretical accuracy advantage. 
			The trapezoid rule, by contrast, uses the FFT’s existing uniform grid directly, making \( \mathbf{O}(N) \) to apply after an \( \mathbf{O}(N \log N) \) transform, with no extra sampling.
			<br/><br/>
			In short, when your data is periodic and uniformly sampled — as it is after an FFT — the trapezoid rule is not just “good enough,” it is <em>optimal</em> in both accuracy and efficiency.
		</div>
<div class="pseudocode">
# Composite Trapezoid Rule Pseudocode
function trapezoid(f, a, b, n):
    h = (b - a) / n
    sum = 0.5 * (f(a) + f(b))
    for i from 1 to n-1:
        x = a + i * h
        sum += f(x)
    return h * sum
        </div>
<div class="layman">
<strong>Imagine measuring</strong> a curved shape by breaking it into many small trapezoids instead of rectangles. It's like using slanted roof pieces instead of flat blocks to cover a curved surface - you get a better fit with the same number of pieces!
        </div>
<h3 id="section-2-2">Simpson's Rule</h3>
<div class="formal">
			Simpson's rule uses quadratic interpolation for better accuracy:
			

		\[
				\int_a^b f(x)\,dx \approx \frac{b-a}{6} \left[f(a) + 4f\left(\frac{a+b}{2}\right) + f(b)\right]
			\]


			It is a special case of the <em>Newton–Cotes formulas</em>, which approximate the integrand by an \(n\)-degree polynomial passing through \(n+1\) equally spaced points, then integrate that polynomial exactly.
		</div>
<h4>Beyond Quadratics: Higher-Degree Newton–Cotes Rules</h4>
<div class="formal">
			By increasing the degree of the interpolating polynomial, we can create more accurate rules (for smooth functions) without refining the grid:
			<ul>
<li><strong>Trapezoid Rule</strong> — degree 1 (linear interpolation)</li>
<li><strong>Simpson's Rule</strong> — degree 2 (quadratic interpolation)</li>
<li><strong>Simpson’s 3/8 Rule</strong> — degree 3 (cubic interpolation):
					

		\[
						\int_a^b f(x)\,dx \approx \frac{3h}{8} \left[f(x_0) + 3f(x_1) + 3f(x_2) + f(x_3)\right]
					\]


					where \(h = \frac{b-a}{3}\).
				</li>
<li><strong>Boole’s Rule</strong> — degree 4 (quartic interpolation):
					

		\[
						\int_a^b f(x)\,dx \approx \frac{2h}{45} \left[7f(x_0) + 32f(x_1) + 12f(x_2) + 32f(x_3) + 7f(x_4)\right]
					\]


					where \(h = \frac{b-a}{4}\).
				</li>
</ul>
			These higher-order rules can integrate polynomials of degree up to \(n\) exactly, but they may suffer from <em>Runge’s phenomenon</em> and numerical instability if \(n\) is too large over wide intervals. In practice, they are often applied in <strong>composite form</strong> — breaking the domain into smaller subintervals and applying the low-degree rule repeatedly.
		</div>
<div class="layman">
<strong>Think of it like this:</strong> Simpson’s rule is like fitting a smooth arch between three points. If you add more points, you can fit fancier curves — cubic, quartic, and beyond — which hug the function more closely. But if you try to fit too fancy a curve over too wide a stretch, it can wiggle wildly between points. That’s why we often use these rules in small chunks and piece them together. The concept sounds similar to splines, but in splines there are gradient constraints between pieces in place to ensure smoothness.
		</div>
<h4>Beyond Polynomials: Spline-Based Numerical &amp; Analytical Hybrid Integration</h4>
<div class="formal">
  Spline integration uses <strong>piecewise polynomial functions</strong> to approximate a function and then integrates the spline instead of the original function. This approach gives smoothness and numerical stability, especially when the function’s behavior varies across intervals. It is a hybrid as the integration of spline curves can be done analytically.
        The integral is computed by summing the area under each spline segment:

        \[
        \int_a^b f(x)\,dx \approx \sum_{i=1}^{n} \int_{x_{i-1}}^{x_i} S_i(x)\,dx
        \]

        where \(S_i(x)\) is the spline polynomial on the \(i\)-th interval.

        <strong>Runge’s Phenomenon:</strong> When using high-degree polynomials over wide intervals, the interpolation may oscillate wildly between points, especially near the edges. This leads to poor approximations and instability. Spline methods mitigate this by using low-degree polynomials locally.


  <ul>
<li>
<strong>Linear Spline Integration</strong> — piecewise degree 1 (equivalent to trapezoid rule):<br/>
      On each interval \(I_i=[x_i,x_{i+1}]\) with \(h_i=x_{i+1}-x_i\),
      \[
        S_i(x)=y_i + d_i\,(x-x_i),\quad d_i=\frac{y_{i+1}-y_i}{h_i}.
      \]
      The exact integral of this segment is the trapezoid rule:
      \[
        \int_{x_i}^{x_{i+1}} S_i(x)\,dx
        \;=\;\frac{h_i}{2}\,\big(y_i+y_{i+1}\big).
      \]
    </li>
<li>
<strong>Cubic Spline Integration</strong> — piecewise degree 3 with \(C^2\) continuity:<br/>
      Let \(M_i\) denote the second derivative at node \(x_i\) (e.g., a <em>natural</em> spline uses \(M_0=M_n=0\)). On \(I_i=[x_i,x_{i+1}]\),
      \[
      \begin{aligned}
        S_i(x)&amp;=\frac{M_i}{6h_i}\,(x_{i+1}-x)^3+\frac{M_{i+1}}{6h_i}\,(x-x_i)^3\\
              &amp;\quad+\left(y_i-\frac{M_i h_i^2}{6}\right)\frac{x_{i+1}-x}{h_i}
                   +\left(y_{i+1}-\frac{M_{i+1} h_i^2}{6}\right)\frac{x-x_i}{h_i}.
      \end{aligned}
      \]
      Each \(S_i\) is a cubic polynomial, so \(\int_{x_i}^{x_{i+1}} S_i(x)\,dx\) is obtained analytically by integrating the above terms.
    </li>
<li>
<strong>Hermite Spline Integration</strong> — piecewise cubic with derivative constraints (cubic Hermite):<br/>
      Using values \(y_i=f(x_i)\) and slopes \(m_i=f'(x_i)\), with \(t=\frac{x-x_i}{h_i}\) on \(I_i\),
      \[
      \begin{aligned}
        S_i(x) &amp;= y_i\,h_{00}(t) + h_i m_i\,h_{10}(t) + y_{i+1}\,h_{01}(t) + h_i m_{i+1}\,h_{11}(t),\\
        h_{00}(t)&amp;=2t^3-3t^2+1,\quad
        h_{10}(t)=t^3-2t^2+t,\\
        h_{01}(t)&amp;=-2t^3+3t^2,\quad
        h_{11}(t)=t^3-t^2.
      \end{aligned}
      \]
      This form is smooth and integrates exactly per segment by integrating the basis polynomials \(h_{rs}(t)\).
    </li>
<li>
<strong>PCHIP (Piecewise Cubic Hermite Interpolating Polynomial)</strong> — monotone, shape-preserving cubic Hermite:<br/>
      PCHIP is the Hermite spline above with slopes \(m_i\) chosen to preserve local monotonicity and avoid overshoot. Let \(h_{k}=x_{k+1}-x_k\) and
      \[
        d_{k}=\frac{y_{k+1}-y_k}{h_k}.
      \]
      For interior nodes \(k=1,\dots,n-1\),
      \[
        m_k=\begin{cases}
          0, &amp; \text{if } d_{k-1}\,d_k\le 0,\[6pt]
          \displaystyle\frac{w_1+w_2}{\frac{w_1}{d_{k-1}}+\frac{w_2}{d_k}}, &amp; \text{otherwise,}
        \end{cases}
      \]
      where \(w_1=2h_k+h_{k-1}\) and \(w_2=h_k+2h_{k-1}\).
      (End slopes use one‑sided, shape‑preserving formulas.) Integration then follows the Hermite segment integral.
    </li>
<li>
<strong>B‑Spline Integration</strong> — basis‑spline representation with local control:<br/>
      Represent the fitted spline as
      \[
        S(x)=\sum_{j} c_j\,N_{j,p}(x),
      \]
      where \(N_{j,p}\) are B‑spline basis functions of degree \(p\) defined by the Cox–de Boor recursion with knot vector \(\{t_k\}\):
      \[
        N_{j,0}(x)=\begin{cases}1,&amp;t_j\le x<t_{j+1}\\0,&\text{otherwise}\end{cases},\qquad (or="" <="" \[="" \]="" \int_a^b="" \int_{-\infty}^{\infty}="" basis="" be="" c_j\int_a^b="" can="" computed="" definite="" exactly="" hence,="" high‑order="" if="" integral="" integrals;="" its="" knot="" li="" n_{j,p}(x)="\frac{x-t_j}{t_{j+p}-t_j}N_{j,p-1}(x)\;+\;\frac{t_{j+p+1}-x}{t_{j+p+1}-t_{j+1}}N_{j+1,p-1}(x)." n_{j,p}(x)\,dx="\frac{t_{j+p+1}-t_j}{p+1}." n_{j,p}(x)\,dx,="" on="" or="" per="" preferred).="" quadrature="" s(x)\,dx\;\approx\;\sum_j="" span="" span‑by‑span="" support,="" the="" via="" with="">
<li>
<strong>Mixed Splines</strong> — adaptively choose the spline per region:<br/>
      Partition \([a,b]\) into subregions \(\{R_s\}\). In smooth regions use a cubic spline for \(C^2\) accuracy; near kinks, steps, or strongly monotone stretches use PCHIP to prevent overshoot:
      \[
        \int_a^b f(x)\,dx\;\approx\;\sum_s \int_{R_s} S^{(s)}(x)\,dx,\qquad
        S^{(s)}(x)\in\{\text{cubic},\text{PCHIP},\text{linear}\},
      \]
      enforcing at least \(C^0\) continuity (and optionally \(C^1\)) at region boundaries.
    </li>
</t_{j+1}\\0,&\text{otherwise}\end{cases},\qquad></li></ul>
<p>The composite spline integral is the sum of segment integrals:</p>
  \[
    \int_a^b f(x)\,dx \;\approx\; \sum_{i=1}^{n} \int_{x_{i-1}}^{x_i} S_i(x)\,dx.
  \]

  <p><strong>Runge’s Phenomenon:</strong> High‑degree single‑interval polynomials may oscillate near the edges, producing unstable approximations. Splines mitigate this by using low‑degree polynomials locally with continuity constraints.</p>
<h5>Real‑World Examples</h5>
<ul>
<li><strong>Finance (Yield Curves):</strong> Integrate discount factors or forward rates. PCHIP is often preferred to avoid spurious arbitrage (overshoots) in sparsely sampled maturities.</li>
<li><strong>Engineering (Stress–Strain Energy):</strong> Integrate smooth cubic splines of experimental stress–strain data to compute specific energy absorption.</li>
<li><strong>Robotics &amp; Motion Planning:</strong> B‑splines model trajectories; integrating speed along a spline yields time/energy estimates with local control over segments.</li>
<li><strong>Medical/Imaging:</strong> Integrate dose–response or intensity profiles reconstructed with splines to get total dose/flux while preserving monotonic sections with PCHIP.</li>
<li><strong>Computer Graphics/CAD:</strong> B‑splines (and NURBS) provide precise curve/surface control; integrating along parameterized splines gives arc length or area.</li>
</ul>
</div>
<div class="pseudocode">
# Spline-Based Numerical &amp; Analytical Hybrid Integration
// Inputs: x[0..n], y[0..n]  (monotone x), method ∈ { "linear", "cubic", "pchip", "bspline" }
// Optional params: bc, knots, degree, quad_points, region_selector (for "mixed")
function spline_integrate(x, y, method, params):

    if method == "mixed":
        regions = params.region_selector(x, y)   // e.g., segment by smoothness/monotonicity
        total = 0
        for R in regions:                        // R = {idx_start, idx_end, methodR}
            total += spline_integrate(x[R], y[R], methodR, params_for(methodR))
        return total

    // 1) Fit the chosen spline
    if method == "linear":
        S = fit_linear_spline(x, y)              // store per-interval slope d_i
    else if method == "cubic":
        S = fit_cubic_spline(x, y, bc=params.bc) // solve tri-diagonal for second derivs M_i
    else if method == "pchip":
        S = fit_pchip(x, y)                      // compute monotone slopes m_i (Fritsch–Carlson)
    else if method == "bspline":
        (t, c, p) = bspline_fit(x, y, degree=params.degree, knots=params.knots)

    // 2) Integrate
    total = 0
    if method in {"linear","cubic","pchip"}:
        for i in 1..n:
            a = x[i-1]; b = x[i]
            // retrieve polynomial coefficients for S_i(x) on [a,b]
            coeffs = local_polynomial(S, i)      // e.g., a0 + a1*(x-a) + a2*(x-a)^2 + a3*(x-a)^3
            total += poly_segment_integral(coeffs, a, b) // analytic antiderivative
        return total

    else if method == "bspline":
        // integrate span-by-span on knot intervals where basis is low-degree and sparse
        for k in valid_knot_spans(t):
            a = t[k]; b = t[k+1]
            if b &lt;= a: continue
            // Quadrature (robust for arbitrary knots). p+3 Gauss points is usually enough.
            total += gauss_legendre(
                        f(x) = sum_j c[j]*B_spline_basis(j, p, x, t),
                        a, b, m = params.quad_points or (p + 3))
        return total
  </div>
<div class="layman">
<strong>Think of it like this:</strong> Instead of stretching one big curve over all your data (which can wiggle too much), spline integration fits lots of small, smooth curves between pairs of points, but also ensures smoothness at the joints for non-linear ones. Hermite splines even use the slope at each point to make the fit smarter. Then, you add up the area under each little curve to get the total. It’s like laying flexible tiles over a bumpy surface — much more accurate and stable than one big sheet!
        </div>
<div class="diagram">
<h5>Visual Comparison of Interpolation Methods</h5>
<p>The diagram below shows how different interpolation methods behave. Notice how the high-degree polynomial (red) oscillates near the edges — this is Runge’s phenomenon. Cubic and Hermite splines (blue and green) follow the true function (dashed black but hidden by the splines) more smoothly.</p>
<svg height="360" viewbox="0 0 640 360" width="640" xmlns="http://www.w3.org/2000/svg">
<defs>
<style>
      .c{fill:none;stroke-width:2.2;stroke-linecap:round;stroke-linejoin:round}
      text{font-family:system-ui,Segoe UI,Roboto,Arial,sans-serif}
    </style>
</defs>
<text font-size="14" font-weight="600" text-anchor="middle" x="320" y="20">
    Interpolation Methods on Runge's Function
  </text>
<!-- Frame -->
<rect fill="none" height="296" stroke="#bbb" width="580" x="48" y="28"></rect>
<!-- Curves (computed): true function (dashed), polynomial (red), cubic spline (blue), PCHIP (green) -->
<path class="c" d="M 48.00,103.35 L 69.48,102.95 L 90.96,102.44 L 112.44,101.79 L 133.93,100.95 L 155.41,99.83 L 176.89,98.30 L 198.37,96.16 L 219.85,93.04 L 241.33,88.38 L 262.81,81.20 L 284.30,70.26 L 305.78,55.31 L 327.26,41.90 L 348.74,41.90 L 370.22,55.31 L 391.70,70.26 L 413.19,81.20 L 434.67,88.38 L 456.15,93.04 L 477.63,96.16 L 499.11,98.30 L 520.59,99.83 L 542.07,100.95 L 563.56,101.79 L 585.04,102.44 L 606.52,102.95 L 628.00,103.35" stroke="#000" stroke-dasharray="4,6"></path>
<path class="c" d="M 48.00,103.35 L 69.48,310.55 L 90.96,130.48 L 112.44,69.36 L 133.93,88.43 L 155.41,107.11 L 176.89,104.59 L 198.37,94.42 L 219.85,89.25 L 241.33,88.38 L 262.81,83.84 L 284.30,71.04 L 305.78,53.82 L 327.26,41.45 L 348.74,41.45 L 370.22,53.82 L 391.70,71.04 L 413.19,83.84 L 434.67,88.38 L 456.15,89.25 L 477.63,94.42 L 499.11,104.59 L 520.59,107.11 L 542.07,88.43 L 563.56,69.36 L 585.04,130.48 L 606.52,310.55 L 628.00,103.35" stroke="#d62728"></path>
<path class="c" d="M 48.00,103.35 L 69.48,102.93 L 90.96,102.44 L 112.44,101.80 L 133.93,100.95 L 155.41,99.83 L 176.89,98.30 L 198.37,96.16 L 219.85,93.06 L 241.33,88.38 L 262.81,81.29 L 284.30,70.35 L 305.78,54.86 L 327.26,41.85 L 348.74,41.85 L 370.22,54.86 L 391.70,70.35 L 413.19,81.29 L 434.67,88.38 L 456.15,93.06 L 477.63,96.16 L 499.11,98.30 L 520.59,99.83 L 542.07,100.95 L 563.56,101.80 L 585.04,102.44 L 606.52,102.93 L 628.00,103.35" stroke="#1f77b4"></path>
<path class="c" d="M 48.00,103.35 L 69.48,102.97 L 90.96,102.44 L 112.44,101.79 L 133.93,100.95 L 155.41,99.83 L 176.89,98.29 L 198.37,96.16 L 219.85,92.97 L 241.33,88.38 L 262.81,80.37 L 284.30,69.55 L 305.78,56.24 L 327.26,42.20 L 348.74,42.20 L 370.22,56.24 L 391.70,69.55 L 413.19,80.37 L 434.67,88.38 L 456.15,92.97 L 477.63,96.16 L 499.11,98.29 L 520.59,99.83 L 542.07,100.95 L 563.56,101.79 L 585.04,102.44 L 606.52,102.97 L 628.00,103.35" stroke="#2ca02c"></path>
</svg>
</div>
<h3 id="section-2-3">Romberg Integration</h3>
<div class="formal">
			Romberg integration is a systematic way to refine trapezoidal rule estimates by applying 
			<strong>Richardson extrapolation</strong> — a technique that uses results at different step sizes 
			to cancel out leading error terms and accelerate convergence.
			<br/><br/>
			The trapezoidal rule has an error term proportional to \(h^2\) for smooth functions, where \(h\) is the step size. 
			If we compute the trapezoid estimate with \(h\) and with \(h/2\), we can combine them to eliminate the \(h^2\) term, 
			leaving an error of order \(h^4\).
			<br/><br/>
			Romberg’s recursive scheme:
			

		\[
				R(k,0) = \text{Trapezoid estimate with } 2^k \text{ subintervals}
			\]


			

		\[
				R(k,m) = R(k,m-1) + \frac{R(k,m-1) - R(k-1,m-1)}{4^m - 1}
			\]


			Here:
			<ul>
<li>\(k\) = refinement level (more subintervals)</li>
<li>\(m\) = extrapolation level (higher-order correction)</li>
<li>\(4^m\) appears because halving \(h\) reduces the error term by a factor of \(2^{2m}\) for even-order methods</li>
</ul>
</div>
<h4>Two-Step Example</h4>
<div class="formal">
			Suppose we want \(\int_0^1 e^{-x^2} dx\):
			<ol>
<li>Compute \(R(0,0)\) using 1 trapezoid (\(h=1\)) → say \(0.68394\)</li>
<li>Compute \(R(1,0)\) using 2 trapezoids (\(h=0.5\)) → say \(0.73137\)</li>
<li>Apply Richardson extrapolation for \(m=1\):
					

		\[
						R(1,1) = R(1,0) + \frac{R(1,0) - R(0,0)}{4^1 - 1}
					\]


					

		\[
						R(1,1) = 0.73137 + \frac{0.73137 - 0.68394}{3} \approx 0.74737
					\]


					This is already much closer to the true value \(\approx 0.74682\).
				</li>
</ol>
</div>
<h4>Three-Step Example</h4>
<div class="formal">
			Continuing:
			<ol>
<li>Compute \(R(2,0)\) with 4 trapezoids (\(h=0.25\)) → \(0.74298\)</li>
<li>Extrapolate with \(m=1\):
					

		\[
						R(2,1) = 0.74298 + \frac{0.74298 - 0.73137}{3} \approx 0.74692
					\]


				</li>
<li>Extrapolate again with \(m=2\) using \(R(2,1)\) and \(R(1,1)\):
					

		\[
						R(2,2) = 0.74692 + \frac{0.74692 - 0.74737}{15} \approx 0.74689
					\]


					Now we’re way more accurate after just 3 trapezoid refinements.
				</li>
</ol>
</div>
<div class="layman">
<strong>Analogy 1 – The Coastline Trick:</strong>  
			Imagine measuring the length of a coastline with a big ruler — you miss all the little bays and inlets.  
			Then you measure again with a smaller ruler — you catch more detail, but still miss the tiniest wiggles.  
			If you know how the error shrinks when you halve the ruler size, you can combine the two measurements to cancel most of the bias and get a much better estimate without going to an absurdly tiny ruler.
		</div>
<div class="layman">
<strong>Analogy 2 – The Driving Speed Limit Sign:</strong>  
			Suppose you’re driving toward a road sign with small text under the speed limit. At 100m away you can sort of guess the letters, but they’re fuzzy. At 50m they’re clearer, but still not perfect. If you notice how much clearer things get when halving the distance, you can predict what the sign says even before you’re right next to it.
		</div>
<h2 id="section-3">Advanced Quadrature Techniques</h2>
<section class="chapter" id="gaussian">
<h3 id="section-3-1">Gaussian Quadrature</h3>
<div class="formal">
			Gaussian quadrature chooses both the <em>nodes</em> \(x_i\) and <em>weights</em> \(w_i\) so that the formula
			

		\[
				\int_a^b f(x)\,dx \approx \sum_{i=1}^n w_i f(x_i)
			\]


			is exact for all polynomials up to degree \(2n-1\) (the maximum possible for \(n\) points).
			The nodes are the roots of an orthogonal polynomial associated with a weight function \(w(x)\) on the interval \([a,b]\), and the weights are derived from the orthogonality conditions.
			This optimal placement means fewer points are needed for the same accuracy compared to equally spaced rules.
		</div>
<div class="layman">
<strong>Think of Gaussian quadrature</strong> as sending surveyors to the <em>most informative spots</em> in a landscape.
			Instead of spacing them evenly, you place them exactly where they’ll capture the most detail about the terrain.
			The math behind it guarantees that if your landscape is made of smooth hills (polynomials), you’ll measure it perfectly with surprisingly few surveyors.
		</div>
<h4>Gauss–Legendre</h4>
<div class="formal">
			For general integrals on \([-1, 1]\) with weight function \(w(x) = 1\):
			

		\[
				\int_{-1}^1 f(x)\,dx \approx \sum_{i=1}^n w_i f(x_i)
			\]


			where \(x_i\) are the roots of the Legendre polynomial \(P_n(x)\).
			This is the most common Gaussian quadrature and is easily rescaled to any finite interval \([a,b]\).
		</div>
<h4>Gauss–Hermite</h4>
<div class="formal">
			For integrals with weight \(e^{-x^2}\) on \((-\infty, \infty)\):
			

		\[
				\int_{-\infty}^{\infty} f(x)e^{-x^2}\,dx \approx \sum_{i=1}^n w_i f(x_i)
			\]


			where \(x_i\) are the roots of the Hermite polynomial \(H_n(x)\).
			This is ideal for problems involving the normal distribution, such as option pricing under Black–Scholes.
		</div>
<h4>Gauss–Laguerre</h4>
<div class="formal">
			For integrals with weight \(e^{-x}\) on \([0, \infty)\):
			

		\[
				\int_0^{\infty} f(x)e^{-x}\,dx \approx \sum_{i=1}^n w_i f(x_i)
			\]


			where \(x_i\) are the roots of the Laguerre polynomial \(L_n(x)\).
			Useful for exponential decay problems, such as radioactive decay models or certain Laplace transforms, and it is often used to integrate Lognormal problems - which we encounter a lot in GBM option pricing.
		</div>
<h4 id="gauss-comparison-svg">Gauss Node–Weight Comparison (N=50)</h4>
<figure class="inline-svg">
<!-- Inline SVG: Gauss–Hermite (blue), Gauss–Legendre (red), Gauss–Laguerre (green)
         Sizes scale with weights; lanes labeled; neutral grey axis lines. -->
<svg aria-labelledby="gauss-lanes-title" role="img" viewbox="0 0 1100 430">
<title id="gauss-lanes-title">Gauss Quadrature Nodes and Weights (N=50)</title>
<desc>Hermite on (-∞,∞), Legendre on [-1,1], Laguerre on [0,∞)</desc>
<!-- Axes & lanes -->
<rect fill="white" height="420" width="1100" x="0" y="0"></rect>
<g transform="translate(60,30)">
<!-- Lane guide lines -->
<line stroke="#ddd" stroke-dasharray="6,4" x1="0" x2="980" y1="80" y2="80"></line>
<line stroke="#ddd" stroke-dasharray="6,4" x1="0" x2="980" y1="190" y2="190"></line>
<line stroke="#ddd" stroke-dasharray="6,4" x1="0" x2="980" y1="300" y2="300"></line>
<!-- Lane labels -->
<text fill="#2a7f2a" font-size="14" x="0" y="75">Laguerre [0, ∞) — weight e^{-x}</text>
<text fill="#1c4db3" font-size="14" x="0" y="185">Hermite (−∞, ∞) — weight e^{-x²}</text>
<text fill="#b31818" font-size="14" x="0" y="295">Legendre [−1, 1] — weight 1</text>
<!-- Axis label -->
<text font-size="14" text-anchor="middle" x="490" y="360">Node position xᵢ (scaled per family)</text>
<text font-size="14" text-anchor="end" transform="rotate(-90,-30,10)" x="-30" y="10">Quadrature Type</text>
<!-- (1) Gauss–Laguerre nodes (green) -->
<!-- Approximate layout: nodes concentrated near 0; spread out to the right.
             For clarity, x is nonlinearly mapped to 0..980 (not to scale with Hermite/Legendre). -->
<g id="laguerre">
<!-- generated sample positions (stylized) -->
<!-- You can replace with precise nodes later; visual intent is preserved. -->
<!-- N=50: render points with varying radius ~ weights -->
<!-- Points -->
<!-- (Sample set) -->
<!-- We'll place ~50 circles; varying sizes -->
<g fill="#2ca02c" fill-opacity="0.85" stroke="#1f7a1f" stroke-width="0.5">
<!-- (compact cluster: first 20 near x ~ [10..200]) -->
<!-- (hand-tuned scatter for visual faithfulness) -->
<!-- For brevity, we render a representative set (you can paste the full generated set from the Appendix Python if desired) -->
<!-- Cluster near origin -->
<circle cx="20" cy="80" r="5"></circle><circle cx="35" cy="80" r="6"></circle><circle cx="50" cy="80" r="7"></circle>
<circle cx="68" cy="80" r="8"></circle><circle cx="85" cy="80" r="8.5"></circle><circle cx="102" cy="80" r="8.5"></circle>
<circle cx="120" cy="80" r="8.2"></circle><circle cx="138" cy="80" r="7.8"></circle><circle cx="156" cy="80" r="7.4"></circle>
<circle cx="174" cy="80" r="7.0"></circle><circle cx="192" cy="80" r="6.6"></circle><circle cx="210" cy="80" r="6.2"></circle>
<!-- tails -->
<circle cx="250" cy="80" r="5.5"></circle><circle cx="300" cy="80" r="5.0"></circle><circle cx="360" cy="80" r="4.2"></circle>
<circle cx="430" cy="80" r="3.6"></circle><circle cx="510" cy="80" r="3.1"></circle><circle cx="600" cy="80" r="2.6"></circle>
<circle cx="700" cy="80" r="2.1"></circle><circle cx="820" cy="80" r="1.7"></circle><circle cx="960" cy="80" r="1.3"></circle>
</g>
</g>
<!-- (2) Gauss–Hermite nodes (blue) -->
<!-- Symmetric nodes; largest weights near 0; decay outward -->
<g id="hermite">
<g fill="#1f77b4" fill-opacity="0.9" stroke="#124a74" stroke-width="0.5">
<!-- Center -->
<circle cx="490" cy="190" r="10"></circle>
<!-- symmetric pairs -->
<circle cx="450" cy="190" r="9.3"></circle><circle cx="530" cy="190" r="9.3"></circle>
<circle cx="412" cy="190" r="8.2"></circle><circle cx="568" cy="190" r="8.2"></circle>
<circle cx="378" cy="190" r="7.1"></circle><circle cx="602" cy="190" r="7.1"></circle>
<circle cx="346" cy="190" r="6.1"></circle><circle cx="634" cy="190" r="6.1"></circle>
<circle cx="316" cy="190" r="5.2"></circle><circle cx="664" cy="190" r="5.2"></circle>
<circle cx="288" cy="190" r="4.5"></circle><circle cx="692" cy="190" r="4.5"></circle>
<circle cx="262" cy="190" r="4.0"></circle><circle cx="718" cy="190" r="4.0"></circle>
<circle cx="238" cy="190" r="3.5"></circle><circle cx="742" cy="190" r="3.5"></circle>
<circle cx="216" cy="190" r="3.1"></circle><circle cx="764" cy="190" r="3.1"></circle>
<circle cx="196" cy="190" r="2.7"></circle><circle cx="784" cy="190" r="2.7"></circle>
</g>
</g>
<!-- (3) Gauss–Legendre nodes (red) on [-1,1]: evenly spanning; weights slightly larger mid-interval -->
<g id="legendre">
<g fill="#d62728" fill-opacity="0.9" stroke="#8a1b1b" stroke-width="0.5">
<!-- linear spacing along lane -->
<!-- 20 representative points (again: replace with full set if desired) -->
<circle cx="20" cy="300" r="4.0"></circle><circle cx="70" cy="300" r="4.7"></circle><circle cx="120" cy="300" r="5.3"></circle>
<circle cx="170" cy="300" r="5.8"></circle><circle cx="220" cy="300" r="6.2"></circle><circle cx="270" cy="300" r="6.5"></circle>
<circle cx="320" cy="300" r="6.7"></circle><circle cx="370" cy="300" r="6.8"></circle><circle cx="420" cy="300" r="6.9"></circle>
<circle cx="470" cy="300" r="7.0"></circle><circle cx="520" cy="300" r="7.0"></circle><circle cx="570" cy="300" r="6.9"></circle>
<circle cx="620" cy="300" r="6.8"></circle><circle cx="670" cy="300" r="6.7"></circle><circle cx="720" cy="300" r="6.5"></circle>
<circle cx="770" cy="300" r="6.2"></circle><circle cx="820" cy="300" r="5.8"></circle><circle cx="870" cy="300" r="5.3"></circle>
<circle cx="920" cy="300" r="4.7"></circle><circle cx="970" cy="300" r="4.0"></circle>
</g>
</g>
<!-- Legend -->
<g font-size="13" transform="translate(10,330)">
<rect fill="#fff" height="70" rx="8" ry="8" stroke="#ddd" width="300" x="0" y="0"></rect>
<circle cx="18" cy="18" fill="#1f77b4" r="6" stroke="#124a74"></circle><text x="36" y="22">Gauss–Hermite (−∞,∞)</text>
<circle cx="18" cy="40" fill="#d62728" r="6" stroke="#8a1b1b"></circle><text x="36" y="44">Gauss–Legendre [−1,1]</text>
<circle cx="18" cy="62" fill="#2ca02c" r="6" stroke="#1f7a1f"></circle><text x="36" y="66">Gauss–Laguerre [0,∞)</text>
</g>
</g>
</svg>
<figcaption>Node positions by family (lanes) with marker size ∝ weight. (Illustrative; exact nodes/weights depend on N and scaling.)</figcaption>
</figure>
<h4>Beyond Legendre, Hermite, Laguerre…</h4>
<div class="formal">
			Gaussian quadrature is a <em>family</em> of rules, each tied to a specific orthogonal polynomial and weight function:
			<ul>
<li><strong>Gauss–Chebyshev</strong> (first and second kind) — for weights \((1-x^2)^{-1/2}\) and \((1-x^2)^{1/2}\) on \([-1,1]\), excellent for trigonometric integrals.</li>
<li><strong>Gauss–Jacobi</strong> — generalizes Legendre and Chebyshev with weight \((1-x)^\alpha (1+x)^\beta\), \(\alpha,\beta &gt; -1\).</li>
<li><strong>Gauss–Gegenbauer</strong> — for ultraspherical weights \((1-x^2)^{\lambda - 1/2}\).</li>
<li><strong>Gauss–Radau</strong> — fixes one endpoint as a node; exact for degree \(2n-2\).</li>
<li><strong>Gauss–Lobatto</strong> — fixes both endpoints; exact for degree \(2n-3\).</li>
<li><strong>Gauss–Kronrod</strong> — extends an \(n\)-point Gauss–Legendre rule to \(2n+1\) points, nesting the original nodes for adaptive error estimation.</li>
</ul>
			Each variant is tuned to a specific weight function or integration constraint, allowing you to exploit known structure in the integrand.
		</div>
<div class="layman">
<strong>Think of these variants</strong> as different “dial settings” on the same precision instrument.
			If you know your function has certain quirks — like it’s heavier near the edges, or it lives on an infinite domain — you pick the Gaussian rule that’s already tuned for that shape.
			That way, you get more accuracy with fewer points, just by matching the tool to the job.
		</div>
<h4>Gauss–Chebyshev</h4>
<div class="formal">
			Gauss–Chebyshev quadrature is tailored for integrals with the weight function 
			\( w(x) = \frac{1}{\sqrt{1 - x^2}} \) on \([-1, 1]\):
			

		\[
				\int_{-1}^1 \frac{f(x)}{\sqrt{1 - x^2}}\,dx \approx \frac{\pi}{n} \sum_{i=1}^n f\!\left(\cos\frac{2i-1}{2n}\pi\right)
			\]


			The nodes are simply \( x_i = \cos\frac{2i-1}{2n}\pi \) and all weights are equal to \( \pi/n \).
			This rule is exact for polynomials of degree up to \( 2n-1 \) multiplied by the weight \( w(x) \).
			It is especially efficient for integrals that can be transformed into this form, such as many trigonometric integrals.
		</div>
<div class="layman">
<strong>Think of Gauss–Chebyshev</strong> as a method that already “knows” your function lives on a circle.
			It places its measuring points at equally spaced angles around that circle, which makes it perfect for problems involving sines, cosines, or anything periodic in disguise.
		</div>
<h4>Gauss–Jacobi</h4>
<div class="formal">
			Gauss–Jacobi quadrature generalizes Gauss–Legendre and Gauss–Chebyshev by using the weight function
			\( w(x) = (1-x)^\alpha (1+x)^\beta \), with \( \alpha, \beta &gt; -1 \), on \([-1, 1]\):
			

		\[
				\int_{-1}^1 (1-x)^\alpha (1+x)^\beta f(x)\,dx \approx \sum_{i=1}^n w_i f(x_i)
			\]


			The nodes \(x_i\) are the roots of the Jacobi polynomial \(P_n^{(\alpha,\beta)}(x)\), and the weights \(w_i\) are computed from orthogonality conditions.
			By tuning \(\alpha\) and \(\beta\), you can emphasize accuracy near one or both endpoints — useful for integrands with endpoint singularities or sharp features.
		</div>
<div class="layman">
<strong>Think of Gauss–Jacobi</strong> as a “custom‑fit” quadrature.
			If your function has tricky behavior near one or both ends of the interval, you can tell Gauss–Jacobi to send more surveyors there.
			It’s like assigning extra inspectors to the weak spots in a bridge.
		</div>
<h4>Gauss–Lobatto</h4>
<div class="formal">
			Gauss–Lobatto quadrature is a variant of Gaussian quadrature that <em>includes both endpoints</em> of the interval \([-1, 1]\) as nodes.
			It is exact for polynomials of degree up to \(2n-3\) (slightly less than Gauss–Legendre for the same \(n\)), but is valuable when endpoint values are known or required:
			

		\[
				\int_{-1}^1 f(x)\,dx \approx w_1 f(-1) + \sum_{i=2}^{n-1} w_i f(x_i) + w_n f(1)
			\]


			The interior nodes are the roots of \(P'_{n-1}(x)\), the derivative of the Legendre polynomial of degree \(n-1\).
			Gauss–Lobatto is popular in spectral methods and finite element analysis, where boundary values play a key role.
		</div>
<div class="layman">
<strong>Think of Gauss–Lobatto</strong> as a measuring crew that insists on checking the gates at both ends of a fence as well as the posts in between.
			It’s perfect when you care about what’s happening right at the boundaries, not just in the middle.
		</div>
</section>
<h3 id="section-3-2">Adaptive Quadrature</h3>
<div class="formal">
		  Adaptive methods allocate work where the integrand demands it. They compare a <em>coarse</em> estimate on an interval with a <em>refined</em> estimate obtained by splitting the interval; the difference acts as an <strong>error estimator</strong>. Subintervals whose error exceeds tolerance are split recursively until local targets are met, then results are aggregated.
		  <br/><br/>
<strong>Simpson’s rule (1D):</strong>
		  \[
			S[a,b] \;=\; \frac{b-a}{6}\Big(f(a) + 4 f\!\big(\tfrac{a+b}{2}\big) + f(b)\Big),
			\quad
			E_S[a,b] \;=\; -\,\frac{(b-a)^5}{2880}\,f^{(4)}(\xi)
		  \]
		  The adaptive estimator uses
		  \[
			\Delta_S \;=\; \big|S[a,b] - \big(S[a,c]+S[c,b]\big)\big| \;\approx\; \tfrac{1}{15}\,\big|S[a,b]-S[a,c]-S[c,b]\big|
		  \]
		  as a proxy for the local truncation error (with Richardson correction).
		  <br/><br/>
<strong>Gauss–Kronrod (GK 7–15):</strong> A 7‑point Gauss rule is <em>embedded</em> in a 15‑point Kronrod rule on the same nodes plus extras. Let \(I_G\) be the Gauss estimate and \(I_K\) the Kronrod estimate on a subinterval. An effective error indicator is
		  \[
			\Delta_{GK} \;=\; \big|I_K - I_G\big|,
		  \]
		  with robust variants that scale by norms of \(f\) to avoid under/over‑estimation. Subdivide where \(\Delta\) exceeds a local tolerance.
		</div>
<div class="pseudocode">
# Adaptive Simpson (recursive, with Richardson correction and depth cap)

function asr(f, a, b, fa, fm, fb, S, tol, depth, max_depth):
	c   = 0.5*(a + b)
	m1  = 0.5*(a + c)
	m2  = 0.5*(c + b)
	fm1 = f(m1)
	fm2 = f(m2)
	Sl  = (c - a)/6 * (fa + 4*fm1 + fm)
	Sr  = (b - c)/6 * (fm + 4*fm2 + fb)
	S2  = Sl + Sr
	err = |S2 - S|

	if (err &lt;= 15*tol) or (depth &gt;= max_depth):
		# Richardson correction improves order to O(h^5)
		return S2 + (S2 - S)/15

	# Recurse on halves with halved tolerances
	left  = asr(f, a, c, fa, fm1, fm, Sl, tol/2, depth+1, max_depth)
	right = asr(f, c, b, fm, fm2, fb, Sr, tol/2, depth+1, max_depth)
	return left + right

function adaptive_simpson(f, a, b, tol, max_depth=20):
	fa = f(a); fb = f(b); fm = f(0.5*(a+b))
	S  = (b - a)/6 * (fa + 4*fm + fb)
	return asr(f, a, b, fa, fm, fb, S, tol, 0, max_depth)
		</div>
<div class="pseudocode">
# Adaptive Gauss–Kronrod 15(7) (iterative, stack-based)

function gk15_eval(f, a, b):
	# standard abscissae x_k in [0,1] (map to [-1,1] then to [a,b])
	# and weights w_g (Gauss 7) and w_k (Kronrod 15)
	# returns (I_K, I_G, err_indic, f_norm)
	...
	return (IK, IG, |IK - IG|, norm_est)

function adaptive_gk(f, a, b, epsabs, epsrel, max_depth, limit_nodes):
	stack = [(a, b, 0)]
	total = 0
	used_nodes = 0
	while stack not empty:
		(u, v, depth) = stack.pop()
		(IK, IG, err, fnorm) = gk15_eval(f, u, v)
		used_nodes += 15
		tol_loc = max(epsabs, epsrel*abs(IK))
		if (err &lt;= tol_loc) or (depth &gt;= max_depth) or (used_nodes &gt;= limit_nodes):
			total += IK
		else:
			m = 0.5*(u + v)
			stack.push((m, v, depth+1))
			stack.push((u, m, depth+1))
	return total
		</div>
<div class="layman">
<strong>Adaptive quadrature</strong> is a smart spotlight. It shines brighter (uses more points) where the function is complicated and dims (saves points) where it’s smooth. Instead of wasting effort evenly, it pays attention and spends effort where it matters.
		</div>
<h4>Worked mini–examples</h4>
<div class="example-box">
<p class="example-title">1) Endpoint singularity (integrable): \(\int_0^1 x^{-1/2}\,dx = 2\)</p>
<p><strong>Why adapt?</strong> Near \(x=0\), \(f(x)\) spikes. Adaptive methods place many subintervals near 0 and few near 1, achieving high accuracy quickly.</p>
<p class="equation">\[
			\int_0^1 x^{-1/2}\,dx = \left[2 x^{1/2}\right]_0^1 = 2.
		  \]</p>
</div>
<div class="example-box">
<p class="example-title">2) Highly oscillatory: \(\int_0^1 \frac{\sin(50x)}{x}\,dx\)</p>
<p><strong>Why adapt?</strong> Oscillations demand more points where the wavelength is short. Adaptive splitting concentrates effort near regions of rapid variation; elsewhere, larger panels suffice.</p>
</div>
<div class="example-box">
<p class="example-title">3) Localized spikes: \(\int_{-3}^{3} e^{-x^2}\,dx = \sqrt{\pi}\)</p>
<p><strong>Why adapt?</strong> Most contribution comes from \(|x|\lesssim 2\). Adaptive schemes refine where \(e^{-x^2}\) is large and coarsen in the tails.</p>
</div>
<h4>Design details that matter</h4>
<ul>
<li><strong>Tolerance policy:</strong> Use absolute/relative blend \( \mathrm{tol}_{\text{loc}} = \max(\epsilon_{\text{abs}}, \epsilon_{\text{rel}}\cdot|I_{\text{loc}}|) \).</li>
<li><strong>Depth &amp; size caps:</strong> Set <code>max_depth</code> and minimum panel width to prevent infinite recursion on pathological \(f\).</li>
<li><strong>Reuse evaluations:</strong> Cache and pass <em>endpoint/midpoint</em> values down the recursion to avoid recomputing \(f\).</li>
<li><strong>Robustness:</strong> If <code>NaN</code>/<code>Inf</code> encountered, split aggressively or transform variables (e.g., endpoint maps for algebraic singularities).</li>
<li><strong>Performance:</strong> Prefer iterative stacks over deep recursion in Python to avoid overhead; vectorize batched point evaluations where possible.</li>
</ul>
<h4>Python reference implementations</h4>
<div class="pseudocode">
# Adaptive Simpson (robust, recursive with depth cap)
from __future__ import annotations
import math
from typing import Callable

def adaptive_simpson(f: Callable[[float], float],
					 a: float, b: float,
					 epsabs: float = 1e-10,
					 epsrel: float = 1e-8,
					 max_depth: int = 20) -&gt; float:
	"""Adaptive Simpson integration on [a,b]."""

	def S(fa, fm, fb, a, b):
		return (b - a) * (fa + 4*fm + fb) / 6.0

	def recurse(a, b, fa, fm, fb, Sab, tol, depth):
		c   = 0.5*(a + b)
		m1  = 0.5*(a + c)
		m2  = 0.5*(c + b)
		fm1 = f(m1)
		fm2 = f(m2)
		Sl  = S(fa, fm1, fm, a, c)
		Sr  = S(fm, fm2, fb, c, b)
		S2  = Sl + Sr
		err = abs(S2 - Sab)
		if (err &lt;= 15*tol) or (depth &gt;= max_depth):
			# Richardson correction
			return S2 + (S2 - Sab)/15.0
		# Split tolerance
		return (recurse(a, c, fa, fm1, fm, Sl, 0.5*tol, depth+1) +
				recurse(c, b, fm, fm2, fb, Sr, 0.5*tol, depth+1))

	fa = f(a); fb = f(b); fm = f(0.5*(a + b))
	Sab = S(fa, fm, fb, a, b)
	# Global tolerance target for the full interval
	tol0 = max(epsabs, epsrel*abs(Sab))
	return recurse(a, b, fa, fm, fb, Sab, tol0, 0)
		</div>
<div class="pseudocode">
# Adaptive Gauss–Kronrod 15(7) (iterative, QUADPACK-style)
from __future__ import annotations
import math
from typing import Callable, List, Tuple

# Kronrod 15 abscissae (nonnegative) and weights; Gauss-7 weights embedded
_xgk = [0.9914553711208126, 0.9491079123427585, 0.8648644233597691,
		0.7415311855993945, 0.5860872354676911, 0.4058451513773972,
		0.2077849550078985, 0.0]
_wg  = [0.1294849661688697, 0.2797053914892767,
		0.3818300505051189, 0.4179591836734694]  # Gauss 7 (nonnegative)
_wk  = [0.02293532201052922, 0.06309209262997855, 0.10479001032225019,
		0.14065325971552592, 0.16900472663926790, 0.19035057806478541,
		0.20443294007529889, 0.20948214108472783]  # Kronrod 15 (nonnegative)

def _gk15_eval(f: Callable[[float], float], a: float, b: float) -&gt; Tuple[float,float,float,float]:
	"""Evaluate GK15 on [a,b]. Return (IK, IG, err_est, fnorm)."""
	c = 0.5*(a + b)
	h = 0.5*(b - a)
	# function values at positive nodes and symmetry
	fk_sum = 0.0
	fg_sum = 0.0
	absf_sum = 0.0

	# center
	fc = f(c)
	fk_sum += _wk[-1] * fc
	absf_sum += _wk[-1] * abs(fc)

	# loop over the 7 positive Kronrod abscissae
	for j in range(7):
		x = _xgk[j]
		w_k = _wk[j]
		xh = h * x
		f1 = f(c - xh)
		f2 = f(c + xh)
		fk_sum += w_k * (f1 + f2)
		absf_sum += w_k * (abs(f1) + abs(f2))

		# Gauss nodes are a subset for j = 1,3,5,7 in xgk order:
		# their indices in our loop correspond to 1,3,5,6 (because of 0-based)
	# Gauss contribution (explicitly pick corresponding nodes)
	gauss_idx = [1, 3, 5]  # indexes into _xgk (0-based) for Gauss positive nodes
	for gi, wg in zip(gauss_idx, _wg[:-1]):
		x = _xgk[gi]
		xh = h * x
		f1 = f(c - xh)
		f2 = f(c + xh)
		fg_sum += wg * (f1 + f2)
	# Actually for Gauss-7, the center x=0 is a node with weight 0.417959..., covered by _wg[-1].

	IK = h * fk_sum
	IG = h * (_wg[-1]*fc + fg_sum - _wg[-1]*fc)  # fg_sum already accounts; keep clarity
	IG = h * (fg_sum + _wg[-1] * fc)

	# Error indicator and a mild norm-based safeguard
	err = abs(IK - IG)
	fnorm = h * absf_sum
	return IK, IG, err, fnorm

def quadgk(f: Callable[[float], float],
		   a: float, b: float,
		   epsabs: float = 1e-10,
		   epsrel: float = 1e-8,
		   max_depth: int = 20,
		   limit: int = 10_000) -&gt; float:
	"""Adaptive Gauss–Kronrod 15(7) on [a,b]."""
	# Work stack: (a, b, depth, last_IK, last_err) -- IK,err cached optional
	stack: List[Tuple[float,float,int]] = [(a, b, 0)]
	total = 0.0
	used = 0

	while stack:
		u, v, d = stack.pop()
		IK, IG, err, fnorm = _gk15_eval(f, u, v)
		used += 15
		tol = max(epsabs, epsrel * abs(IK))
		# Mild rescaling of err using norm (QUADPACK-like guard)
		if fnorm &gt; 0:
			err = min(err, 200 * math.ulp(1.0) * fnorm + err)

		if (err &lt;= tol) or (d &gt;= max_depth) or (used &gt;= limit):
			total += IK
		else:
			m = 0.5*(u + v)
			stack.append((m, v, d+1))
			stack.append((u, m, d+1))
	return total
		</div>
<h4>Quick sanity checks</h4>
<div class="pseudocode">
# 1) Endpoint singularity (integrable): ∫_0^1 x^{-1/2} dx = 2
f1 = lambda x: 1.0/math.sqrt(x) if x &gt; 0 else 0.0
print(adaptive_simpson(f1, 0.0, 1.0, 1e-10, 1e-10))
print(quadgk(f1, 0.0, 1.0, 1e-10, 1e-10))

# 2) Oscillatory: ∫_0^1 sin(50x)/x dx  (well-defined as Si(50))
f2 = lambda x: math.sin(50*x)/x if x != 0 else 50.0
print(adaptive_simpson(f2, 0.0, 1.0, 1e-8, 1e-8))
print(quadgk(f2, 0.0, 1.0, 1e-8, 1e-8))

# 3) Gaussian bell: ∫_{-3}^{3} e^{-x^2} dx ≈ √π erf(3) ≈ 1.77241...
f3 = lambda x: math.exp(-x*x)
print(adaptive_simpson(f3, -3.0, 3.0, 1e-12, 1e-10))
print(quadgk(f3, -3.0, 3.0, 1e-12, 1e-10))
		</div>
<div class="key-point">
<strong>Where each shines:</strong>
<ul>
<li><strong>Adaptive Simpson:</strong> Excellent for smooth, low-cost integrands; very few points per panel; simple, accurate error control.</li>
<li><strong>Adaptive Gauss–Kronrod:</strong> More function calls per panel but higher-order accuracy and a robust embedded error estimator; the go‑to general-purpose choice.</li>
</ul>
</div>
<div class="layman">
<strong>Adaptive quadrature</strong> is like a smart photographer who takes more pictures of the interesting parts of a scene and fewer of the boring parts. It focuses computational effort where the function is most "active" or changing rapidly.
        </div>
<h3 id="section-3-3">Clenshaw–Curtis and Tanh–Sinh Quadrature</h3>
<div class="formal">
<p>
<strong>Clenshaw–Curtis quadrature</strong> approximates
				

		\[
					I = \int_{-1}^1 f(x)\,dx
				\]


				by interpolating \(f(x)\) at <em>Chebyshev–Lobatto nodes</em>
				

		\[
					x_k = \cos\left(\frac{k\pi}{n}\right), \quad k = 0,1,\dots,n.
				\]


				These nodes cluster near \(\pm 1\), mitigating Runge’s phenomenon and capturing endpoint behaviour efficiently. The interpolant is expressed as a cosine series via the discrete cosine transform (DCT), and the integral is computed exactly for each cosine term. The resulting weights \(w_k\) can be precomputed, and because the nodes are <em>nested</em> (e.g., \(n=2^m\)), previously computed function values can be reused in adaptive schemes.
			</p>
<p>
				The quadrature formula is:
				

		\[
					I_n \;=\; \sum_{k=0}^n w_k\,f(x_k),
				\]


				where \(w_k\) are derived from the cosine coefficients \(a_j\) of the interpolant:
				

		\[
					a_j = \frac{2}{n} \sum_{k=0}^n f(x_k) \cos\left(\frac{j k \pi}{n}\right), \quad j=0,\dots,n.
				\]


				Integration term‑by‑term yields:
				

		\[
					w_k = \frac{2}{n} \left[ 1 - \sum_{\substack{j=1 \\ j\ \mathrm{even}}}^{n-1} \frac{2}{1-j^2} \cos\left(\frac{j k \pi}{n}\right) \right].
				\]


			</p>
<p>
<strong>When to use:</strong> Smooth functions on finite intervals, especially when endpoint behaviour matters or when you want to reuse samples for increasing \(n\). It is competitive with Gauss–Legendre in accuracy but cheaper to implement with FFT‑based weight generation.
			</p>
<hr/>
<p>
<strong>Tanh–Sinh quadrature</strong> (also called <em>double‑exponential</em> or DE quadrature) excels for integrals with endpoint singularities or infinite derivatives at the boundaries. It maps \([-1,1]\) to \((-\infty,\infty)\) via:
				

		\[
					x = \tanh\!\left( \frac{\pi}{2} \sinh t \right),
				\]


				which causes the integrand to decay <em>double‑exponentially</em> as \(|t| \to \infty\). The transformed integral becomes:
				

		\[
					I = \int_{-\infty}^{\infty} f\!\left( \tanh\!\left( \frac{\pi}{2} \sinh t \right) \right)
						\cdot \frac{\frac{\pi}{2} \cosh t}{\cosh^2\!\left( \frac{\pi}{2} \sinh t \right)} \, dt.
				\]


				The Jacobian factor
				

		\[
					\phi'(t) = \frac{\frac{\pi}{2} \cosh t}{\cosh^2\!\left( \frac{\pi}{2} \sinh t \right)}
				\]


				decays so rapidly that the trapezoidal rule in \(t\) converges at an astonishing rate — often doubling the number of correct digits with each halving of the step size.
			</p>
<p>
<strong>When to use:</strong> Integrals with algebraic or logarithmic endpoint singularities, or functions analytic in a strip around the real axis. Also effective for Cauchy principal value integrals after suitable symmetrisation.
			</p>
</div>
<div class="layman">
<p>
<strong>Clenshaw–Curtis</strong> is like taking measurements at points that naturally “bunch up” near the edges of your interval, where functions often misbehave. You then fit a smooth curve through those points using only cosines — which are easy to integrate — and add up the contributions.
			</p>
<p>
<strong>Tanh–Sinh</strong> is like stretching the ends of your interval to infinity in a clever way, so any nasty spikes at the edges get squashed flat. Once flattened, you can march along with evenly spaced steps and still capture all the detail, because the transformation makes the function fade away super‑fast.
			</p>
</div>
<h4>Why they are powerful</h4>
<ul>
<li><strong>Clenshaw–Curtis:</strong>
<ul>
<li>Nodes are <em>nested</em> — reuse old function values when increasing resolution.</li>
<li>FFT/DCT‑based weight computation is \(O(n \log n)\) vs \(O(n^2)\) for Gauss–Legendre.</li>
<li>Excellent for smooth functions; competitive accuracy with Gauss rules.</li>
</ul>
</li>
<li><strong>Tanh–Sinh:</strong>
<ul>
<li>Handles endpoint singularities without special‑case code.</li>
<li>Double‑exponential decay after transformation → extremely fast convergence.</li>
<li>Simple uniform‑step trapezoidal rule in transformed space.</li>
</ul>
</li>
</ul>
<h4>Mini‑examples</h4>
<div class="example-box">
<p class="example-title">Clenshaw–Curtis: \(\int_{-1}^1 \frac{1}{1+25x^2} dx\)</p>
<p>Runge’s function has steep changes near the ends. Chebyshev nodes cluster there, capturing the shape efficiently.</p>
</div>
<div class="example-box">
<p class="example-title">Tanh–Sinh: \(\int_{0}^1 x^{-1/2} \, dx\)</p>
<p>Square‑root singularity at \(x=0\) is flattened by the DE transform, so the trapezoidal rule converges rapidly without special handling.</p>
</div>
<div class="pseudocode">
import numpy as np
from math import pi
from scipy.fft import dct as _dct

def clenshaw_curtis(f, a: float, b: float, n: int = 128) -&gt; float:
"""
Clenshaw–Curtis integration on [a,b] with n panels (n+1 Chebyshev–Lobatto nodes).
"""
if b == a:
	return 0.0

# Chebyshev–Lobatto nodes on [-1,1]
k = np.arange(n + 1)
x_cheb = np.cos(pi * k / n)

# Map to [a,b]
xm = 0.5 * (b + a) + 0.5 * (b - a) * x_cheb

# Sample f
y = np.array([f(x) for x in xm], dtype=float)

# DCT-I (unnormalized) → Chebyshev coefficients
c = _dct(y, type=1, norm=None)
a_cheb = c / n
a_cheb[0] *= 0.5
a_cheb[-1] *= 0.5

# Integrate cosine series on [-1,1]:
# ∫ f ≈ 2*a_0 + sum_{even m&gt;=2} 2*a_m/(1 - m^2)
I_hat = 2.0 * a_cheb[0]
if n &gt;= 2:
	m_even = np.arange(2, n + 1, 2)
	I_hat += np.sum(2.0 * a_cheb[m_even] / (1.0 - m_even**2))

# Scale to [a,b]
return 0.5 * (b - a) * I_hat


def tanh_sinh(f, a: float, b: float, epsabs: float = 1e-12, epsrel: float = 1e-12,
		  h0: float = 0.5, max_refinements: int = 12, max_terms: int = 10_000) -&gt; float:
"""
Tanh–Sinh (double-exponential) quadrature on [a,b].
Handles endpoint singularities via x = (a+b)/2 + (b-a)/2 * tanh( (π/2) sinh t ).
"""
if b == a:
	return 0.0

def phi(t):
	u = 0.5 * pi * np.sinh(t)
	return 0.5 * (a + b) + 0.5 * (b - a) * np.tanh(u)

def phi_prime(t):
	u = 0.5 * pi * np.sinh(t)
	return 0.5 * (b - a) * (0.5 * pi * np.cosh(t)) / (np.cosh(u) ** 2)

def g(t):
	x = phi(t)
	return f(x) * phi_prime(t)

Ih_prev = None
h = h0

for _ in range(max_refinements + 1):
	# Trapezoidal on (-∞, ∞): h * [ g(0) + Σ_{k=1..K} (g(kh)+g(-kh)) ]
	S = g(0.0)
	k = 1
	tail_ok_runs = 0

	while k &lt;= max_terms:
		t = k * h
		gp = g(t)
		gm = g(-t)
		S += gp + gm

		# Tail criterion: consecutive small contributions
		tail_contrib = h * (abs(gp) + abs(gm))
		tol_here = max(epsabs, epsrel * abs(h * S))
		if tail_contrib &lt; 0.25 * tol_here:
			tail_ok_runs += 1
			if tail_ok_runs &gt;= 3:
				break
		else:
			tail_ok_runs = 0

		k += 1

	Ih = h * S

	if Ih_prev is not None:
		tol_refine = max(epsabs, epsrel * abs(Ih))
		if abs(Ih - Ih_prev) &lt;= tol_refine:
			return Ih

	Ih_prev = Ih
	h *= 0.5  # refine step and repeat

return Ih_prev if Ih_prev is not None else 0.0
		</div>
<div class="pseudocode">
import math

# 1) Smooth (Clenshaw–Curtis): Runge-type on [-1,1]
f_smooth = lambda x: 1.0 / (1.0 + 25.0*x*x)
print("Clenshaw–Curtis ≈", clenshaw_curtis(f_smooth, -1.0, 1.0, n=256))

# 2) Endpoint singularity (Tanh–Sinh): ∫_0^1 x^{-1/2} dx = 2
f_sing = lambda x: 1.0 / math.sqrt(x) if x &gt; 0 else 0.0
print("Tanh–Sinh (x^{-1/2}) ≈", tanh_sinh(f_sing, 0.0, 1.0, 1e-12, 1e-12))

# 3) Log singularity (Tanh–Sinh): ∫_0^1 log(x) dx = -1
f_log = lambda x: math.log(x) if x &gt; 0 else 0.0
print("Tanh–Sinh (log) ≈", tanh_sinh(f_log, 0.0, 1.0, 1e-12, 1e-12))
		</div>
<h3 id="section-3-4">Filon and Levin Quadrature for Oscillatory Integrals</h3>
<div class="formal">
  Standard quadrature methods struggle with highly oscillatory integrals like:
  \[
    \int_a^b f(x)\,e^{i\omega x}\,dx \quad \text{or} \quad \int_a^b f(x)\,\cos(\omega x)\,dx,
  \]
  especially when \(\omega\) is large. Filon and Levin quadrature are specialized techniques that exploit the structure of such integrals to achieve high accuracy without excessive sampling.

  <h4>Filon Quadrature</h4>
  Filon’s method is designed for integrals of the form:
  \[
    \int_a^b f(x)\,\cos(\omega x)\,dx \quad \text{or} \quad \int_a^b f(x)\,\sin(\omega x)\,dx,
  \]
  where \(f(x)\) is slowly varying and \(\omega\) is large. The idea is to:
  <ul>
<li>Approximate \(f(x)\) using a low-degree polynomial (typically quadratic).</li>
<li>Integrate the product of this polynomial with the oscillatory function <em>analytically</em>.</li>
</ul>
  This avoids the need to resolve every oscillation numerically and yields accurate results even for large \(\omega\).

  <h4>Levin Quadrature</h4>
  Levin’s method handles more general oscillatory integrals:
  \[
    \int_a^b f(x)\,e^{i\omega g(x)}\,dx,
  \]
  where \(g(x)\) is a known phase function and \(f(x)\) is smooth. It works by:
  <ul>
<li>Rewriting the integral as a differential equation using integration by parts.</li>
<li>Approximating the solution using basis functions (e.g., polynomials or splines).</li>
<li>Solving a linear system to compute the integral efficiently.</li>
</ul>
  Levin’s method is especially powerful when \(g(x)\) is nonlinear or when the oscillations are not uniform.

  <h5>When to Use</h5>
<ul>
<li><strong>Filon:</strong> Best for integrals with known sinusoidal oscillations and smooth amplitude functions.</li>
<li><strong>Levin:</strong> Best for general oscillatory integrals with nonlinear phase or variable frequency.</li>
</ul>
</div>
<div class="pseudocode">
# Filon Quadrature (simplified for cosine)
function filon_cos(f, a, b, omega):
    h = (b - a) / 2
    x0 = a
    x1 = (a + b) / 2
    x2 = b
    f0, f1, f2 = f(x0), f(x1), f(x2)

    theta = omega * h
    A = 2 * np.sin(theta) / theta
    B = 4 * np.sin(theta) / (theta**2)
    C = 2 * (np.sin(theta) - theta * np.cos(theta)) / (theta**3)

    return h * (A * f0 + B * f1 + C * f2)

# Levin Quadrature (conceptual)
function levin_integrate(f, g, a, b, omega):
    # Approximate f(x) using basis functions φ_j(x)
    # Solve for coefficients c_j such that:
    #   d/dx [c_j(x) * e^{iωg(x)}] ≈ f(x) * e^{iωg(x)}
    # Integrate analytically or numerically
    return sum_j c_j * ∫ φ_j(x) dx
</div>
<div class="layman">
<strong>Imagine trying to measure a rapidly vibrating string</strong> — if you use a ruler at fixed intervals, you’ll miss the fine details. Filon’s method says: “Let’s fit a smooth curve to the string’s shape and compute the area under the curve times the vibration.” Levin’s method says: “Let’s understand how the vibration behaves and solve the math from that angle.” Both are smarter than brute-force measuring every wiggle!
</div>
<h2 id="section-4">Building Custom Quadrature Rules</h2>
<div class="formal">
            The Golub-Welsch algorithm computes nodes and weights for Gaussian quadrature by solving an eigenvalue problem for the Jacobi matrix derived from orthogonal polynomial recurrence relations.
        </div>
<div class="pseudocode">
# Golub-Welsch Algorithm Summary
# 1) Get recurrence coefficients a_k, b_k for weight w(x)
# 2) Build Jacobi tridiagonal matrix J
# 3) Eigen-decompose J → eigenvalues x_i and eigenvectors v_i
# 4) Weights w_i = μ₀ * (v_i[0])²
        </div>
<h3 id="section-4-1">Generalized Gaussian Quadrature (Golub–Welsch)</h3>
<div class="formal">
		  Gaussian quadrature rules can be generated generically from the three-term recurrence of orthogonal polynomials with respect to a weight \(w(x)\) on a support \([a,b]\) (possibly infinite). The <em>Golub–Welsch</em> method constructs the symmetric tridiagonal <strong>Jacobi matrix</strong> from recurrence coefficients and obtains nodes as its eigenvalues and weights from the first components of its normalized eigenvectors:
		  <div class="equation">
			J = 
			\begin{bmatrix}
			  a_0 &amp; \sqrt{\beta_1} \\
			  \sqrt{\beta_1} &amp; a_1 &amp; \sqrt{\beta_2} \\
			  &amp; \ddots &amp; \ddots &amp; \ddots \\
			  &amp; &amp; \sqrt{\beta_{n-1}} &amp; a_{n-1}
			\end{bmatrix}, 
			<td>
			  \( \quad x_i = \lambda_i(J), \quad w_i = \mu_0\, (v_{1i})^2 \)
			</td>
</div>
		  Here \(a_k\) and \(\beta_k\) come from the orthonormal polynomial recurrence for \(w(x)\), \(\mu_0=\int w(x)\,dx\) is the zeroth moment, \(x_i\) are the nodes, and \(w_i\) are the weights. This single mechanism yields Gauss–Legendre, –Hermite, –Laguerre, –Jacobi, and more.
		</div>
<div class="layman">
<strong>Plain-speak:</strong> If you know how a function is “weighted” over an interval, you can build a tiny matrix whose eigenvalues tell you exactly where to sample, and how much each sample “counts.” That’s why Gaussian rules are so accurate with so few points.
		</div>
<div class="pseudocode">
<!-- Python implementation -->
# Generalized Gaussian quadrature via Golub–Welsch
# Families: Jacobi (incl. Legendre/Chebyshev), Laguerre, Hermite
# Utilities: map to finite intervals, integrate with given nodes/weights

import numpy as np
from math import gamma, sqrt, pi

def golub_welsch(a: np.ndarray, b: np.ndarray, mu0: float):
	"""
	Nodes/weights from orthonormal three-term recurrence:
	  p_{k+1}(x) = (x - a_k) p_k(x) - β_k p_{k-1}(x),  β_k&gt;0
	Inputs:
	  a: (n,)   diagonal coeffs a_k
	  b: (n-1,) positive coeffs β_k, k=1..n-1
	  mu0: zeroth moment ∫ w(x) dx over the support
	Returns:
	  x: (n,) nodes, eigenvalues of Jacobi matrix
	  w: (n,) weights, mu0 * (first-eigenvector-components)^2
	"""
	n = len(a)
	if len(b) != n - 1:
		raise ValueError("b must have length n-1.")
	J = np.zeros((n, n), dtype=float)
	np.fill_diagonal(J, a)
	off = np.sqrt(b)
	np.fill_diagonal(J[1:], off)
	np.fill_diagonal(J[:, 1:], off)
	x, V = np.linalg.eigh(J)
	w = mu0 * (V[0, :] ** 2)
	return x, w

# ---------- Families ----------

def gauss_jacobi(n: int, alpha: float, beta: float):
	"""
	On [-1,1], weight (1-x)^alpha (1+x)^beta, alpha,beta&gt;-1.
	Exactness: polynomials up to degree 2n-1 under this weight.
	"""
	if alpha &lt;= -1 or beta &lt;= -1:
		raise ValueError("alpha and beta must be &gt; -1.")
	k = np.arange(n, dtype=float)
	a = (beta**2 - alpha**2) / ((2*k + alpha + beta) * (2*k + alpha + beta + 2.0))
	km = np.arange(1, n, dtype=float)
	num = 4.0 * km * (km + alpha) * (km + beta) * (km + alpha + beta)
	den = (2*km + alpha + beta)**2 * (2*km + alpha + beta + 1.0) * (2*km + alpha + beta - 1.0)
	b = num / den
	mu0 = 2.0**(alpha + beta + 1.0) * gamma(alpha + 1.0) * gamma(beta + 1.0) / gamma(alpha + beta + 2.0)
	return golub_welsch(a, b, mu0)

def gauss_legendre(n: int):
	"""Legendre = Jacobi(alpha=0, beta=0) on [-1,1], weight 1."""
	return gauss_jacobi(n, 0.0, 0.0)

def gauss_hermite(n: int):
	"""Hermite, weight exp(-x^2) on (-inf, inf)."""
	a = np.zeros(n, dtype=float)
	b = 0.5 * np.arange(1, n, dtype=float) if n &gt; 1 else np.array([], dtype=float)
	mu0 = sqrt(pi)  # ∫ exp(-x^2) dx
	return golub_welsch(a, b, mu0)

def gauss_laguerre(n: int, alpha: float = 0.0):
	"""Laguerre, weight x^alpha * exp(-x) on [0, inf), alpha&gt;-1."""
	if alpha &lt;= -1:
		raise ValueError("alpha must be &gt; -1.")
	a = 2.0 * np.arange(n, dtype=float) + alpha + 1.0
	b = np.arange(1, n, dtype=float) * (np.arange(1, n, dtype=float) + alpha) if n &gt; 1 else np.array([], dtype=float)
	mu0 = gamma(alpha + 1.0)
	return golub_welsch(a, b, mu0)

# ---------- Utilities ----------

def map_to_interval(x: np.ndarray, w: np.ndarray, a: float, b: float):
	"""Affine map from [-1,1] to [a,b]."""
	xm = 0.5 * (b - a) * x + 0.5 * (a + b)
	wm = 0.5 * (b - a) * w
	return xm, wm

def integrate(f, x: np.ndarray, w: np.ndarray):
	"""Compute sum_i w_i f(x_i) for the given rule."""
	return np.dot(w, np.vectorize(f)(x))
		</div>
<h4>Usage Examples</h4>
<div class="pseudocode">
# 1) Legendre on [-1,1]: ∫ x^4 dx = 2/5, exact with n=3
x, w = gauss_legendre(3)
print(integrate(lambda t: t**4, x, w))  # -&gt; 0.4

# Map Legendre to [0,2] and integrate x^2
xg, wg = gauss_legendre(4)
xm, wm = map_to_interval(xg, wg, 0.0, 2.0)
print(np.dot(wm, xm**2))  # -&gt; 8/3 ≈ 2.666666...

# 2) Hermite: ∫_{-∞}^{∞} x^2 e^{-x^2} dx = √π / 2, exact with n=2
xh, wh = gauss_hermite(2)
print(integrate(lambda t: t**2, xh, wh))  # -&gt; sqrt(pi)/2

# 3) Laguerre (alpha=0): ∫_0^∞ x^2 e^{-x} dx = Γ(3) = 2, exact with n=2
xl, wl = gauss_laguerre(2, alpha=0.0)
print(integrate(lambda t: t**2, xl, wl))  # -&gt; 2.0

# 4) Jacobi as Chebyshev (first kind): α=β=-1/2 on [-1,1]
xj, wj = gauss_jacobi(5, alpha=-0.5, beta=-0.5)
print(integrate(lambda t: 1.0, xj, wj))  # -&gt; π
		</div>
<div class="key-point">
<strong>Notes &amp; Best Practices:</strong>
<ul>
<li><strong>Match the weight:</strong> These rules integrate \(f(x)\,w(x)\). For plain \(\int f(x)\,dx\) on finite intervals, use Legendre and map to \([a,b]\).</li>
<li><strong>Exactness:</strong> Degree \(2n-1\) (with respect to the weight). Increase \(n\) for non-polynomial \(f\).</li>
<li><strong>Stability:</strong> Use orthonormal recurrences and symmetric tridiagonals (as here) for robust nodes/weights.</li>
<li><strong>Performance:</strong> Vectorize \(f\) and reuse nodes/weights across integrals; precompute per interval/model.</li>
</ul>
</div>
<section class="chapter" id="custom">
<h3 id="section-4-2">Generalized (Non‑Gaussian) Quadrature Creation</h3>
<div class="formal">
			Gaussian quadrature is optimal when the weight function \( w(x) \) is one of the classical orthogonal polynomial families (Legendre, Hermite, Laguerre, Jacobi, etc.). 
			But in many applications — from heavy‑tailed finance models to skewed probability densities in physics — the natural weight function does not match these classical forms.
			<br/><br/>
<strong>Generalized quadrature</strong> refers to constructing nodes \(x_i\) and weights \(w_i\) for <em>any</em> given weight function \(w(x)\) on a domain \(\Omega\), so that:
			

		\[
				\int_{\Omega} f(x)\,w(x)\,dx \ \approx\ \sum_{i=1}^n w_i\,f(x_i)
			\]


			The process typically involves:
			<ol>
<li><strong>Defining the weight function:</strong> \(w(x)\) may be a PDF, a physical kernel, or any known shape.</li>
<li><strong>Choosing a node strategy:</strong> Quantile‑based placement (via inverse CDF), clustering where \(w(x)\) is large, or solving orthogonal polynomial recurrences for \(w(x)\).</li>
<li><strong>Computing weights:</strong> Integrating \(w(x)\) over Voronoi cells around each node, or solving a moment‑matching system so the rule is exact for a chosen basis (e.g., polynomials up to degree \(m\)).</li>
<li><strong>Normalizing:</strong> If \(w(x)\) is a PDF, ensure \(\sum w_i = 1\).</li>
</ol>
			This approach generalizes the Golub–Welsch idea: instead of using pre‑tabulated recurrence coefficients for classical weights, you derive them numerically for your custom \(w(x)\), or bypass them entirely with direct bin‑integration.
		</div>
<div class="layman">
<strong>Think of it like designing a custom fishing net:</strong> 
			Gaussian quadrature nets are pre‑made for certain fish (functions) in certain waters (weight functions). 
			But if you’re fishing in a strange new sea — say, one full of heavy‑tailed monsters or oddly‑shaped schools — you weave your own net to match the catch. 
			You decide where the knots (nodes) go and how big each mesh (weight) should be, so you scoop up the most important parts without wasting effort.
		</div>
<div class="key-point">
<strong>Key Insight:</strong> 
			Generalized quadrature lets you integrate efficiently under <em>any</em> weight function — from Student‑t to Beta to exotic kernels — by matching the rule to the shape of the problem. 
			This is especially powerful in finance, physics, and statistics, where the “natural” weight is rarely one of the textbook cases.
		</div>
<h4>Example: Custom Quadrature for a Student‑t Distribution
<div class="formal">
			Sometimes the integrand’s natural weight function is not one of the classical Gaussian families. 
			In such cases, we can build a <em>custom quadrature rule</em> tuned to that weight. 
			For example, suppose our integration is naturally weighted by the PDF of a Student‑t distribution with 
			\(\nu\) degrees of freedom:
			

		\[
				w(x) = f_{\text{Student-t},\nu}(x)
			\]


			We can:
			<ol>
<li>Select <strong>nodes</strong> as quantiles of the distribution (via its inverse CDF), avoiding extreme tails.</li>
<li>Compute <strong>weights</strong> by integrating the PDF over small bins around each node.</li>
<li>Normalise the weights so they sum to 1 (if \(w\) is a PDF).</li>
</ol>
			This produces a quadrature rule that concentrates effort where the distribution has most of its mass, 
			improving efficiency for heavy‑tailed or skewed weights.
		</div>
<div class="pseudocode">
# Custom Quadrature for any distribution with PDF and PPF
function custom_quadrature(pdf, ppf, N, tail_cut):
	percentiles = linspace(tail_cut, 1 - tail_cut, N)
	nodes = ppf(percentiles)
	weights = zeros(N)
	for i in 0..N-1:
		left  = midpoint(nodes[i], nodes[i-1]) if i &gt; 0 else -∞
		right = midpoint(nodes[i], nodes[i+1]) if i &lt; N-1 else +∞
		weights[i] = integrate(pdf, left, right)
	weights /= sum(weights)
	return nodes, weights

# Example: Student-t with ν=5
pdf = lambda x: t_pdf(x, nu=5)
ppf = lambda p: t_ppf(p, nu=5)
nodes, weights = custom_quadrature(pdf, ppf, N=50, tail_cut=0.05)
		</div>
<div class="layman">
<strong>Think of it like taste‑testing soup:</strong> 
			If you know where the flavour is concentrated — the spice pockets, the chunky bits — 
			you don’t take random sips. You sample exactly from those spots and weigh them according to 
			how much they contribute to the whole pot. 
			That’s what a custom quadrature rule does: it samples where the action is.
		</div>
<div class="key-point">
<strong>Key Insight:</strong> Custom quadrature rules let you integrate efficiently under 
			<em>any</em> distribution — Student‑t, Beta, Gamma, skew‑normal — by matching the nodes and weights 
			to the shape of the weight function.
		</div>
<section class="chapter" id="custom">
<!-- ... your existing content ... -->
<h4 id="custom-quantile">Quantile-based Custom Quadrature (Lognormal)</h4>
<pre><code class="language-pseudo">
# Goal: approximate E[f(S)] where S ~ Lognormal(μ, σ) without Monte Carlo.
# Idea: Use the quantile transform S = F^{-1}(p), p ~ Uniform(0,1).
# Then: E[f(S)] = ∫_0^1 f(F^{-1}(p)) dp  ≈  Σ w_i f(F^{-1}(p_i))

INPUT:
  μ, σ              # log-space mean &amp; vol (Black–Scholes: μ = ln(S0) + (r - q - ½σ²)T, σ = σ√T)
  N                 # number of nodes (e.g., 100)
  tail_cut ∈ (0,1)  # avoid extreme tails, e.g., 1e-4

CONSTRUCT NODES (uniform in probability):
  p_i = linspace(tail_cut, 1 - tail_cut, N)
  x_i = Φ^{-1}(p_i)                # standard normal quantile (any accurate approx)
  S_i = exp(μ + σ x_i)             # lognormal nodes

WEIGHTS (on p-space, sum to ~1):
  For i = 0..N-1:
    if i == 0:     w_i = (p_1 - p_0)/2
    elif i == N-1: w_i = (p_{N-1} - p_{N-2})/2
    else:          w_i = (p_{i+1} - p_{i-1})/2
  Normalize: w_i = w_i / Σ_j w_j

ESTIMATE EXPECTATION:
  E[f(S)] ≈ Σ_i w_i * f(S_i)

NOTES:
- This targets the mass where it lives (efficient for skewed/heavy tails).
- No PDF factor is needed because dp “is” the weight measure.
- Replace Lognormal by any distribution with PPF (quantile) available numerically.
  </code></pre>
<figure class="inline-svg">
<svg aria-labelledby="logn-quad-title" role="img" viewbox="0 0 1100 320">
<title id="logn-quad-title">Custom Quantile Quadrature Nodes (Lognormal, N=100)</title>
<desc>Nodes S_i along the axis; marker size ∝ quantile weight Δp.</desc>
<rect fill="white" height="320" width="1100" x="0" y="0"></rect>
<g transform="translate(60,20)">
<line stroke="#bbb" stroke-dasharray="6,4" x1="0" x2="980" y1="140" y2="140"></line>
<text font-size="14" text-anchor="middle" x="490" y="280">Node Sᵢ (log scale shown visually)</text>
<text font-size="14" text-anchor="end" transform="rotate(-90,-30,10)" x="-30" y="10">Quantile rule</text>
<!-- illustrative log-scale axis ticks -->
<g fill="#666" font-size="12">
<text x="0" y="170">~20</text>
<text x="245" y="170">~60</text>
<text x="490" y="170">~100</text>
<text x="735" y="170">~160</text>
<text x="960" y="170">~220</text>
</g>
<!-- Nodes: left = small S (thin weights), center ~ S0 (larger), right tail again thin -->
<g fill="#2062b8" fill-opacity="0.85" stroke="#133a6d" stroke-width="0.4">
<!-- Representative subset (for full resolution, use Appendix Python) -->
<!-- left tail -->
<circle cx="10" cy="140" r="2.0"></circle><circle cx="20" cy="140" r="2.2"></circle><circle cx="32" cy="140" r="2.5"></circle>
<circle cx="46" cy="140" r="2.9"></circle><circle cx="62" cy="140" r="3.3"></circle><circle cx="80" cy="140" r="3.7"></circle>
<circle cx="100" cy="140" r="4.1"></circle><circle cx="122" cy="140" r="4.5"></circle><circle cx="146" cy="140" r="4.9"></circle>
<!-- around median -->
<circle cx="200" cy="140" r="5.6"></circle><circle cx="245" cy="140" r="6.1"></circle><circle cx="300" cy="140" r="6.6"></circle>
<circle cx="360" cy="140" r="6.9"></circle><circle cx="420" cy="140" r="7.1"></circle><circle cx="490" cy="140" r="7.2"></circle>
<circle cx="560" cy="140" r="7.1"></circle><circle cx="620" cy="140" r="6.9"></circle><circle cx="680" cy="140" r="6.6"></circle>
<circle cx="735" cy="140" r="6.1"></circle><circle cx="780" cy="140" r="5.6"></circle>
<!-- right tail -->
<circle cx="822" cy="140" r="4.9"></circle><circle cx="858" cy="140" r="4.5"></circle><circle cx="890" cy="140" r="4.1"></circle>
<circle cx="918" cy="140" r="3.7"></circle><circle cx="942" cy="140" r="3.3"></circle><circle cx="962" cy="140" r="2.9"></circle>
<circle cx="978" cy="140" r="2.5"></circle>
</g>
<g font-size="13" transform="translate(10,210)">
<rect fill="#fff" height="52" rx="8" ry="8" stroke="#ddd" width="300" x="0" y="0"></rect>
<circle cx="18" cy="18" fill="#2062b8" r="6" stroke="#133a6d"></circle>
<text x="36" y="22">Quantile nodes Sᵢ with weights wᵢ ≈ Δp</text>
<text x="36" y="42">Example: μ = ln(100), σ = 0.2, N = 100</text>
</g>
</g>
</svg>
<figcaption>Custom quantile rule for Lognormal: take evenly spaced probabilities pᵢ, map through the PPF to nodes Sᵢ, and use Δp weights.</figcaption>
</figure>
</section>
<div class="layman">
<strong>Creating your own quadrature rule</strong> is like designing a custom measuring tool for a specific shape. If you know something about what you're measuring (like where it's bumpy or flat), you can place your measurement points strategically to get accurate results with fewer measurements.
        </div>
<div class="key-point">
<strong>Key Insight:</strong> More knowledge about the function → fewer nodes needed. Clever node placement → higher accuracy with same computation.
        </div>
<h2 id="section-5">Modern Quadrature Architectures</h2>
<div class="formal">
            Contemporary quadrature implementations leverage parallel computing architectures:
            <ul>
<li>GPU acceleration for evaluating integrands at multiple nodes simultaneously</li>
<li>Multi-core processing for dividing integration domains</li>
<li>Distributed computing for high-dimensional integrals</li>
</ul>
</div>
<div class="layman">
<strong>Modern quadrature</strong> is like having a team of people measure a large field together instead of one person doing all the work. GPUs are like having hundreds of helpers working in parallel, making the process much faster for complex problems.
        </div>
<h4>GPU-Accelerated Quadrature with CUDA</h4>
<div class="formal">
		  The core idea is to map each node evaluation \(f(x_i)\) to an independent GPU thread. This is ideal for embarrassingly parallel quadrature where nodes do not depend on each other. The GPU computes values in parallel; the host performs the weighted reduction.
		</div>
<div class="pseudocode">
# =========================
# CUDA C++ (single GPU)
# =========================

__device__ double f(double x) {
	// device-compatible integrand
	return ...;
}

__global__ void eval_integrand(const double* __restrict__ nodes,
							   double* __restrict__ out,
							   int N) {
	int i = blockIdx.x * blockDim.x + threadIdx.x;
	if (i &lt; N) out[i] = f(nodes[i]);
}

double quad_gpu(const double* nodes_h, const double* weights_h, int N) {
	// 1) Allocate on device
	double *nodes_d, *vals_d;
	cudaMalloc(&amp;nodes_d, N * sizeof(double));
	cudaMalloc(&amp;vals_d,  N * sizeof(double));

	// 2) H2D copy
	cudaMemcpy(nodes_d, nodes_h, N * sizeof(double), cudaMemcpyHostToDevice);

	// 3) Launch kernel
	int TPB = 256;
	int BPG = (N + TPB - 1) / TPB;
	eval_integrand&lt;&lt;<bpg, tpb="">&gt;&gt;(nodes_d, vals_d, N);

	// 4) D2H copy and weighted sum on host
	std::vector<double> vals(N);
	cudaMemcpy(vals.data(), vals_d, N * sizeof(double), cudaMemcpyDeviceToHost);

	double sum = 0.0;
	for (int i = 0; i &lt; N; ++i) sum += weights_h[i] * vals[i];

	cudaFree(nodes_d); cudaFree(vals_d);
	return sum;
}
		</double></bpg,></div>
<div class="pseudocode">
# =========================
# Python (Numba CUDA, single GPU)
# =========================
from numba import cuda
import numpy as np

@cuda.jit(device=True)
def f(x):
	# device-compatible integrand
	return ...

@cuda.jit
def eval_integrand(nodes, out):
	i = cuda.grid(1)
	if i &lt; nodes.size:
		out[i] = f(nodes[i])

def quad_gpu(nodes, weights):
	N = len(nodes)
	d_nodes  = cuda.to_device(nodes)
	d_vals   = cuda.device_array(N, dtype=np.float64)

	TPB = 256
	BPG = (N + TPB - 1) // TPB
	eval_integrand[BPG, TPB](d_nodes, d_vals)

	vals = d_vals.copy_to_host()
	return np.dot(weights, vals)
		</div>
<div class="layman">
<strong>Analogy:</strong> Think of a giant kitchen with hundreds of chefs. Each chef cooks one dish (node) at the same time. You just plate the dishes (sum with weights) and serve the final result.
		</div>
<div class="section-divider"></div>
<h4>Multi‑GPU Quadrature for Huge Jobs</h4>
<div class="formal">
		  When a single GPU is not enough (memory or throughput), split the node set into shards and assign each shard to a different GPU. Each GPU computes its partial weighted sum; the host aggregates the partials. Keep shards balanced to maximize utilization.
		</div>
<div class="pseudocode">
# =========================
# CUDA C++ (multi‑GPU, single node)
# =========================

double quad_multi_gpu(const double* nodes, const double* weights,
					  long long N, int num_gpus) {
	long long chunk = (N + num_gpus - 1) / num_gpus;
	std::vector<double> partial(num_gpus, 0.0);

	parallel_for (int g = 0; g &lt; num_gpus; ++g) {
		cudaSetDevice(g);
		long long start = g * chunk;
		long long end   = std::min(start + chunk, N);
		long long M     = std::max(0LL, end - start);
		if (M == 0) { partial[g] = 0.0; continue; }

		// Device alloc
		double *d_nodes, *d_vals;
		cudaMalloc(&amp;d_nodes, M * sizeof(double));
		cudaMalloc(&amp;d_vals,  M * sizeof(double));

		// Copy shard of nodes
		cudaMemcpy(d_nodes, nodes + start, M * sizeof(double), cudaMemcpyHostToDevice);

		// Launch kernel
		int TPB = 256, BPG = (int)((M + TPB - 1) / TPB);
		eval_integrand&lt;&lt;<bpg, tpb="">&gt;&gt;(d_nodes, d_vals, (int)M);

		// Copy back and reduce on host
		std::vector<double> vals(M);
		cudaMemcpy(vals.data(), d_vals, M * sizeof(double), cudaMemcpyDeviceToHost);

		double s = 0.0;
		for (long long i = 0; i &lt; M; ++i) s += weights[start + i] * vals[i];
		partial[g] = s;

		cudaFree(d_nodes); cudaFree(d_vals);
	}

	// Host aggregate
	double total = 0.0;
	for (int g = 0; g &lt; num_gpus; ++g) total += partial[g];
	return total;
}
		</double></bpg,></double></div>
<div class="pseudocode">
# =========================
# Python (Numba CUDA, multi‑GPU, single node)
# =========================
from numba import cuda
import numpy as np

@cuda.jit
def eval_integrand(nodes, out):
	i = cuda.grid(1)
	if i &lt; nodes.size:
		out[i] = f(nodes[i])

def quad_multi_gpu(nodes, weights, num_gpus):
	N = len(nodes)
	chunk = int(np.ceil(N / num_gpus))
	partials = []

	for g in range(num_gpus):
		cuda.select_device(g)
		start, end = g*chunk, min((g+1)*chunk, N)
		if end &lt;= start: partials.append(0.0); continue

		nodes_g = nodes[start:end]
		weights_g = weights[start:end]

		d_nodes  = cuda.to_device(nodes_g)
		d_vals   = cuda.device_array(len(nodes_g), dtype=np.float64)

		TPB = 256
		BPG = (len(nodes_g) + TPB - 1) // TPB
		eval_integrand[BPG, TPB](d_nodes, d_vals)

		vals = d_vals.copy_to_host()
		partials.append(np.dot(weights_g, vals))

	return float(np.sum(partials))
		</div>
<div class="key-point">
<strong>Best practices:</strong>
<ul>
<li><strong>Shard evenly:</strong> Balance work to avoid idle GPUs.</li>
<li><strong>Batch large jobs:</strong> Process nodes in tiles that fit device memory.</li>
<li><strong>Overlap transfers:</strong> Use pinned memory, cudaMemcpyAsync, and streams to overlap copy/compute.</li>
<li><strong>Stable reduction:</strong> For very large N, do hierarchical Kahan or pairwise sums.</li>
</ul>
</div>
<div class="section-divider"></div>
<h4>Let's Get Colossal: Multi‑Node, Multi‑GPU with MPI + CUDA</h4>
<div class="formal">
		  Scale across a cluster by assigning each Message Passing Interface (MPI) rank a GPU (1 rank ↔ 1 GPU). 
		  Partition the workload globally across ranks so that each GPU processes a distinct subset of data or quadrature nodes. 
		  Each rank computes a partial weighted sum locally on its GPU, and then participates in a collective 
		  <em>all-reduce</em> operation to combine these partial results into the final integral. 
		  For systems with multiple GPUs per node, use NCCL (NVIDIA Collective Communications Library) to perform 
		  fast intra-node reductions, followed by MPI for inter-node reductions across the cluster. 
		  This hierarchical strategy minimizes communication overhead and scales efficiently from a single node 
		  to large multi-node GPU clusters.
		</div>
<div class="layman">
		  Imagine you have many calculators (GPUs), each held by a different person (MPI ranks) in a big team spread 
		  across different rooms (nodes). Each person is given a piece of the problem to solve, and they use their 
		  calculator to work out their share. When everyone is done, they all shout out their answers into a system 
		  that adds everything together (the all-reduce). If a room has multiple people with calculators, they first 
		  whisper to each other to combine their results quickly (NCCL within a node), and then one representative 
		  from the room shares the total with the rest of the team across all rooms (MPI across nodes). 
		  This way, the work gets done much faster than if only one person did all the calculations.
		</div>
<div class="pseudocode">
# ===========================================
# MPI + CUDA (C++-style design, 1 rank ↔ 1 GPU)
# ===========================================

int main(int argc, char** argv) {
	MPI_Init(&amp;argc, &amp;argv);
	int world_size, world_rank;
	MPI_Comm_size(MPI_COMM_WORLD, &amp;world_size);
	MPI_Comm_rank(MPI_COMM_WORLD, &amp;world_rank);

	// Map rank to local GPU (e.g., via local_rank env)
	int local_rank = get_local_rank();           // implementation-specific
	cudaSetDevice(local_rank);

	// Global partition
	long long N = total_nodes();
	long long chunk = (N + world_size - 1) / world_size;
	long long start = world_rank * chunk;
	long long end   = std::min(start + chunk, N);
	long long M     = std::max(0LL, end - start);

	// Tile loop for out-of-core workloads
	double partial_sum = 0.0;
	for (long long off = 0; off &lt; M; off += TILE) {
		long long m = std::min((long long)TILE, M - off);

		// Stage tile nodes/weights (H2D async with pinned host memory)
		cudaMemcpyAsync(d_nodes, h_nodes + start + off, m*sizeof(double), H2D, stream);
		eval_integrand&lt;&lt;<grid(m), block="">&gt;&gt;(d_nodes, d_vals, (int)m);
		cudaMemcpyAsync(h_vals, d_vals, m*sizeof(double), D2H, stream);
		cudaStreamSynchronize(stream);

		// Host-side weighted sum (or device-side reduction kernel)
		partial_sum += weighted_sum(h_vals, h_weights + start + off, m);
	}

	// Global reduction
	double total = 0.0;
	MPI_Allreduce(&amp;partial_sum, &amp;total, 1, MPI_DOUBLE, MPI_SUM, MPI_COMM_WORLD);

	// Final result on all ranks
	if (world_rank == 0) print(total);

	MPI_Finalize();
}
		</grid(m),></div>
<div class="pseudocode">
# ===========================================
# Python (mpi4py + Numba CUDA), 1 rank ↔ 1 GPU
# ===========================================
from mpi4py import MPI
from numba import cuda
import numpy as np

@cuda.jit
def eval_integrand(nodes, out):
	i = cuda.grid(1)
	if i &lt; nodes.size:
		out[i] = f(nodes[i])

def quad_mpi_cuda(all_nodes, all_weights):
	comm = MPI.COMM_WORLD
	size = comm.Get_size()
	rank = comm.Get_rank()

	# Rank→GPU mapping (e.g., modulo GPUs per node)
	gpu_id = rank % cuda.gpus.lst.size
	cuda.select_device(gpu_id)

	N = len(all_nodes)
	chunk = (N + size - 1) // size
	start, end = rank*chunk, min((rank+1)*chunk, N)

	partial = 0.0
	TILE = 1_000_000  # tune to memory
	for off in range(start, end, TILE):
		hi = min(off + TILE, end)
		nodes = all_nodes[off:hi]
		weights = all_weights[off:hi]

		d_nodes = cuda.to_device(nodes)
		d_vals  = cuda.device_array(nodes.size, dtype=np.float64)

		TPB = 256
		BPG = (nodes.size + TPB - 1)//TPB
		eval_integrand[BPG, TPB](d_nodes, d_vals)
		vals = d_vals.copy_to_host()

		partial += float(np.dot(weights, vals))

	total = comm.allreduce(partial, op=MPI.SUM)
	return total
		</div>
<div class="key-point">
<strong>Conceptual tips for Colossal Titans:</strong>
<ul>
<li><strong>Ranks and GPUs:</strong> 
			  Use one MPI (<em>Message Passing Interface</em>) rank per GPU (<em>Graphics Processing Unit</em>); 
			  bind ranks to GPUs deterministically (e.g., <code>local_rank</code>, which is the rank ID within a node).
			</li>
<li><strong>Two-tier reduction:</strong> 
			  Intra-node (NCCL = <em>NVIDIA Collective Communications Library</em> all-reduce) 
			  then inter-node (MPI Allreduce) for efficiency.
			</li>
<li><strong>Overlap:</strong> 
			  Use pinned host buffers (<em>page-locked host memory for faster transfer</em>) + multiple CUDA streams 
			  (<em>asynchronous command queues on the GPU</em>) to overlap H2D (<em>host-to-device</em>) 
			  / D2H (<em>device-to-host</em>) transfers with compute.
			</li>
<li><strong>Chunking:</strong> 
			  Tile nodes when they exceed device memory; pipeline tiles per stream to keep GPUs busy.
			</li>
<li><strong>Numerical care:</strong> 
			  Use pairwise summation (<em>tree-like addition order</em>) or Kahan summation (<em>compensated arithmetic to reduce round-off error</em>) 
			  for large reductions; validate against CPU (<em>Central Processing Unit</em>) baselines.
			</li>
<li><strong>Load balance:</strong> 
			  If f(x) cost varies across x, use work-stealing (<em>idle ranks grab extra work</em>) or dynamic shards (<em>divide workload adaptively</em>) 
			  to avoid stragglers.
			</li>
</ul>
</div>
<!-- MPI + GPU SVI diagram (pure HTML + inline SVG) -->
<figure aria-labelledby="mpi-gpu-title" style="max-width:900px;margin:1rem auto;">
<svg aria-describedby="mpi-gpu-desc" height="auto" preserveaspectratio="xMinYMin meet" role="img" style="display:block;overflow:visible;max-width:100%" viewbox="0 0 900 420" width="100%">
<title id="mpi-gpu-title">MPI with GPUs — NCCL intra-node and MPI inter-node reductions</title>
<desc id="mpi-gpu-desc">
			  Two compute nodes each with two GPUs. Blue double arrows show fast intra-node NCCL reductions.
			  A red arrow between node centers shows MPI inter-node reduction. Each GPU box is labeled with GPU and MPI rank.
			</desc>
<!-- Styles -->
<style>
			  .node-box { fill: none; stroke: #2b2b2b; stroke-width: 3; rx: 8; }
			  .gpu { rx:6; fill:#4DB6AC; stroke:#2b7a6f; stroke-width:1.5; }
			  .gpu-text { fill:#ffffff; font-family: Inter, Arial, sans-serif; font-size:13px; font-weight:600; pointer-events:none; }
			  .label { fill:#222; font-family: Inter, Arial, sans-serif; font-size:16px; font-weight:700; }
			  .sub-label { fill:#222; font-family: Inter, Arial, sans-serif; font-size:13px; }
			  .ncc lline, .nccarrow { stroke:#1976D2; stroke-width:3; }
			  .mpi-line { stroke:#D32F2F; stroke-width:4; stroke-linecap:round; }
			  .note { fill:#0d0d0d; font-family: Inter, Arial, sans-serif; font-size:12px; }
			</style>
<!-- Node 1 (left) -->
<g id="node1">
<rect class="node-box" height="320" width="380" x="40" y="40"></rect>
<text class="label" text-anchor="middle" x="230" y="68">Node 1</text>
<!-- GPUs inside Node 1 -->
<g transform="translate(70,110)">
<rect class="gpu" height="70" width="150" x="0" y="0"></rect>
<text class="gpu-text" text-anchor="middle" x="75" y="24">GPU 0</text>
<text class="gpu-text" style="font-weight:500;font-size:12px;" text-anchor="middle" x="75" y="48">Rank 0</text>
</g>
<g transform="translate(220,110)">
<rect class="gpu" height="70" width="150" x="0" y="0"></rect>
<text class="gpu-text" text-anchor="middle" x="75" y="24">GPU 1</text>
<text class="gpu-text" style="font-weight:500;font-size:12px;" text-anchor="middle" x="75" y="48">Rank 1</text>
</g>
<!-- NCCL intra-node double arrow (vertical) -->
<g transform="translate(170,200)">
<!-- line up -->
<line stroke="#1976D2" stroke-linecap="round" stroke-width="6" x1="-60" x2="-60" y1="-40" y2="40"></line>
<!-- arrowheads -->
<path d="M -60 -40 l -8 -12 l 12 0 z" fill="#1976D2"></path>
<path d="M -60 40 l 8 12 l -12 0 z" fill="#1976D2"></path>
<text class="note" x="-120" y="6">NCCL (intra-node)</text>
</g>
</g>
<!-- Node 2 (right) -->
<g id="node2">
<rect class="node-box" height="320" width="380" x="480" y="40"></rect>
<text class="label" text-anchor="middle" x="670" y="68">Node 2</text>
<!-- GPUs inside Node 2 -->
<g transform="translate(510,110)">
<rect class="gpu" height="70" width="150" x="0" y="0"></rect>
<text class="gpu-text" text-anchor="middle" x="75" y="24">GPU 2</text>
<text class="gpu-text" style="font-weight:500;font-size:12px;" text-anchor="middle" x="75" y="48">Rank 2</text>
</g>
<g transform="translate(660,110)">
<rect class="gpu" height="70" width="150" x="0" y="0"></rect>
<text class="gpu-text" text-anchor="middle" x="75" y="24">GPU 3</text>
<text class="gpu-text" style="font-weight:500;font-size:12px;" text-anchor="middle" x="75" y="48">Rank 3</text>
</g>
<!-- NCCL intra-node double arrow (vertical) -->
<g transform="translate(590,200)">
<!-- line up -->
<line stroke="#1976D2" stroke-linecap="round" stroke-width="6" x1="-60" x2="-60" y1="-40" y2="40"></line>
<!-- arrowheads -->
<path d="M -60 -40 l -8 -12 l 12 0 z" fill="#1976D2"></path>
<path d="M -60 40 l 8 12 l -12 0 z" fill="#1976D2"></path>
<text class="note" x="-120" y="6">NCCL (intra-node)</text>
</g>
</g>
<!-- MPI inter-node arrow (horizontal) -->
<defs>
<marker id="arrowhead" markerheight="8" markerwidth="10" orient="auto" refx="9" refy="4">
<path d="M 0 0 L 9 4 L 0 8 z" fill="#D32F2F"></path>
</marker>
</defs>
<g transform="translate(0,0)">
<!-- faint center connector line -->
<line class="mpi-line" marker-end="url(#arrowhead)" marker-start="url(#arrowhead)" x1="420" x2="480" y1="210" y2="210"></line>
<text class="note" text-anchor="middle" x="450" y="190">MPI (inter-node)</text>
</g>
<!-- Final aggregation / All-Reduce label -->
<g>
<rect fill="#fff7e6" height="80" rx="8" stroke="#b36b00" stroke-width="1.8" width="140" x="380" y="260"></rect>
<text class="label" style="font-size:14px" text-anchor="middle" x="450" y="292">All-Reduce</text>
<text class="sub-label" text-anchor="middle" x="450" y="315">Combine partial weighted sums</text>
</g>
<!-- Small legend -->
<g transform="translate(40,370)">
<rect fill="#f7f7f7" height="38" rx="6" stroke="#e6e6e6" width="820" x="0" y="-6"></rect>
<text class="note" x="12" y="18">Legend:</text>
<rect fill="#4DB6AC" height="12" stroke="#2b7a6f" width="18" x="90" y="5"></rect>
<text class="note" x="118" y="18">GPU (MPI rank)</text>
<line stroke="#1976D2" stroke-linecap="round" stroke-width="6" x1="260" x2="280" y1="12" y2="12"></line>
<text class="note" x="292" y="18">NCCL (intra-node reduction)</text>
<line stroke="#D32F2F" stroke-linecap="round" stroke-width="6" x1="472" x2="512" y1="12" y2="12"></line>
<text class="note" x="524" y="18">MPI (inter-node reduction)</text>
</g>
</svg>
<figcaption style="font-family:Inter, Arial, sans-serif; font-size:13px; color:#333; margin-top:6px;">
			Diagram: GPUs are mapped 1:1 with MPI ranks; intra-node reductions use NCCL for speed and an inter-node MPI all-reduce
			completes the global aggregation. Drop this SVG into your HTML — it is scalable and editable.
		  </figcaption>
</figure>
<div class="layman">
<strong>Analogy:</strong> Imagine mapping an entire continent with many fleets of drones. Each ship (node) carries drones (GPUs), each drone scans a region (shard). Ships first merge their maps locally, then all ships merge their maps together into one seamless atlas.
		</div>
<h2 id="section-6">Past infinity, past certainty — into the (purely conceptual) quantum fold of Quadrature</h2>
<div class="note" style="background:#fff8e1;padding:0.8em;border-left:4px solid #ffb300;">
<strong>Author’s note:</strong> This “Quantum Quadrature” section describes an imaginary concept — 
			how quantum integration techniques <em>could</em> perhaps be applied to numerical quadrature in finance and other fields. 
			While the underlying algorithms (e.g., quantum amplitude estimation) are real and actively researched, 
			large‑scale, production‑ready implementations are not yet available on current quantum hardware. 
			This is a projection of where the technology may head, rather than a description of tools you can use today.
		</div>
<!-- Continuous vs Quantized (Quantum) — drop-in SVG -->
<figure style="max-width:800px;margin:0.8rem auto;">
<svg aria-labelledby="quant-title" height="auto" role="img" viewbox="0 0 900 320" width="100%" xmlns="http://www.w3.org/2000/svg">
<title id="quant-title">Continuous versus Quantized — illustration of quantum steps</title>
<desc id="quant-desc">
			  Left panel: a smooth continuous ramp labeled "Continuous (classical)".
			  Right panel: a staircase labeled "Quantized (quantum)" showing discrete levels and jumps.
			  Arrows and captions explain that quantum means values come in discrete units.
			</desc>
<style>
			  .panel { fill: #ffffff; stroke: #e6e6e6; stroke-width: 1.2; rx: 8; }
			  .label { font-family: Inter, Arial, sans-serif; font-size:16px; fill:#222; font-weight:700; }
			  .sub { font-family: Inter, Arial, sans-serif; font-size:13px; fill:#444; }
			  .axis { stroke:#888; stroke-width:1.2; stroke-linecap:round; }
			  .ramp { fill:url(#rgrad); stroke:#2b7aeb; stroke-width:2; opacity:0.95; }
			  .step { fill:url(#sgrad); stroke:#d9534f; stroke-width:2; opacity:0.95; }
			  .dot { fill:#2b7aeb; stroke:#fff; stroke-width:1.2; }
			  .jump { stroke:#d9534f; stroke-width:2.2; stroke-dasharray:6 4; }
			  .note { font-family: Inter, Arial, sans-serif; font-size:12px; fill:#333; }
			</style>
<!-- Gradients -->
<defs>
<lineargradient id="rgrad" x1="0" x2="1">
<stop offset="0%" stop-color="#cfe9ff"></stop>
<stop offset="100%" stop-color="#7fc3ff"></stop>
</lineargradient>
<lineargradient id="sgrad" x1="0" x2="1">
<stop offset="0%" stop-color="#ffe6e6"></stop>
<stop offset="100%" stop-color="#ffb3b3"></stop>
</lineargradient>
<marker id="arrow" markerheight="8" markerwidth="8" orient="auto" refx="8" refy="4">
<path d="M0 0 L8 4 L0 8 z" fill="#444"></path>
</marker>
</defs>
<!-- Left panel (Continuous) -->
<g transform="translate(30,20)">
<rect class="panel" height="270" rx="8" ry="8" width="410" x="0" y="0"></rect>
<text class="label" text-anchor="middle" x="205" y="28">Continuous (classical)</text>
<!-- axis -->
<line class="axis" x1="40" x2="370" y1="230" y2="230"></line>
<line class="axis" x1="40" x2="40" y1="230" y2="20"></line>
<!-- smooth ramp path -->
<path class="ramp" d="M45,220 Q110,140 180,100 T355,40 L355,220 Z"></path>
<!-- sample points along ramp -->
<circle class="dot" cx="90" cy="170" r="5"></circle>
<circle class="dot" cx="150" cy="125" r="5"></circle>
<circle class="dot" cx="220" cy="95" r="5"></circle>
<circle class="dot" cx="300" cy="60" r="5"></circle>
<text class="sub" x="45" y="250">Value varies smoothly — any intermediate values allowed.</text>
</g>
<!-- Right panel (Quantized) -->
<g transform="translate(460,20)">
<rect class="panel" height="270" rx="8" ry="8" width="410" x="0" y="0"></rect>
<text class="label" text-anchor="middle" x="205" y="28">Quantized (quantum)</text>
<!-- axis -->
<line class="axis" x1="40" x2="370" y1="230" y2="230"></line>
<line class="axis" x1="40" x2="40" y1="230" y2="20"></line>
<!-- staircase steps -->
<g transform="translate(40,40)">
<!-- level 3 -->
<rect class="step" height="150" width="120" x="220" y="10"></rect>
<!-- level 2 -->
<rect class="step" height="100" width="100" x="120" y="60"></rect>
<!-- level 1 -->
<rect class="step" height="50" width="80" x="40" y="110"></rect>
<!-- baseline -->
<rect fill="#f2f2f2" height="10" stroke="none" width="360" x="0" y="170"></rect>
<!-- sample allowed levels (dots centered on each plateau) -->
<circle cx="80" cy="135" fill="#d9534f" r="6" stroke="#fff" stroke-width="1.2"></circle>
<circle cx="170" cy="95" fill="#d9534f" r="6" stroke="#fff" stroke-width="1.2"></circle>
<circle cx="280" cy="40" fill="#d9534f" r="6" stroke="#fff" stroke-width="1.2"></circle>
<!-- vertical arrows showing jump between levels -->
<line class="jump" x1="110" x2="110" y1="125" y2="95"></line>
<line class="jump" x1="200" x2="200" y1="85" y2="55"></line>
<text class="note" text-anchor="middle" x="110" y="140">Jump</text>
</g>
<text class="sub" x="45" y="250">Values are restricted to discrete levels — transitions happen as jumps.</text>
</g>
<!-- Center explanation / caption -->
<g transform="translate(0,0)">
<text class="note" text-anchor="middle" x="450" y="300">
				Quantum = discrete "packets" or levels. Classical = continuous values. Measurement yields one allowed quantum.
			  </text>
</g>
</svg>
<figcaption style="font-family:Inter, Arial, sans-serif; font-size:13px; color:#333; margin-top:6px;">
			Illustration: left = continuous variable (any value allowed), right = quantized levels (discrete steps).
		  </figcaption>
</figure>
<div class="formal">
			Classical quadrature rules — even when accelerated on GPUs or clusters — still evaluate the integrand at a finite set of nodes, summing weighted function values to approximate the integral. 
			<strong>Quantum quadrature</strong> takes a fundamentally different approach: it encodes the integration problem into the amplitudes of a quantum state and applies 
			<em>quantum amplitude estimation</em> (QAE) to extract the expectation value with <em>quadratically fewer</em> samples than classical Monte Carlo methods require.
			Instead of looping over nodes one by one, a quantum computer can, in principle, prepare a superposition over all nodes simultaneously, evaluate the integrand across that superposition, and use interference to read out the mean value.
			<br/><br/>
			In theory, this could make certain high‑dimensional or computationally expensive integrals tractable, especially when the integrand can be efficiently encoded as a quantum oracle and the weight function can be prepared as a quantum state. 
			However, large‑scale, production‑ready implementations of such “quantum quadrature” are not yet available on current hardware — this is a forward‑looking concept based on existing quantum algorithms like QAE, rather than a tool you can run today.

			<br/><br/>
			In practical terms, <em>quantum amplitude estimation</em> works by preparing a quantum state whose amplitudes reflect the probability distribution of interest, then using controlled interference to estimate the average value of the integrand with far fewer oracle calls than classical sampling would require. The use of <em>superposition</em> means the quantum processor can, in a single logical step, represent and evolve all node evaluations at once — a form of parallelism fundamentally different from splitting work across classical cores or GPUs. This theoretical advantage is why quantum quadrature is attracting attention for problems like multi‑dimensional Monte Carlo in finance or physics, where classical cost grows prohibitively. That said, today’s quantum devices remain limited in qubit count, coherence time, and error rates, so these methods are still in the research and prototyping stage rather than in production deployment.
			<br/><br/>
<div class="note" style="background:#f0f8ff; padding:1em; border-left:4px solid #1e90ff; margin:1.5em 0;">
<h4>Term Explainers</h4>
<p><strong>Quantum Oracle:</strong> In quantum computing, an oracle is a special-purpose quantum circuit that encodes a function \( f(x) \) into the state of your qubits. 
				It’s a “black box” you can query inside a quantum algorithm without knowing its internal details, but it must be <em>unitary</em> (reversible) so it can run forwards and backwards. 
				For integration problems, the oracle is built to mark amplitudes or phases according to \( f(x) \), so that algorithms like Quantum Amplitude Estimation can extract the average value efficiently.</p>
<p class="layman"><strong>Layman’s view:</strong> Think of it as a sealed vending machine that, when you press a button labelled \(x\), instantly lights up the answer \(f(x)\). 
				In a quantum setting, you can press <em>all the buttons at once</em> in superposition, and the machine lights up all the answers in parallel.</p>
<p><strong>Qubits:</strong> A qubit (quantum bit) is the basic unit of quantum information. 
				Unlike a classical bit, which is either 0 or 1, a qubit can be in a <em>superposition</em> of 0 and 1:
				

			\[
					|\psi\rangle = \alpha|0\rangle + \beta|1\rangle
				\]


				where \(\alpha\) and \(\beta\) are complex amplitudes with \(|\alpha|^2 + |\beta|^2 = 1\). 
				Multiple qubits can also be <em>entangled</em>, meaning their states are linked in ways that have no classical counterpart.</p>
<p class="layman"><strong>Layman’s view:</strong> If a classical bit is like a coin lying flat — heads (0) or tails (1) — a qubit is like a spinning coin that’s both heads and tails until you catch it. 
				And if you spin two coins together in just the right way, they can become entangled — flip one, and the other “knows” instantly, no matter how far apart they are.</p>
</div>


			Given a weight function \( w(x) \) (often a probability density) and an integrand \( f(x) \), we can write:
			

		\[
				I = \int_{\Omega} f(x) w(x) \, dx
			\]


			The quantum procedure:
			<ol>
<li><strong>State preparation:</strong> Prepare a quantum state \(|\psi_w\rangle = \sum_x \sqrt{w(x)}\,|x\rangle\) so that measuring yields samples from \(w(x)\).</li>
<li><strong>Function encoding:</strong> Implement \(f(x)\) as a quantum oracle \(U_f\) that encodes \(f(x)\) into an ancilla qubit's amplitude or phase.</li>
<li><strong>Amplitude estimation:</strong> Use QAE to estimate the mean value of \(f(x)\) over \(w(x)\) with \(O(1/\epsilon)\) complexity instead of \(O(1/\epsilon^2)\).</li>
<li><strong>Result extraction:</strong> The estimated amplitude corresponds to the integral value (up to known scaling factors).</li>
</ol>
			This approach is, if sound, quite promising for high‑dimensional integrals in finance, physics, and Bayesian inference, where classical methods struggle.
		</div>
<div class="layman">
<strong>Think of quantum quadrature</strong> like tasting <em>every spoonful of soup at once</em> in a quantum superposition. 
			Instead of sampling one spoonful at a time, you prepare a magical state that contains the flavour of the whole pot. 
			Then, with a clever interference trick, you read out the average flavour with far fewer sips than any classical chef could manage.
		</div>
<h4>Pseudocode: Quantum Amplitude Estimation for Integration</h4>
<div class="pseudocode">
# Quantum Quadrature via Amplitude Estimation (conceptual)

Given:
	w(x)  - weight function (PDF)
	f(x)  - integrand
	ε     - target error

1. Discretize domain Ω into M basis states |x_j⟩
2. Prepare |ψ_w⟩ = Σ_j sqrt(w(x_j)) |x_j⟩
   # State preparation circuit encodes w(x) into amplitudes

3. Define oracle U_f:
   |x_j⟩|0⟩ → |x_j⟩( sqrt(1 - f(x_j))|0⟩ + sqrt(f(x_j))|1⟩ )
   # Encodes f(x_j) into amplitude of ancilla qubit

4. Construct Grover-like operator Q from U_f and |ψ_w⟩

5. Apply Quantum Amplitude Estimation:
   - For k = 0..m-1:
	   Apply controlled-Q^(2^k) to |ψ_w⟩
   - Perform inverse Quantum Fourier Transform on control register
   - Measure to obtain estimate â of amplitude a

6. Return I ≈ â  (scaled appropriately if w(x) not normalized)
		</div>
<h4>Key Advantages</h4>
<div class="formal">
<ul>
<li><strong>Quadratic speedup</strong> in sample complexity over classical Monte Carlo.</li>
<li>Natural fit for integrals expressed as expectations under a known distribution \(w(x)\).</li>
<li>Potential to handle very high‑dimensional problems where classical quadrature is infeasible.</li>
</ul>
</div>
<h4>Challenges</h4>
<div class="formal">
<ul>
<li><strong>State preparation:</strong> Efficiently encoding arbitrary \(w(x)\) into amplitudes is non‑trivial.</li>
<li><strong>Oracle construction:</strong> \(f(x)\) must be implemented as a quantum circuit with low depth.</li>
<li><strong>Hardware limits:</strong> Current 'Noisy Intermediate-Scale Quantum' (NISQ) devices have noise and qubit count constraints.</li>
<li><strong>Discretization:</strong> Continuous domains must be mapped to finite qubit registers.</li>
</ul>
</div>
<div class="key-point">
<strong>Key Insight:</strong> Quantum quadrature doesn’t replace classical methods today — but it points to a future where integrals that are currently intractable could be estimated efficiently by exploiting quantum parallelism and amplitude estimation. 
		What’s real and in active use right now is <em>Quantum Monte Carlo</em> — though the term means different things in different contexts. 
		In physics and chemistry, it refers to powerful <em>classical</em> stochastic algorithms for simulating quantum systems. 
		In quantum computing theory, it describes algorithms like <em>Quantum Amplitude Estimation</em> that, in principle, can accelerate Monte Carlo–style integration by a quadratic factor. 
		The former is a mature, widely deployed tool; the latter is a 'maybe' approach awaiting hardware capable of running it at scale.
	</div>
<div class="layman" style="margin:1em 0; padding:0.8em; background:#f9f9f9; border-left:4px solid #ccc;">
<strong>Quantum coffee break:</strong> I tried to make a quantum leap into another dimension… 
    but ended up in the kitchen instead, with a cup noodle and coffee. I have no recollection of anything. Apparently my wavefunction collapsed towards the microwave. That said, let's resume with actual 'our'-worldly algorithmic applications and methods.
</div>
<h2 id="section-7">Option Pricing with Quadrature Methods</h2>
<h3 id="section-7-1">Black-Scholes Model</h3>
<div class="formal">
            The Black-Scholes model prices options under geometric Brownian motion:
            \[ C = S_0N(d_1) - Ke^{-rT}N(d_2) \]
            where
            \[ d_1 = \frac{\ln(S_0/K) + (r + \sigma^2/2)T}{\sigma\sqrt{T}}, \quad d_2 = d_1 - \sigma\sqrt{T} \]
        </div>
<h3 id="section-7-2">Gauss-Hermite for Option Pricing</h3>
<div class="formal">
            Using the identity for normal distribution:
            \[ C = e^{-rT} \int_{-\infty}^{\infty} \max(S_0 e^{(r-\sigma^2/2)T + \sigma\sqrt{T} x} - K, 0) \phi(x)dx \]
            where \(\phi(x)\) is the standard normal density. Gauss-Hermite quadrature approximates this as:
            \[ C \approx e^{-rT} \sum_{i=1}^n w_i \max(S_0 e^{(r-\sigma^2/2)T + \sigma\sqrt{T} x_i} - K, 0) \]
        </div>
<h3 id="section-7-3">Strike-Aware Formulation</h3>
<div class="formal">
            Rather than lazily integrating over the entire real line, we can more intelligently integrate from the strike price to infinity (the value domain):
            \[ C = e^{-rT} \int_{\ln(K/S_0)}^{\infty} (S_0 e^y - K) f(y)dy \]
            where \(f(y)\) is the log-normal density function.
        </div>
<div class="pseudocode">
# Strike-aware Gauss-Legendre Pseudocode
sigsqrt = sigma * sqrt(T)
mu = (r - q) * T + 0.5 * sigma^2 * T
z_star = -(ln(S0/K) + mu) / sigsqrt
a = z_star
b = z_star + 10 * sigsqrt  # Upper bound

x_gl, w_gl = gauss_legendre_nodes_weights(n)
sum = 0
for i in 1..n:
    z = 0.5*(b-a)*x_gl[i] + 0.5*(a+b)
    S = S0 * exp(mu + sigsqrt * z)
    payoff = max(S - K, 0)
    density = (1/sqrt(2*pi)) * exp(-0.5*z*z)
    sum += w_gl[i] * payoff * density
C = exp(-r*T) * 0.5*(b-a) * sum
        </div>
<div class="layman">
<strong>Strike-aware quadrature</strong> is like knowing exactly where the treasure is buried and digging only in that area instead of searching the entire island. It's more efficient because you focus your efforts where it matters most. And for that we need an experienced scout or treasure hunter holding a Geiger Counter, not a radar.
        </div>
<h3 id="section-7-4">Carr-Madan Quadrature Formulation</h3>
<div class="formal">
<p>
				The Carr–Madan approach prices calls by working in the Fourier domain with an
				exponentially <em>damped</em> payoff to ensure integrability. Let \(k=\ln K\) and choose
				\(\alpha&gt;0\) such that the damped call \(e^{\alpha k} C(T,K)\) has a finite Fourier transform.
				Define the log-price characteristic function under the risk‑neutral measure,
				\(\varphi(u)=\mathbb{E}\!\left[e^{iu\ln S_T}\right]\). Then a common quadrature form is
			  </p>
<p class="equation">
				\[
				  C(T,K)
				  \;=\;
				  \frac{e^{-\alpha k}}{\pi}
				  \int_0^\infty
				  \Re\!\left\{
					e^{-iuk}\,
					\psi(u)
				  \right\}\,du,
				  \quad
				  \psi(u)
				  \;=\;
				  \frac{e^{-rT}\,\varphi\!\big(u - i(\alpha+1)\big)}
					   {\alpha^2+\alpha - u^2 + i(2\alpha+1)u}.
				\]
			  </p>
<p>
				Practical details:
			  </p>
<ul>
<li><strong>Damping</strong>: Typical choices \(\alpha\in[1,2]\) stabilize the integral across many models (Black–Scholes, Heston, Variance‑Gamma, etc.).</li>
<li><strong>Oscillations</strong>: Use the real–imaginary split
				  \(\Re\{e^{-iuk}\psi(u)\} = \cos(uk)\Re\psi(u)+\sin(uk)\Im\psi(u)\) to reduce cancellation and control error.</li>
<li><strong>Truncation</strong>: Replace \([0,\infty)\) with \([0,U_{\max}]\). Increase \(U_{\max}\) until the tail is negligible or use a variable transform (e.g., tanh–sinh) to compress infinity.</li>
<li><strong>Grid vs. FFT</strong>: For a <em>surface</em> of strikes, discretize \(u\) on a uniform grid and use FFT. For <em>individual strikes</em>, a high‑order quadrature (Clenshaw–Curtis, Gauss–Laguerre on a mapped domain, or adaptive GK15) is often simpler and more precise.</li>
<li><strong>Puts</strong>: Recover with put–call parity or use a separate damping regime (negative \(\alpha\)).</li>
</ul>
</div>
<div class="pseudocode">
# Carr–Madan by direct quadrature (single strike)

input: S0, r, q, T, K, alpha&gt;0, charfun  # charfun(u) = E[e^{iu ln S_T}] under Q
k    = ln(K)
psi(u) = exp(-r*T) * charfun(u - i*(alpha + 1))
		 / (alpha*alpha + alpha - u*u + i*(2*alpha + 1)*u)

# real–imag split integrand for stability
integrand(u) = cos(u*k) * Re(psi(u)) + sin(u*k) * Im(psi(u))

# choose truncation and quadrature
Umax = choose_cutoff_by_tolerance()
I = adaptive_GaussKronrod( integrand, 0, Umax )  # or Clenshaw–Curtis on [0,Umax]

price = exp(-alpha*k) * I / pi
return price
			</div>
<div class="pseudocode">
# Carr–Madan by FFT (many strikes on a log-strike grid)

input: S0, r, q, T, alpha, N (power of two), du (u spacing)
u_j   = j * du, j=0..N-1
k0    = chosen_min_log_strike
dk    = 2*pi/(N*du)

# compute damped transform samples with Simpson or trapezoid weights
for j in 0..N-1:
  u = u_j
  psi = exp(-r*T) * charfun(u - i*(alpha + 1))
		/ (alpha*alpha + alpha - u*u + i*(2*alpha + 1)*u)
  g[j] = psi * exp(-i*u*k0) * w_j     # w_j: integration weights (e.g., Simpson)

# inverse FFT-like recovery
G = FFT(g)
for m in 0..N-1:
  k_m   = k0 + m*dk
  C[m]  = exp(-alpha*k_m) * Re(G[m]) / pi

# map log-strikes k_m to strikes K_m = exp(k_m)
return { (K_m, C[m]) } across m
			</div>
<div class="layman">
<strong>Intuition:</strong> Instead of adding up payoffs in price space,
			  Carr–Madan moves to the “frequency” world, where complex shapes become easier to handle.
			  Damping acts like a gentle fade‑out so the integral doesn’t “ring” at infinity. For many strikes,
			  the FFT turns thousands of prices into a single lightning‑fast transform. The author has also shown in another article 'Fast Fourier Transform as an Engine for the Convolution of PDFs
and Black-Scholes Option Pricing Using Python' that one could integrate the payoff and the PDF directly using FFT convolution, which will churn out thousands of option values for thousands of corresponding spot prices. These can be interpolated to a high accuracy for any spot price using a Hermite (PCHIP) spline.
			</div>
<div class="key-point">
<strong>When to use:</strong> If you have a clean characteristic function (BS, Heston, Lévy models), Carr–Madan is ideal for fast, multi‑strike pricing (FFT), or precise single‑strike pricing (quadrature). Tune \(\alpha\), the truncation \(U_{\max}\), and use a stable real–imag split.
			</div>
<div class="formal">
<h4>Characteristic Functions</h4>
<p><strong>Black–Scholes (Log-Normal) model:</strong></p>
<div class="equation">
			  \[
			  \begin{aligned}
				\varphi_{BS}(u;T)
				&amp;= \exp\Bigl(
					 i\,u\bigl[\ln S_{0} + (r - q - \tfrac{1}{2}\sigma^{2})\,T\bigr] \\
				&amp;\qquad\quad
					 -\,\tfrac{1}{2}\,\sigma^{2}\,u^{2}T
				  \Bigr)\,.
			  \end{aligned}
			  \]
			  </div>
<p><strong>Heston (Stochastic Volatility) model:</strong></p>
<div class="equation">
			  \[
			  \begin{aligned}
				\varphi_{H}(u;T)
				&amp;= \exp\Bigl\{
					 i\,u\,\ln S_{0}
				   + i\,u\,(r - q)\,T
				   + \frac{\kappa\theta}{\sigma^{2}}
					 \Bigl[
					   (\kappa - i\rho\sigma\,u - d)\,T
					   - 2\,\ln\frac{1 - g\,e^{-dT}}{1 - g}
					 \Bigr] \\
				&amp;\qquad\quad
				   + \frac{v_{0}}{\sigma^{2}}\,
					 (\kappa - i\rho\sigma\,u - d)\,
					 \frac{1 - e^{-dT}}{1 - g\,e^{-dT}}
				  \Bigr\}, \[0.6em]
				\text{where}\quad
				d &amp;= \sqrt{(\kappa - i\rho\sigma\,u)^{2} + \sigma^{2}\,(u^{2} + i\,u)}\,, \[0.4em]
				g &amp;= \frac{\kappa - i\rho\sigma\,u - d}
							 {\kappa - i\rho\sigma\,u + d}\,.
			  \end{aligned}
			  \]
			  </div>
<p><strong>Variance-Gamma (Lévy) model:</strong></p>
<div class="equation">
			  \[
			  \begin{aligned}
				\varphi_{VG}(u;T)
				&amp;= \exp\Bigl(
					 i\,u\,\omega\,T
					 \;-\;
					 \frac{T}{\nu}\,
					 \ln\!\bigl(
					   1 - i\,\theta\,\nu\,u + \tfrac{1}{2}\,\sigma^{2}\,\nu\,u^{2}
					 \bigr)
				  \Bigr)\,, \[0.6em]
				\omega
				&amp;= \frac{1}{\nu}\,
				   \ln\!\bigl(1 - \theta\,\nu - \tfrac{1}{2}\,\sigma^{2}\,\nu\bigr)\,.
			  \end{aligned}
			  \]
			  </div>
</div>
<h3 id="section-7-5">Stop-Loss Premium Formulation</h3>
<div class="formal">
<p>
				The call can be seen as a discounted <em>stop‑loss premium</em> (a tail expectation):
			  </p>
<p class="equation">
				\[
				  C \;=\; e^{-rT}\,\mathbb{E}\big[(S_T - K)^+\big]
				  \;=\;
				  e^{-rT}\int_K^\infty (s-K)\,f_{S_T}(s)\,ds.
				\]
			  </p>
<p>
				A powerful reformulation integrates over <em>quantiles</em> rather than prices:
			  </p>
<p class="equation">
				\[
				  C \;=\; e^{-rT}\int_{F(K)}^{1}\left(F^{-1}(p)-K\right)\,dp,
				\]
			  </p>
<p>
					where \(F\) is the CDF of \(S_T\) and \(F^{-1}\) its quantile (PPF). This is model‑agnostic:
					any distribution with a PPF can be priced this way. It directly targets the region \(p\in[F(K),1]\)
					where the payoff is nonzero, and it avoids oscillations entirely.
					<br/><br/>
					In practice, when specifying \(F\) under the risk‑neutral measure \(\mathbb{Q}\), a <em>drift correction</em> is often required so that the discounted asset price is a martingale. This is typically achieved via a <strong>Girsanov change of measure</strong>, adjusting the drift term in the log‑price process so that
					\(\mathbb{E}_{\mathbb{Q}}[S_T] = S_0 e^{(r-q)T}\).
					For Lévy or other non‑Gaussian models, this correction ensures the characteristic exponent \(\psi(u)\) satisfies \(\psi(-i) = r-q\). Note that this applies to all the option pricing models in this document.
					<br/><br/>
					If the PPF is not available in closed form but the <em>moment generating function</em> (MGF) \(M_X(t) = \mathbb{E}[e^{tX}]\) of \(X = \ln S_T\) is known, one can recover \(F\) or its tail probabilities via numerical Laplace/Fourier inversion of the MGF or characteristic function (FT of the PDF). This allows the same stop‑loss/quantile integration framework to be applied to a wide class of distributions, including those defined only through their MGF.
				</p>
<ul>
<li><strong>Numerical stability</strong>: The integrand is monotone in \(p\); use adaptive Simpson or Gauss–Kronrod on \([F(K),1]\).</li>
<li><strong>Endpoint handling</strong>: Near \(p\to1\), the integrand may be flat. Use an endpoint map
				  (e.g., \(p = 1 - e^{-y}\)) if needed to expose exponential tail decay.</li>
<li><strong>Heavy tails</strong>: If the tail is heavy, use a tail cut \(p_{\max}&lt;1\) with a bound for the remainder, or accelerate with a change of variables.</li>
<li><strong>Links</strong>: The integral is the (discounted) Tail Value‑at‑Risk (TVaR) of \(S_T\) shifted by \(K\): \(C = e^{-rT}\!\int_{F(K)}^{1}\!F^{-1}(p)\,dp - e^{-rT}\,K\,(1-F(K))\).</li>
</ul>
</div>
<div class="pseudocode">
# Stop-Loss premium by quantile integration (model-agnostic)

input: K, r, T, CDF F(s), quantile PPF Q(p) = F^{-1}(p)

pK = F(K)

integrand(p) = Q(p) - K

# adaptive integration on [pK, 1]
# consider change of variables to improve endpoint behavior:
#   p = 1 - exp(-y), y in [ -ln(1 - pK), ∞ )
#   dp = exp(-y) dy
#   ∫_{pK}^1 (Q(p) - K) dp = ∫_{y0}^{∞} ( Q(1 - e^{-y}) - K ) e^{-y} dy

if endpoint_flat:
  y0 = -ln(1 - pK)
  J  = adaptive_GaussKronrod( lambda y: (Q(1 - exp(-y)) - K)*exp(-y), y0, Ymax )
else:
  J  = adaptive_Simpson( integrand, pK, 1 )

price = exp(-r*T) * J
return price
			</div>
<div class="pseudocode">
# Alternative: direct price-space integration with mapping to (0, ∞)

# substitute s = K + x, x in [0, ∞)
# C = e^{-rT} ∫_0^∞ x * f_S(K + x) dx
# map x = g(t) using tanh–sinh or x = exp(t) - 1 transforms for robust tails

g(t)   = tanh_sinh_map_to_infinity(t)     # e.g., x = tanh_sinh_transform(t)
g'(t)  = derivative_of_g(t)
integrand(t) = g(t) * f_S(K + g(t)) * g'(t)

price = e^{-rT} * trapezoid_over_R( integrand )  # DE tanh–sinh on t ∈ (-∞, ∞)
return price
			</div>
<div class="formal">
<p>
			Let's discuss why this is a superb formulation. The stop-loss premium formulation powerfully rewrites the call price as
		  </p>
<div class="equation">
			\(C = e^{-rT}\int_{F(K)}^1 \bigl(F^{-1}(p) - K\bigr)\,dp.\)
		  </div>
<p>
			Because the quantile (PPF) \(F^{-1}(p)\) is strictly increasing on \([0,1]\), the integrand 
			\(\;F^{-1}(p)-K\); is a monotonic, single-sign function on \([F(K),1]\). 
			In contrast to integrating a bell-shaped density (with its peaks and tails) or an oscillatory Fourier kernel, this smooth, unidirectional curve:
		  </p>
<ul>
<li>Eliminates oscillations and sign-changes, so uniform or adaptive quadrature converges with far fewer nodes.</li>
<li>Delivers stable, predictable error estimates—no hidden extrema to confound local refinement.</li>
<li>Restricts computation to the true payoff region \([F(K),1]\), avoiding wasted evaluations where the payoff is zero.</li>
<li>Works with any model that provides an invertible CDF–quantile pair, from lognormal to heavy-tailed or skewed distributions.</li>
</ul>
</div>
<figure style="max-width:800px;margin:0.8rem auto;">
<svg aria-labelledby="quant-title" height="70mm" preserveaspectratio="xMinYMin meet" role="img" viewbox="0 0 910 420" width="180mm" xmlns="http://www.w3.org/2000/svg">
<style>
    .panel-title { font: 16px "Segoe UI", Roboto, Arial; fill: #0b3d91; font-weight:700; }
    .axis { stroke: #666; stroke-width: 1.5; }
    .pdf-curve { fill:none; stroke:#d1495b; stroke-width:2.5; }
    .pdf-shade { fill:#f6d7da; opacity:0.9; }
    .quantile-curve { fill:none; stroke:#2f8fbc; stroke-width:2.5; }
    .quantile-shade { fill:#d7eef8; opacity:0.95; }
    .fourier-osc { fill:none; stroke:#8a5fb5; stroke-width:2; stroke-dasharray:4 2; }
    .label { font: 12px "Segoe UI", Roboto, Arial; fill: #222; }
    .math { font: 12px "Times New Roman", serif; fill:#111; }
    .kline { stroke:#333; stroke-width:1; stroke-dasharray:3 3; }
  </style>
<!-- PANEL BACKGROUNDS -->
<rect fill="#ffffff" height="380" rx="6" stroke="#e6eef8" width="356" x="10" y="10"></rect>
<rect fill="#ffffff" height="380" rx="6" stroke="#e6eef8" width="356" x="380" y="10"></rect>
<rect fill="#ffffff" height="380" rx="6" stroke="#e6eef8" width="356" x="750" y="10"></rect>
<!-- TITLES -->
<text class="panel-title" x="24" y="34">1) Standard PDF integral</text>
<text class="panel-title" x="394" y="34">2) Stop-loss (quantile)</text>
<text class="panel-title" x="764" y="34">3) Carr–Madan (Fourier)</text>
<!-- PANEL 1: PDF integral -->
<g transform="translate(10,50)">
<line class="axis" x1="0" x2="320" y1="250" y2="250"></line>
<line class="axis" x1="0" x2="0" y1="0" y2="250"></line>
<path class="pdf-curve" d="M 0 200 C 40 120, 80 60, 140 40 C 200 30, 260 60, 320 200"></path>
<line class="kline" x1="192" x2="192" y1="0" y2="250"></line>
<path class="pdf-shade" d="M 192 250 L 192 110 C 220 92, 250 94, 320 200 L 320 250 Z"></path>
<text class="label" text-anchor="middle" x="160" y="272">s (underlying)</text>
<text class="label" x="-6" y="-6">f(s)</text>
<text class="math" x="0" y="320">∫ (s - K)_+ f(s) ds — integrate payoff × density</text>
</g>
<!-- PANEL 2: Stop-loss / quantile -->
<g transform="translate(380,50)">
<line class="axis" x1="0" x2="320" y1="250" y2="250"></line>
<line class="axis" x1="0" x2="0" y1="0" y2="250"></line>
<line class="kline" x1="192" x2="192" y1="0" y2="250"></line>
<path class="quantile-curve" d="M 0 220 C 60 180, 120 150, 160 120 C 200 90, 260 60, 320 40"></path>
<path class="quantile-shade" d="M 192 250 L 192 120 C 220 106, 260 84, 320 40 L 320 250 Z"></path>
<text class="label" text-anchor="middle" x="160" y="272">p (probability)</text>
<text class="label" x="-6" y="-6">F⁻¹(p) − K</text>
<text class="math" x="0" y="320">∫_{F(K)}^{1} (F⁻¹(p) − K) dp — monotone, single sign</text>
</g>
<!-- PANEL 3: Carr-Madan / Fourier -->
<g transform="translate(750,50)">
<line class="axis" x1="0" x2="320" y1="250" y2="250"></line>
<line class="axis" x1="0" x2="0" y1="0" y2="250"></line>
<path class="fourier-osc" d="M 0 130 C 20 70, 40 190, 60 130 C 80 80, 100 180, 120 130 C 140 110, 160 150, 180 130 C 200 122, 220 136, 240 130 C 260 126, 280 132, 320 130"></path>
<text class="label" text-anchor="middle" x="160" y="272">u (Fourier freq.)</text>
<text class="label" x="-6" y="-6">Re[Kernel×φ]</text>
<text class="math" x="0" y="320">∫ e^{-iu ln K} φ(u) W(u) du — oscillatory kernel</text>
</g>
<!-- CAPTION -->
<text class="label" font-size="14px" x="24" y="410">
    Visualization: left = PDF × payoff; middle = stop-loss quantile; right = Carr-Madan (oscillatory)
  </text>
</svg>
<div class="layman">
<strong>Intuition:</strong> The stop‑loss view says, “Only the part of the distribution above the strike matters.” - just like the strike-aware Gauss-Legendre mentioned above.
			  Sliding to quantiles lets you walk straight along that tail from “just at the strike” to “way above it,”
			  adding up how much the option is in‑the‑money at each probability level.
			</div>
<div class="key-point">
<strong>When to use:</strong> If your model gives a <em>quantile function</em> (PPF) or an easy way to sample/invert the CDF,
			  this method is trivial to implement, robust, and fast for single‑strike pricing. It generalizes beyond lognormal
			  to any distribution with a PPF and pairs naturally with your custom‑quadrature toolkit on \([p_K,1]\).
			</div>
<h3 id="section-7-6">Practical guidance and examples </h3>
<div class="formal">
<h4>Choosing parameters and transforms</h4>
<ul>
<li><strong>Carr–Madan:</strong> Start with \(\alpha\in[1,2]\). For quadrature, increase \(U_{\max}\) until the residual tail is below tolerance; for FFT, pick \(\Delta u\) and \(N\) to cover the log‑strike span with desired resolution \( \Delta k = 2\pi/(N\,\Delta u)\). Always use the real–imag split.</li>
<li><strong>Stop‑Loss:</strong> Use adaptive Gauss–Kronrod on \([p_K,1]\). If the integrand is too flat near 1, apply \(p=1-e^{-y}\) and integrate over \(y\in[y_0,\infty)\) with tanh–sinh or a halved‑step trapezoid (double‑exponential decay aids convergence).</li>
</ul>
<h4>Model examples (sketch)</h4>
<ul>
<li><strong>Black–Scholes:</strong> \(\varphi(u)=\exp\!\big(iu m - \tfrac{1}{2}\sigma_T^2 u^2\big)\),
				  with \(m=\ln S_0 + (r-q - \tfrac{1}{2}\sigma^2)T\), \(\sigma_T=\sigma\sqrt{T}\).
				  For Stop‑Loss, \(Q(p)=\exp\!\big(m + \sigma_T \Phi^{-1}(p)\big)\).</li>
<li><strong>Heston:</strong> closed‑form \(\varphi(u)\) available → Carr–Madan integrates cleanly; Stop‑Loss is viable if you can numerically invert the CDF or build a monotone spline PPF from samples.</li>
<li><strong>Lévy models:</strong> characteristic functions are native; Carr–Madan is usually preferred for speed and stability; Stop‑Loss works if a PPF or accurate CDF is available.</li>
</ul>
</div>
<div class="pseudocode">
# Error control heuristics

# Carr–Madan tail test:
# Increase Umax until |∫_{Umax}^{Umax+Δ} integrand(u) du| &lt; tol_tail
for trial in [U1, U2, U3]:
  tail_est = panel_quadrature(integrand, trial, trial + deltaU)
  if abs(tail_est) &lt; tol_tail:
	Umax = trial; break

# Stop-Loss endpoint test:
# Refine adaptive integration until successive estimates differ &lt; tol
I_prev = None
for refine in 1..R:
  I = adaptive_GK(integrand_on_quantiles, pK, 1, tol_refine)
  if I_prev is not None and abs(I - I_prev) &lt; tol:
	break
  I_prev = I; tol_refine *= 0.5
			</div>
<div class="key-point">
<strong>Some Notes:</strong>
<ul>
<li>
<strong>Monte Carlo:</strong> A numerical integration technique based on random sampling of the domain. 
			  It converges slowly in one dimension (\(O(N^{-1/2})\)) but its convergence rate is independent of dimension, 
			  making it valuable for very high‑dimensional problems such as portfolio risk or path‑dependent option pricing.
			</li>
<li>
<strong>Lebesgue integration:</strong> A measure‑theoretic framework that integrates by summing over 
			  <em>ranges of function values</em> rather than slices of the domain. 
			  It underpins modern probability theory and risk‑neutral pricing. 
			  Monte Carlo methods are a natural numerical realisation of Lebesgue integration when expectations are taken under a probability measure. 
			  Deterministic quadrature rules like Gaussian quadrature can also be seen as Lebesgue‑style in that they weight function values according to a measure (e.g. a weight function \(w(x)\)).
			</li>
<li>
<strong>Riemann sums:</strong> The classical definition of an integral as the limit of sums of 
			  \(f(\xi_i)\,\Delta x_i\) over partitions of the domain. 
			  In numerical analysis, this viewpoint leads directly to <em>rectangular rules</em> (left, right, midpoint), 
			  and by refinement to <em>trapezoidal</em> and <em>Simpson’s</em> rules. 
			  These are efficient for smooth, low‑dimensional integrands but can be inefficient for oscillatory payoffs or high‑dimensional finance problems, if implemented naively.
			</li>
</ul>
</div>
<h4 id="bs-ko-quantile">Black–Scholes KO (single observation) via custom quantile quadrature</h4>
<div class="formal">
<p>
				This is an example just to show that using quadrature means in code, you are really just looking at the integral as it is penned on paper, any modifications to the payoff are direct modifications in the code, making everything very clear, concise, and easy to manage:
			  </p>
<div class="pseudocode">
# Price: European up-and-out call with single barrier observation at T
# Payoff: (S_T - K)_+ * 1{ S_T &lt; B }         # knock-out if S_T &gt;= B
# Under risk-neutral BS: ln S_T ~ N(m, v),   m = ln S0 + (r - q - ½σ²)T,   v = σ² T

INPUT:
  S0, K, B, T, r, q, σ
  N        # e.g., 200 quantile nodes
  tail_cut # e.g., 1e-4

SETUP (lognormal params):
  m = ln(S0) + (r - q - 0.5*σ^2)*T
  s = σ * sqrt(T)

NODES (probability space):
  p_i = linspace(tail_cut, 1 - tail_cut, N)
  z_i = Φ^{-1}(p_i)
  S_i = exp(m + s * z_i)

WEIGHTS (Δp):
  w_i from centered differences on p_i (normalize to 1)

PAYOFF FILTER:
  for each i: g_i = max(S_i - K, 0) * 1{ S_i &lt; B }

PRICE:
  V ≈ exp(-r*T) * Σ_i w_i * g_i

NOTES:
- This is extremely stable: monotone payoff, no oscillations.
- For multi-observation barriers, integrate conditional survival per step or switch to path methods.
                </div>
</div>
<h2 id="section-8">Beyond Black-Scholes &amp; Finance</h2>
<div class="formal">
<p>
			Quadrature methods extend naturally to more complex models, where the option price is still an
			expectation under the risk‑neutral measure but the underlying dynamics change the form of the
			characteristic function or density:
		  </p>
<ul>
<li>
<strong>Stochastic volatility models (Heston, SABR):</strong><br/>
			  Heston’s log‑price \(X_t = \ln S_t\) has a closed‑form characteristic function:
			  <div class="equation">
			  

			\[
			  \begin{aligned}
				\varphi_{H}(u;T)
				&amp;= \exp\Bigg\{
					 i\,u\,\ln S_0
				   + i\,u\,(r-q)\,T \\
				&amp;\quad
				   + \frac{\kappa\theta}{\sigma^2}
					 \left[
					   (\kappa - i\rho\sigma\,u - d)\,T
					   - 2\,\ln\!\left(
						   \frac{1 - g\,e^{-dT}}{1 - g}
						 \right)
					 \right] \\
				&amp;\quad
				   + \frac{v_0}{\sigma^2}\,
					 \frac{\kappa - i\rho\sigma\,u - d}
						  {1 - g\,e^{-dT}}\,
					 \big(1 - e^{-dT}\big)
				   \Bigg\},
			  \end{aligned}
			  \]


			  </div>
			  with
			  

			\[
				d = \sqrt{(\kappa - i\rho\sigma\,u)^2 + \sigma^2\,(u^2 + i\,u)}, 
				\quad
				g = \frac{\kappa - i\rho\sigma\,u - d}
						 {\kappa - i\rho\sigma\,u + d}.
			  \]


			  Quadrature methods (Carr–Madan, COS, etc.) integrate this \(\varphi_H\) against the chosen payoff kernel.
			</li>
<li>
<strong>Jump‑diffusion processes (Merton, Kou):</strong><br/>
			  Merton’s model augments Black–Scholes with normally distributed jumps:
			  

		\[
				\varphi_{M}(u;T) = \exp\!\left\{ i u m T - \tfrac{1}{2}\sigma^2 u^2 T
				+ \lambda T \left( e^{i u \mu_J - \tfrac{1}{2}\sigma_J^2 u^2} - 1 \right) \right\},
			  \]


			  where \(\lambda\) is jump intensity, \(\mu_J\) and \(\sigma_J\) are jump mean and volatility.  
			  Kou’s double‑exponential jumps replace the Gaussian jump term with
			  \(\lambda T \left( \frac{p\eta_1}{\eta_1 - i u} + \frac{(1-p)\eta_2}{\eta_2 + i u} - 1 \right)\).
			</li>
<li>
<strong>Variance Gamma and Lévy processes:</strong><br/>
			  Variance Gamma has characteristic function
			  

		\[
				\varphi_{VG}(u;T) = \left( 1 - i\theta\nu u + \tfrac{1}{2}\sigma^2 \nu u^2 \right)^{-T/\nu}
				\exp\!\left( i u \omega T \right),
			  \]


			  with drift correction \(\omega = \frac{1}{\nu} \ln\!\big(1 - \theta\nu - \tfrac{1}{2}\sigma^2\nu\big)\).  
			  More generally, for a Lévy process with exponent \(\psi(u)\),  
			  \(\varphi(u;T) = \exp\!\big( T\,\psi(u) \big)\) and quadrature integrates this against the payoff transform.
			</li>
<li>
<strong>Multi‑asset options (tensor product quadrature, sparse grids):</strong><br/>
			  For \(d\) underlyings with joint density \(f(\mathbf{s})\), the price is
			  

		\[
				V = e^{-rT} \int_{\mathbb{R}^d} \text{Payoff}(\mathbf{s})\, f(\mathbf{s})\, d\mathbf{s}.
			  \]


			  Tensor‑product Gaussian quadrature uses
			  \(\sum_{i_1=1}^{n_1} \cdots \sum_{i_d=1}^{n_d} w_{i_1}\cdots w_{i_d} \, \text{Payoff}(s_{i_1},\dots,s_{i_d})\),  
			  while sparse grids reduce the number of nodes from \(O(n^d)\) to \(O(n (\log n)^{d-1})\) for smooth payoffs.
			</li>
</ul>
</div>
<div class="layman">
<strong>Quadrature is like a Swiss Army knife</strong> for quantitative finance—once you understand how to use it, you can price all sorts of financial instruments, not just simple options. It's particularly useful for options that depend on multiple assets or have unusual payoff structures.
        </div>
<h3 id="section-8-1">Multidimensional Integrals</h3>

            For multidimensional integrals, tensor products of 1D rules can be used, but the number of points grows exponentially with dimension. Smolyak sparse grids dramatically reduce nodes while maintaining accuracy for smooth integrands.


		<div class="formal">
<p>
<strong>Full tensor product quadrature.</strong> Given 1D rules on \([-1,1]\),
		  </p>
<div class="equation">
		  \[
			U^{(j)}_{n_j}[g]
			\;=\;
			\sum_{i=1}^{n_j} w^{(j)}_{i}\,g\!\big(x^{(j)}_{i}\big),
			\qquad j=1,\dots,d,
		  \]
		  </div>
<p>
			the \(d\)-dimensional integral on \(\Omega=[-1,1]^d\) is approximated by the tensor product
		  </p>
<div class="equation">
		  \[
			I_d \;=\; \int_{\Omega} f(\mathbf{x})\,d\mathbf{x}
			\;\approx\;
			\sum_{i_1=1}^{n_1}\cdots\sum_{i_d=1}^{n_d}
			  \Big(\prod_{j=1}^{d} w^{(j)}_{i_j}\Big)\;
			  f\!\big(x^{(1)}_{i_1},\dots,x^{(d)}_{i_d}\big),
		  \]
		  </div>
<p>
			with node count \(N_{\text{tensor}}=\prod_{j=1}^d n_j = O(n^d)\) for \(n_j\!\asymp n\) — the “curse of dimensionality.”
		  </p>
<hr/>
<p>
<strong>Smolyak sparse grids (anisotropic form).</strong> Let \(\{U^{(j)}_{\ell}\}_{\ell\ge 1}\) be a nested sequence of 1D rules (e.g., Clenshaw–Curtis), and define hierarchical surpluses
		  </p>
<div class="equation">
		  \[
			\Delta^{(j)}_{\ell} \;=\; U^{(j)}_{\ell} - U^{(j)}_{\ell-1}, \qquad U^{(j)}_{0} \equiv 0.
		  \]
		  </div>
<p>
			The Smolyak operator of level \(q\) in \(d\) dimensions is
		  </p>
<div class="equation">
		  \[
			\mathcal{A}^{(d)}_{q}[f]
			\;=\;
			\sum_{\substack{\boldsymbol{\ell}\in\mathbb{N}^d\\ \|\boldsymbol{\ell}\|_1 \le q + d - 1}}
			  \Big(\Delta^{(1)}_{\ell_1}\otimes \cdots \otimes \Delta^{(d)}_{\ell_d}\Big)[f],
		  \]
		  </div>
<p>
			or, in an <em>anisotropic</em> setting with importance weights \(\boldsymbol{\gamma}=(\gamma_1,\dots,\gamma_d)\),
		  </p>
<div class="equation">
		  \[
			\mathcal{A}^{(d)}_{q,\boldsymbol{\gamma}}[f]
			\;=\;
			\sum_{\substack{\boldsymbol{\ell}\in\mathbb{N}^d\\ \sum_{j=1}^d \gamma_j(\ell_j-1)\;\le\; q}}
			  \Big(\Delta^{(1)}_{\ell_1}\otimes \cdots \otimes \Delta^{(d)}_{\ell_d}\Big)[f].
		  \]
		  </div>
<p>
			For nested rules with \(n_{\ell}\!\approx\!2^{\ell-1}+1\), the total nodes satisfy
			\(N_{\text{sparse}} = O\!\big(2^{q}\,q^{d-1}\big)\), dramatically smaller than \(O(2^{qd})\).
			For functions with bounded mixed derivatives (mixed Sobolev smoothness), the error decays near the 1D rate up to polylog factors in \(d\).
		  </p>
</div>
<div class="layman">
<p><strong>Why sparse grids matter:</strong> A full grid in 10D is like trying to photograph every grain of sand on a beach. Smolyak takes smart snapshots along carefully chosen “slices” so you still see the big picture without counting every grain.</p>
</div>
<h4>Visual intuition: dense vs. sparse grids (2D)</h4>
<div class="svg-container">
<svg aria-label="Full tensor grid versus Smolyak sparse grid in 2D" role="img" viewbox="0 0 900 320" xmlns="http://www.w3.org/2000/svg">
<style>
			  .title{font:700 16px system-ui,Segoe UI,Roboto,sans-serif; fill:#222}
			  .label{font:500 13px system-ui,Segoe UI,Roboto,sans-serif; fill:#444}
			  .panel{fill:#fff; stroke:#e2e2e2; stroke-width:1.2}
			  .axis{stroke:#888; stroke-width:1}
			  .pt-dense{fill:#1f77b4}
			  .pt-sparse{fill:#d62728}
			  .legend{font:12px system-ui,Segoe UI,Roboto,sans-serif; fill:#444}
			  .key{stroke:#666; stroke-dasharray:4 4}
			</style>
<!-- Titles -->
<text class="title" text-anchor="middle" x="225" y="28">Full tensor grid (9×9 = 81 points)</text>
<text class="title" text-anchor="middle" x="675" y="28">Smolyak sparse grid (level 3, 29 points)</text>
<!-- Left panel: dense -->
<rect class="panel" height="260" rx="6" width="370" x="40" y="40"></rect>
<line class="axis" x1="70" x2="380" y1="280" y2="280"></line>
<line class="axis" x1="70" x2="70" y1="80" y2="280"></line>
<text class="label" text-anchor="end" x="380" y="300">x</text>
<text class="label" x="60" y="80">y</text>
<!-- Dense grid points -->
<!-- 9x9 lattice inside [80, 370] × [90, 270] -->
<g>
<!-- generate points -->
<!-- rows -->
<g>
<!-- y positions -->
<g>
<circle class="pt-dense" cx="100" cy="260" r="3.5"></circle>
<circle class="pt-dense" cx="140" cy="260" r="3.5"></circle>
<circle class="pt-dense" cx="180" cy="260" r="3.5"></circle>
<circle class="pt-dense" cx="220" cy="260" r="3.5"></circle>
<circle class="pt-dense" cx="260" cy="260" r="3.5"></circle>
<circle class="pt-dense" cx="300" cy="260" r="3.5"></circle>
<circle class="pt-dense" cx="340" cy="260" r="3.5"></circle>
<circle class="pt-dense" cx="380" cy="260" r="3.5"></circle>
</g>
<g>
<circle class="pt-dense" cx="100" cy="230" r="3.5"></circle>
<circle class="pt-dense" cx="140" cy="230" r="3.5"></circle>
<circle class="pt-dense" cx="180" cy="230" r="3.5"></circle>
<circle class="pt-dense" cx="220" cy="230" r="3.5"></circle>
<circle class="pt-dense" cx="260" cy="230" r="3.5"></circle>
<circle class="pt-dense" cx="300" cy="230" r="3.5"></circle>
<circle class="pt-dense" cx="340" cy="230" r="3.5"></circle>
<circle class="pt-dense" cx="380" cy="230" r="3.5"></circle>
</g>
<g>
<circle class="pt-dense" cx="100" cy="200" r="3.5"></circle>
<circle class="pt-dense" cx="140" cy="200" r="3.5"></circle>
<circle class="pt-dense" cx="180" cy="200" r="3.5"></circle>
<circle class="pt-dense" cx="220" cy="200" r="3.5"></circle>
<circle class="pt-dense" cx="260" cy="200" r="3.5"></circle>
<circle class="pt-dense" cx="300" cy="200" r="3.5"></circle>
<circle class="pt-dense" cx="340" cy="200" r="3.5"></circle>
<circle class="pt-dense" cx="380" cy="200" r="3.5"></circle>
</g>
<g>
<circle class="pt-dense" cx="100" cy="170" r="3.5"></circle>
<circle class="pt-dense" cx="140" cy="170" r="3.5"></circle>
<circle class="pt-dense" cx="180" cy="170" r="3.5"></circle>
<circle class="pt-dense" cx="220" cy="170" r="3.5"></circle>
<circle class="pt-dense" cx="260" cy="170" r="3.5"></circle>
<circle class="pt-dense" cx="300" cy="170" r="3.5"></circle>
<circle class="pt-dense" cx="340" cy="170" r="3.5"></circle>
<circle class="pt-dense" cx="380" cy="170" r="3.5"></circle>
</g>
<g>
<circle class="pt-dense" cx="100" cy="140" r="3.5"></circle>
<circle class="pt-dense" cx="140" cy="140" r="3.5"></circle>
<circle class="pt-dense" cx="180" cy="140" r="3.5"></circle>
<circle class="pt-dense" cx="220" cy="140" r="3.5"></circle>
<circle class="pt-dense" cx="260" cy="140" r="3.5"></circle>
<circle class="pt-dense" cx="300" cy="140" r="3.5"></circle>
<circle class="pt-dense" cx="340" cy="140" r="3.5"></circle>
<circle class="pt-dense" cx="380" cy="140" r="3.5"></circle>
</g>
<g>
<circle class="pt-dense" cx="100" cy="110" r="3.5"></circle>
<circle class="pt-dense" cx="140" cy="110" r="3.5"></circle>
<circle class="pt-dense" cx="180" cy="110" r="3.5"></circle>
<circle class="pt-dense" cx="220" cy="110" r="3.5"></circle>
<circle class="pt-dense" cx="260" cy="110" r="3.5"></circle>
<circle class="pt-dense" cx="300" cy="110" r="3.5"></circle>
<circle class="pt-dense" cx="340" cy="110" r="3.5"></circle>
<circle class="pt-dense" cx="380" cy="110" r="3.5"></circle>
</g>
<g>
<circle class="pt-dense" cx="100" cy="260" r="3.5"></circle>
<circle class="pt-dense" cx="100" cy="110" r="3.5"></circle>
</g>
</g>
<!-- x=100 column already there; add leftmost x=70 column and rightmost x=420? keep in bounds -->
<!-- Add leftmost and rightmost columns -->
<g>
<!-- leftmost -->
<circle class="pt-dense" cx="70" cy="260" r="3.5"></circle>
<circle class="pt-dense" cx="70" cy="230" r="3.5"></circle>
<circle class="pt-dense" cx="70" cy="200" r="3.5"></circle>
<circle class="pt-dense" cx="70" cy="170" r="3.5"></circle>
<circle class="pt-dense" cx="70" cy="140" r="3.5"></circle>
<circle class="pt-dense" cx="70" cy="110" r="3.5"></circle>
<!-- rightmost -->
<circle class="pt-dense" cx="420" cy="260" r="3.5"></circle>
<circle class="pt-dense" cx="420" cy="230" r="3.5"></circle>
<circle class="pt-dense" cx="420" cy="200" r="3.5"></circle>
<circle class="pt-dense" cx="420" cy="170" r="3.5"></circle>
<circle class="pt-dense" cx="420" cy="140" r="3.5"></circle>
<circle class="pt-dense" cx="420" cy="110" r="3.5"></circle>
</g>
</g>
<!-- Right panel: sparse -->
<rect class="panel" height="260" rx="6" width="370" x="490" y="40"></rect>
<line class="axis" x1="520" x2="830" y1="280" y2="280"></line>
<line class="axis" x1="520" x2="520" y1="80" y2="280"></line>
<text class="label" text-anchor="end" x="830" y="300">x</text>
<text class="label" x="510" y="80">y</text>
<!-- Sparse grid points (Smolyak L=3 sketch) -->
<g>
<!-- center cross -->
<circle class="pt-sparse" cx="675" cy="180" r="4.2"></circle>
<circle class="pt-sparse" cx="675" cy="120" r="4.2"></circle>
<circle class="pt-sparse" cx="675" cy="240" r="4.2"></circle>
<circle class="pt-sparse" cx="615" cy="180" r="4.2"></circle>
<circle class="pt-sparse" cx="735" cy="180" r="4.2"></circle>
<!-- diagonals -->
<circle class="pt-sparse" cx="615" cy="120" r="4.2"></circle>
<circle class="pt-sparse" cx="735" cy="120" r="4.2"></circle>
<circle class="pt-sparse" cx="615" cy="240" r="4.2"></circle>
<circle class="pt-sparse" cx="735" cy="240" r="4.2"></circle>
<!-- near-boundary refinements -->
<circle class="pt-sparse" cx="555" cy="180" r="4.2"></circle>
<circle class="pt-sparse" cx="795" cy="180" r="4.2"></circle>
<circle class="pt-sparse" cx="675" cy="95" r="4.2"></circle>
<circle class="pt-sparse" cx="675" cy="265" r="4.2"></circle>
<!-- few extra hierarchical nodes -->
<circle class="pt-sparse" cx="585" cy="140" r="4.2"></circle>
<circle class="pt-sparse" cx="765" cy="220" r="4.2"></circle>
<circle class="pt-sparse" cx="585" cy="220" r="4.2"></circle>
<circle class="pt-sparse" cx="765" cy="140" r="4.2"></circle>
</g>
<!-- Legend -->
<rect fill="#f7f7f7" height="18" stroke="#e6e6e6" width="280" x="310" y="300"></rect>
<circle class="pt-dense" cx="325" cy="309" r="4"></circle><text class="legend" x="340" y="313">Full tensor nodes</text>
<circle class="pt-sparse" cx="470" cy="309" r="4"></circle><text class="legend" x="485" y="313">Smolyak nodes</text>
</svg>
<div class="caption">Dense tensor grids explode as \(n^d\). Smolyak sparse grids keep the 1D accuracy trend with far fewer points for smooth, mixed‑regularity integrands.</div>
</div>
<h4>Example integrands</h4>
<div class="formal">
<ul>
<li><strong>Smooth mixed-derivative case:</strong>
			  \[
				f(\mathbf{x})
				= \exp\!\Big(-\sum_{j=1}^d a_j (x_j - c_j)^2\Big)\,
				  \cos\!\Big(2\pi \sum_{j=1}^d b_j x_j\Big), \quad \mathbf{x}\in[-1,1]^d,
			  \]
			  with \(a_j&gt;0\). Ideal for sparse grids (high mixed smoothness).
			</li>
<li><strong>Basket call (via Gaussian map):</strong>
			  \[
				V \;=\; e^{-rT}\,\mathbb{E}\!\left[\big(\sum_{j=1}^d w_j S_{0j} e^{\mu_j+\sigma_j Z_j}-K\big)^+\right],
			  \]
			  where \(\mathbf{Z}\sim\mathcal{N}(0,\Sigma)\).
			  Use Gauss–Hermite in \(d\)D or Cholesky to map \(\mathbf{Z}=L\mathbf{y},\,\mathbf{y}\sim\mathcal{N}(0,I)\) and then apply sparse grids on \(\mathbf{y}\).
			</li>
</ul>
</div>
<h4>Pseudocode: full tensor product quadrature</h4>
<div class="pseudocode">
# Full tensor quadrature (d dimensions)

input:
  - f: function R^d -&gt; R
  - rules[1..d]: 1D quadrature rules, each with nodes x_j[1..n_j], weights w_j[1..n_j]

function tensor_integrate(f, rules):
	idx = [1]*d
	total = 0.0
	while True:
		# build point and weight
		x = [ rules[j].x[ idx[j] ] for j in 1..d ]
		w = 1.0
		for j in 1..d:
			w *= rules[j].w[ idx[j] ]
		total += w * f(x)

		# advance mixed-radix counter
		k = d
		while k &gt;= 1:
			if idx[k] &lt; rules[k].n:
				idx[k] += 1
				for m in k+1..d:
					idx[m] = 1
				break
			else:
				k -= 1
		if k == 0:
			return total
		</div>
<div class="key-point">
<strong>Trade-off:</strong> Full tensor is simple and optimal for small \(d\) (e.g., \(d\le 3\)) with cheap integrands. Cost grows as \(O(n^d)\).
		</div>
<h4>Pseudocode: Smolyak sparse grid (nested 1D rules)</h4>
<div class="pseudocode">
# Smolyak sparse grid of level q with nested 1D rules

input:
  - f: function R^d -&gt; R
  - U_level(j, ell): returns 1D level-ell rule for dimension j:
	  nodes x^{(j)}_{ell}[1..n_ell], weights w^{(j)}_{ell}[1..n_ell]
	with nesting: nodes at ell-1 are contained in ell
  - q: sparse grid level
  - d: dimension

function smolyak_integrate(f, U_level, q, d):
	total = 0.0

	# iterate over all multi-indices ell = (ell_1,...,ell_d)
	for ell in all_multi_indices(d):
		if sum(ell) &lt;= q + d - 1:
			# build tensor product of surpluses Δ_{ell_j}
			nodes_list = []
			weights_list = []
			for j in 1..d:
				# current and previous level rules
				nodes_curr, weights_curr = U_level(j, ell[j])
				if ell[j] &gt; 1:
					nodes_prev, weights_prev = U_level(j, ell[j]-1)
				else:
					nodes_prev, weights_prev = [], []

				# surplus = current rule minus previous rule (matching nodes)
				Δ_nodes, Δ_weights = hierarchical_surplus(nodes_curr, weights_curr,
														  nodes_prev, weights_prev)
				nodes_list.append(Δ_nodes)
				weights_list.append(Δ_weights)

			# tensor product loop over surplus nodes
			for (pt, wt) in tensor_product(nodes_list, weights_list):
				total += wt * f(pt)

	return total
		</div>
<div class="layman">
<strong>Multidimensional quadrature</strong> is like trying to measure the volume of a complex shape. A full grid approach would measure every single point, which is inefficient. Sparse grids are like taking strategic measurements at key locations to estimate the volume without all the work.
        </div>
<div class="formal">
		  In any multidimensional pricing or risk problem, the first step is to clearly identify the <em>integrand</em> — the function you are integrating over the joint distribution of your risk factors. Quants should always build a straightforward Monte Carlo version first: it serves as a “truth” benchmark or at least a ballpark sanity check before attempting more exotic quadrature or sparse grid methods.

		  Once the integrand is understood, make every effort to reduce the effective dimension algebraically. Standard calculus tools apply:

			<ul>
<li>
<strong>Fubini’s theorem:</strong>
				If \(f(x,y)\) is integrable on \(A\times B\), then
				<div class="equation">
				\[
				  \iint_{A\times B} f(x,y)\,dx\,dy
				  \;=\;
				  \int_A\!\left[\int_B f(x,y)\,dy\right]dx
				  \;=\;
				  \int_B\!\left[\int_A f(x,y)\,dx\right]dy.
				\]
				</div>
				This enables <em>iterative integration</em> — integrate out one variable at a time when the inner integral is tractable.

				<em>Finance example 1 (Max of two):</em>
<div class="equation">
					

				\[
					  C = e^{-rT} \iint_{\mathbb{R}^2}
					  \big(\max(s_1,s_2) - K\big)^+ \,
					  f_{S_1}(s_1) f_{S_2}(s_2)\, ds_1 ds_2
					\]


				  </div>
<div class="equation">
					

				\[
					\begin{aligned}
					  C &amp;= e^{-rT} \int_{0}^{\infty} \Bigg(
						\int_{0}^{s_1} (s_1 - K)^+ f_{S_2}(s_2)\,ds_2 \\
						&amp;\quad+\;
						\int_{s_1}^{\infty} (s_2 - K)^+ f_{S_2}(s_2)\,ds_2
					  \Bigg) f_{S_1}(s_1)\,ds_1
					\end{aligned}
					\]


				  </div>
<em>Finance example 2 (Independent factors):</em> For separable \(f(x,y)=g(x)h(y)\),
				<div class="equation">
				\[
				  \iint g(x)h(y)\,dx\,dy
				  \;=\;
				  \left(\int g(x)\,dx\right)\!\left(\int h(y)\,dy\right).
				\]
				</div>
<div class="layman">Peel one layer at a time: fix one variable, finish the easy inside piece, then move on.</div>
<div class="pseudocode">
# Fubini iterative integration (max-call under independence)
def price_max_call(f1, f2, K, r, T):
	def inner(s1):
		t1 = integrate(lambda s2: max(s1 - K, 0.0) * f2(s2), 0.0, s1)
		t2 = integrate(lambda s2: max(s2 - K, 0.0) * f2(s2), s1, inf)
		return (t1 + t2) * f1(s1)
	return math.exp(-r*T) * integrate(inner, 0.0, inf)
				</div>
</li>
<li>
<strong>Change of variables:</strong>
				Use \(\mathbf{u}=T(\mathbf{x})\) to simplify domain/structure; adjust by the Jacobian.
				<div class="equation">
				\[
				  \int_{\Omega} f(\mathbf{x})\,d\mathbf{x}
				  \;=\;
				  \int_{T(\Omega)} f(T^{-1}(\mathbf{u}))\,
				  \big|\det J_{T^{-1}}(\mathbf{u})\big|\,d\mathbf{u}.
				\]
				</div>
<em>Example 1 (Polar):</em> For \(\iint_{x^2+y^2\le R^2} g(\sqrt{x^2+y^2})\,dx\,dy\),
				<div class="equation">
				\[
				  \int_0^{2\pi}\!\!\int_0^R g(r)\, r\, dr\, d\theta
				  \;=\; 2\pi \int_0^R g(r)\, r\, dr.
				\]
				</div>
<em>Example 2 (Finance, decorrelation):</em> For \(Z\sim \mathcal{N}(0,\Sigma)\), take Cholesky \(L\) s.t. \(\Sigma=LL^\top\), set \(Z=L Y\) with \(Y\sim \mathcal{N}(0,I)\).

				<div class="layman">Rotate your axes so the problem lines up with them; straight cuts beat diagonal cuts.</div>
<div class="pseudocode">
# Decorrelate correlated normals via Cholesky
def decorrelated_samples(L, n_samples):
	for _ in range(n_samples):
		y = np.random.normal(size=L.shape[0])
		yield L @ y
				</div>
</li>
<li>
<strong>Change of bounds:</strong>
				Triangles and simplices often become rectangles under a clever map.
				<div class="equation">
				\[
				  \iint_{\substack{x\ge 0,\,y\ge 0\\ x+y\le 1}} h(x,y)\,dx\,dy
				  \;\xrightarrow{u=x+y,\ v=x}\;
				  \int_{0}^{1}\!\left(\int_{0}^{u} h(u-v,v)\, dv\right)\! du.
				\]
				</div>
				If \(h\) depends only on \(u\), then \(\int_0^u h(u)\,dv = u\,h(u)\) and the integral becomes 1D:
				<div class="equation">
				\[
				  \int_0^1 u\,h(u)\,du.
				\]
				</div>
<div class="layman">Redraw a slanted fence into a neat rectangle; measuring becomes trivial.</div>
<div class="pseudocode">
# Change of bounds mapping for triangle x&gt;=0,y&gt;=0,x+y&lt;=1
def triangle_to_uv_integral(h_u):
	# if h(x,y) = H(x+y) only
	return integrate(lambda u: u * h_u(u), 0.0, 1.0)
				</div>
</li>
<li>
<strong>Pre-solving with integration by parts (IBP):</strong>
				Reduce difficult kernels before numerics.
				<div class="equation">
				\[
				  I = \int_{0}^{\infty} e^{-a x}\sin(bx)\,dx
				  \;=\;
				  \frac{b}{a^2+b^2}.
				\]
				</div>
				(One IBP plus a standard Laplace integral.)

				<em>Finance:</em> In Laplace/Fourier pricing, IBP moves derivatives to exponential kernels, simplifying the residual integral.

				<div class="layman">Do the easy algebraic pruning first; then compute only what’s left.</div>
<div class="pseudocode">
# SymPy: verify a closed-form piece before numerics
import sympy as sp
x,a,b = sp.symbols('x a b', positive=True)
expr = sp.exp(-a*x)*sp.sin(b*x)
I = sp.integrate(expr, (x, 0, sp.oo))
print(sp.simplify(I))  # b/(a**2 + b**2)
				</div>
</li>
<li>
<strong>Reduction to a low‑dimensional statistic:</strong>
				If the payoff depends only on \(S=\sum_{j=1}^{d} w_j X_j\), transform to \((S,\text{orthogonal})\) and integrate out orthogonal parts.

				<em>Deelstra’s comonotonic upper bound (Arithmetic basket):</em>
<div class="equation">


					\[
					\begin{aligned}
					  C &amp;= e^{-rT}\,\mathbb{E}\!\left[
							 \left(\tfrac{1}{d}\sum_{j=1}^d S_{T,j} - K\right)^+
						   \right] \\
						&amp;\leadsto\;
						   C_{\text{upper}}
						   = e^{-rT}\!\int_0^1
							 \left(\tfrac{1}{d}\sum_{j=1}^d F_j^{-1}(p) - K \right)^+\, dp
					\end{aligned}
					\]


					</div>
<div class="layman">Assume perfect lockstep across names — you get a safe overestimate that collapses to 1D.</div>
<div class="pseudocode">
# Deelstra's comonotonic bound via quantile integration on [0,1]
def deelstra_upper_bound(F_inv_list, K, r, T, rule):
	def integrand(p):
		avg = sum(F_inv(p) for F_inv in F_inv_list) / len(F_inv_list)
		return max(avg - K, 0.0)
	total = 0.0
	for w, node in zip(rule.weights, rule.nodes):  # nodes in [0,1]
		total += w * integrand(node)
	return math.exp(-r*T) * total
				</div>
</li>
<li>
<strong>Conditioning (Law of iterated expectations):</strong>
				Collapse one variable via a conditional expectation.
				<div class="equation">
				\[
				  \mathbb{E}[g(X,Y)]
				  \;=\;
				  \mathbb{E}\!\big[\ \mathbb{E}[g(X,Y)\mid X]\ \big].
				\]
				</div>
<em>Finance:</em> If \(Y\mid X=x\) is Gaussian and \(g\) is affine in \(Y\), the inner expectation is analytic → 1D left.

				<div class="layman">Ask one question at a time; if the first answer reveals the second, you skip it.</div>
<div class="pseudocode">
# Conditioning reduction: E[g(X,Y)] with Y|X Gaussian
def reduced_integrand(x):
	mu, sig = mu_sigma_given_x(x)
	return analytic_E_over_Y(mu, sig)   # closed form in Y
price = integrate(lambda x: reduced_integrand(x) * fX(x), x_lo, x_hi)
				</div>
</li>
<li>
<strong>Marginalisation / separability:</strong>
				If factors separate, multiply one‑dimensional pieces.
				<div class="equation">
				\[
				  \iint h(x)\,k(y)\,dx\,dy
				  \;=\;
				  \left(\int h(x)\,dx\right)\!\left(\int k(y)\,dy\right).
				\]
				</div>
<em>Finance:</em> Independent term structures, product payoffs.

				<div class="layman">If chores are independent, split the list and finish each separately.</div>
</li>
<li>
<strong>Convolution structure (sum of factors):</strong>
				If \(Z=X+Y\) with independent \(X,Y\),
				<div class="equation">
				\[
				  f_Z(z) \;=\; \int f_X(x)\,f_Y(z-x)\,dx,
				  \qquad
				  \hat f_Z(u) \;=\; \hat f_X(u)\,\hat f_Y(u).
				\]
				</div>
				If the payoff depends only on \(Z\), use 1D convolution or FFT (via characteristic functions).

				<em>Finance:</em> Sum of (approximate) lognormals; price via FFT over \(\varphi_Z\).

				<div class="layman">Add the “shadows” (Fourier) where addition becomes multiplication; then return.</div>
<div class="pseudocode">
# 1D convolution via FFT using characteristic functions
def price_via_fft(payoff_hat, phi_Z, u_grid):
	# payoff_hat(u): Fourier transform of payoff kernel
	# phi_Z(u): characteristic function of Z
	G = payoff_hat(u_grid) * phi_Z(u_grid)
	# inverse FFT to real space (schematic)
	return ifft(G).real
				</div>
</li>
<li>
<strong>Orthogonal transforms (PCA/rotations):</strong>
				Rotate to align one axis with the dominant direction; integrate orthogonal parts out.
				<div class="equation">
				\[
				  \mathbf{x} = Q\mathbf{y},\quad Q^\top Q = I,
				  \qquad
				  \int f(\mathbf{x})\,d\mathbf{x}
				  \;=\;
				  \int f(Q\mathbf{y})\,d\mathbf{y}.
				\]
				</div>
<em>Finance:</em> PCA for correlated Gaussian factors; keep first components, marginalise the rest.

				<div class="layman">Turn the problem so the “action” sits on a single axis; the rest fades out.</div>
</li>
<li>
<strong>Symmetry and group actions:</strong>
				If \(f\) is invariant under permutations/rotations, integrate over a fundamental region and multiply.
				<div class="equation">
				\[
				  \int_{\Omega} f(\mathbf{x})\,d\mathbf{x}
				  \;=\; |\mathcal{G}|\; \int_{\Omega/\mathcal{G}} f(\mathbf{x})\,d\mathbf{x}.
				\]
				</div>
<em>Finance:</em> Exchangeable assets in a symmetric basket.

				<div class="layman">Compute one wedge of the pie, then multiply by the number of wedges.</div>
</li>
<li>
<strong>Indicator-function tricks (region reparametrisation):</strong>
				Replace \(\mathbf{1}_{\{X&gt;Y\}}\) with a variable change \(U=X-Y,\,V=Y\) so the indicator becomes \(\mathbf{1}_{\{U&gt;0\}}\) with simple bounds.

				<div class="layman">Swap to difference+baseline; the inequality becomes a plain “greater than zero.”</div>
</li>
<li>
<strong>Known moments (polynomial integrands):</strong>
				Use moment formulas and avoid integration.
				<div class="equation">
				\[
				  X\sim \mathcal{N}(0,\sigma^2):\quad
				  \mathbb{E}[X^{2n}] = (2n-1)!!\,\sigma^{2n},\qquad \mathbb{E}[X^{2n+1}]=0.
				\]
				</div>
<em>Finance:</em> Polynomial approximations of payoffs; replace integrals by moments.

				<div class="layman">If you know all the averages in advance, you can skip counting every case.</div>
</li>
<li>
<strong>Laplace / Fourier transforms (transform pricing):</strong>
				Move to transform space, integrate there, invert.
				<div class="equation">
				\[
				  \int f(x)\,g(x)\,dx
				  \;=\;
				  \frac{1}{2\pi}\int \hat f(u)\,\hat g(-u)\,du,
				  \qquad
				  \mathcal{L}\{f\}(s) = \int_0^\infty e^{-sx} f(x)\,dx.
				\]
				</div>
<em>Finance:</em> Carr–Madan/COS integrate against characteristic functions; often 1D.

				<div class="layman">Translate a hard sentence into a language where it’s easy — then translate back.</div>
</li>
<li>
<strong>“p‑trivial” variables (drop almost‑sure constants):</strong>
				If a variable is a.s. constant under the measure (\(P\)-trivial), it contributes nothing; remove that dimension.

				<div class="layman">If something never changes, don’t waste time measuring it.</div>
</li>
<li>
<strong>CAS assist (SymPy/Mathematica) before quadrature:</strong>
				Offload closed‑form sub‑integrals, simplifications, or transforms to a CAS, then apply numerics to the residual part.
				<div class="pseudocode">
# SymPy pipeline: pre-solve inner integral, then numeric outer
import sympy as sp
x,y,a = sp.symbols('x y a', positive=True)
inner = sp.integrate(sp.exp(-a*y) * sp.cos(x*y), (y, 0, sp.oo))  # closed form in y
# inner = a / (a**2 + x**2)
outer = sp.integrate(inner * sp.exp(-x**2), (x, -sp.oo, sp.oo))  # or hand to quadgk if hard
print(sp.simplify(inner))
				</div>
</li>
</ul>
<h4>Pseudocode: Deelstra’s comonotonic bound</h4>
<div class="pseudocode">
def deelstra_upper_bound(F_inv_list, K, r, T, quad_rule):
  # F_inv_list: list of quantile functions F_j^{-1}(p)
  def integrand(p):
	  avg = sum(F_inv(p) for F_inv in F_inv_list) / len(F_inv_list)
	  return max(avg - K, 0.0)
  total = 0.0
  for w, node in zip(quad_rule.weights, quad_rule.nodes):
	  total += w * integrand(node)
  return math.exp(-r*T) * total
		  </div>
<h4>Jargon note:</h4>
<ul>
<li><strong>p‑trivial:</strong> In probability theory, an event is “p‑trivial” if it has probability 0 or 1 under the measure \(P\). In integration contexts, a “p‑trivial” variable is one that is almost surely constant — integrating over it adds nothing, so you can drop that dimension entirely.</li>
<li><strong>Comonotonic:</strong> Random variables that move together perfectly — higher values of one always correspond to higher values of the others.</li>
</ul>
</div>
<!-- Dimension Reduction Funnel — drop-in SVG -->
<figure style="max-width:200mm;margin:1rem auto;">
<svg aria-labelledby="dim-red-title" height="auto" role="img" viewbox="0 0 960 560" width="100%" xmlns="http://www.w3.org/2000/svg">
<title id="dim-red-title">Dimension reduction funnel: from high-dimensional integral to 1D</title>
<desc>
			  Three stacked panels illustrate reducing a multivariate integral to lower dimensions using
			  Fubini's theorem, change of variables, symmetry, sufficient statistics, and comonotonic bounds.
			  Top: high-dimensional domain (3D cube proxy) with integrand f(x1,...,xd). Middle: reduced 2D integral.
			  Bottom: 1D integral along a single variable/statistic. Arrows annotate the techniques used.
			</desc>
<style>
			  .panel { fill:#fff; stroke:#e5e5e5; stroke-width:1.5; rx:10; }
			  .title { font:700 16px/1.3 system-ui, -apple-system, Segoe UI, Roboto, sans-serif; fill:#222; }
			  .label { font:600 14px/1.3 system-ui, -apple-system, Segoe UI, Roboto, sans-serif; fill:#333; }
			  .note  { font:12px/1.3 system-ui, -apple-system, Segoe UI, Roboto, sans-serif; fill:#555; }
			  .axis  { stroke:#888; stroke-width:1.2; stroke-linecap:round; }
			  .grid  { stroke:#cfcfcf; stroke-width:1; }
			  .curve-hi { fill:none; stroke:#4a90e2; stroke-width:2; }
			  .curve-2d { fill:none; stroke:#2ca02c; stroke-width:2.2; }
			  .curve-1d { fill:none; stroke:#d62728; stroke-width:2.4; }
			  .shade { fill:#d62728; opacity:0.12; }
			  .dot   { fill:#4a90e2; opacity:0.85; }
			  .arrow { stroke:#444; stroke-width:1.8; marker-end:url(#arr); fill:none; }
			  .tag   { fill:#fff7e6; stroke:#e8c48a; stroke-width:1; rx:6; }
			  .tagtxt{ font:12px/1.2 system-ui, -apple-system, Segoe UI, Roboto, sans-serif; fill:#7a4b00; }
			  .legend-swatch { rx:4; }
			</style>
<defs>
<marker id="arr" markerheight="8" markerwidth="10" orient="auto" refx="9" refy="4">
<path d="M0,0 L9,4 L0,8 Z" fill="#444"></path>
</marker>
</defs>
<!-- PANEL 1: High-dimensional domain (3D proxy) -->
<g transform="translate(40,30)">
<rect class="panel" height="150" width="880" x="0" y="0"></rect>
<text class="title" text-anchor="middle" x="440" y="26">High‑dimensional integral (proxy for d‑D)</text>
<!-- cube proxy -->
<g transform="translate(40,40)">
<!-- back face -->
<rect fill="#f7f7f7" height="90" stroke="#bbb" width="220" x="40" y="0"></rect>
<!-- front face -->
<rect fill="#fafafa" height="90" stroke="#999" width="220" x="110" y="30"></rect>
<!-- connectors -->
<line class="grid" x1="40" x2="110" y1="0" y2="30"></line>
<line class="grid" x1="260" x2="330" y1="0" y2="30"></line>
<line class="grid" x1="40" x2="110" y1="90" y2="120"></line>
<line class="grid" x1="260" x2="330" y1="90" y2="120"></line>
<!-- grid on front -->
<g>
<line class="grid" x1="110" x2="330" y1="60" y2="60"></line>
<line class="grid" x1="110" x2="330" y1="90" y2="90"></line>
<line class="grid" x1="156" x2="156" y1="30" y2="120"></line>
<line class="grid" x1="202" x2="202" y1="30" y2="120"></line>
<line class="grid" x1="248" x2="248" y1="30" y2="120"></line>
<line class="grid" x1="294" x2="294" y1="30" y2="120"></line>
</g>
<!-- scatter 'integrand features' -->
<circle class="dot" cx="140" cy="52" r="3"></circle>
<circle class="dot" cx="176" cy="72" r="3"></circle>
<circle class="dot" cx="222" cy="56" r="3"></circle>
<circle class="dot" cx="270" cy="88" r="3"></circle>
<circle class="dot" cx="310" cy="66" r="3"></circle>
<text class="label" x="360" y="110">f(x1,…,xd)</text>
</g>
<!-- axes label -->
<text class="note" x="560" y="80">Domain ⊂ ℝ^d, d ≫ 1 (illustrated as 3D)</text>
<text class="note" x="560" y="104">Identify structure: separability, symmetry, sufficient statistics</text>
</g>
<!-- Arrow 1: to 2D panel -->
<g>
<line class="arrow" x1="480" x2="480" y1="182" y2="220"></line>
<!-- technique tags -->
<g transform="translate(225,190)">
<rect class="tag" height="22" width="170" x="0" y="0"></rect>
<text class="tagtxt" x="8" y="15">Fubini (iterated integrals)</text>
</g>
<g transform="translate(410,190)">
<rect class="tag" height="22" width="170" x="0" y="0"></rect>
<text class="tagtxt" x="8" y="15">Change of variables</text>
</g>
<g transform="translate(595,190)">
<rect class="tag" height="22" width="130" x="0" y="0"></rect>
<text class="tagtxt" x="8" y="15">Symmetry</text>
</g>
</g>
<!-- PANEL 2: Reduced 2D integral -->
<g transform="translate(40,230)">
<rect class="panel" height="150" width="880" x="0" y="0"></rect>
<text class="title" text-anchor="middle" x="440" y="26">Reduced few‑dimensional integral (e.g., 2D)</text>
<!-- axes -->
<g transform="translate(80,40)">
<line class="axis" x1="0" x2="320" y1="90" y2="90"></line>
<line class="axis" x1="0" x2="0" y1="90" y2="0"></line>
<!-- curve -->
<path class="curve-2d" d="M0,80 C50,40 110,30 160,50 S260,110 320,60"></path>
<text class="note" x="330" y="90">u₁</text>
<text class="note" x="-10" y="12">u₂</text>
</g>
<text class="label" x="500" y="80">f(u₁,u₂) after decorrelation / reparam.</text>
<text class="note" x="500" y="104">Integrate one variable analytically if possible; numerically integrate the rest.</text>
</g>
<!-- Arrow 2: to 1D panel -->
<g>
<line class="arrow" x1="480" x2="480" y1="382" y2="420"></line>
<!-- technique tags -->
<g transform="translate(290,390)">
<rect class="tag" height="22" width="210" x="0" y="0"></rect>
<text class="tagtxt" x="8" y="15">Sufficient statistic (e.g., sum)</text>
</g>
<g transform="translate(510,390)">
<rect class="tag" height="22" width="230" x="0" y="0"></rect>
<text class="tagtxt" x="8" y="15">Comonotonic bound (Deelstra)</text>
</g>
</g>
<!-- PANEL 3: 1D integral -->
<g transform="translate(40,430)">
<rect class="panel" height="100" width="880" x="0" y="0"></rect>
<text class="title" text-anchor="middle" x="440" y="24">1D integral in reduced variable</text>
<!-- axis + 1D curve -->
<g transform="translate(80,40)">
<h3 id="section-8-2">Integrals over a Circle and Arbitrary Shapes</h3>
<h4>Circle / Disk (polar coordinates)</h4>
<div class="layman">
<p><strong>Idea (pizza-slice method):</strong> A disk is easiest to sweep in <em>radius</em> and <em>angle</em>. Imagine slicing a pizza: for each angle <em>θ</em>, move outward from the center (radius <em>r</em>) and add up the contributions of the function. The little area of a thin “ring slice” is not just <em>dr dθ</em>—it’s bigger the farther you are from the center. That stretch factor is the radius <strong>r</strong> (because a small angular step makes a bigger arc at larger radius). That’s why the Jacobian is <strong>r</strong>.</p>
<ul>
<li><strong>Angle</strong> tells you where you are around the circle.</li>
<li><strong>Radius</strong> tells you how far from the center you are.</li>
<li><strong>Jacobian = r</strong> scales the tiny area correctly.</li>
</ul>
<p><em>Fun fact:</em> For smooth periodic functions, the simple uniform trapezoid rule in angle often converges <em>spectacularly</em> fast because it “wraps around” perfectly.</p>
</div>
<div class="formal">
<p>Let the disk be \(D_R = \{(x,y)\in\mathbb{R}^2 : x^2 + y^2 \le R^2\}\). Use the polar map
  \[
    x = r\cos\theta,\quad y = r\sin\theta,\quad r\in[0,R],\ \theta\in[0,2\pi).
  \]
  The Jacobian matrix is
  \[
    J(r,\theta) =
    \begin{bmatrix}
      \partial x/\partial r &amp; \partial x/\partial \theta \\
      \partial y/\partial r &amp; \partial y/\partial \theta \\
    \end{bmatrix}
    =
    \begin{bmatrix}
      \cos\theta &amp; -r\sin\theta \\
      \sin\theta &amp; r\cos\theta
    \end{bmatrix},
  \]
  and \(|\det J| = r\). Thus, for integrable \(f\),
  \[
    \iint_{D_R} f(x,y)\,dx\,dy
    = \int_0^{2\pi}\!\int_0^R f(r\cos\theta, r\sin\theta)\, r\, dr\, d\theta.
  \]</p>
</div>
<div class="equation">
<p><strong>Change-of-variables &amp; quadrature:</strong></p>
<p>Map Gauss–Legendre nodes \(\{\xi_k,w_k\}_{k=1}^{n_r}\) on \([-1,1]\) to \([0,R]\):</p>
<p>
    \[
      r_k = \tfrac{R}{2}(1+\xi_k),\qquad \tilde w_k = \tfrac{R}{2}\, w_k.
    \]
  </p>
<p>Use uniform angles \(\theta_j = \tfrac{2\pi j}{n_\theta}\), \(j=0,\dots,n_\theta-1\), with weights \(w_{\theta j}=\tfrac{2\pi}{n_\theta}\) (trapezoid on a periodic function). Then
  \[
    I \approx \sum_{j=0}^{n_\theta-1}\sum_{k=1}^{n_r}
      w_{\theta j}\, \tilde w_k \, f(r_k\cos\theta_j, r_k\sin\theta_j)\, r_k.
  \]
  </p>
<p><em>Notes:</em> You can alternatively use radial rules designed for a weight \(r\) (e.g., Gauss–Jacobi with \(\alpha=1,\beta=0\) after an affine map) so the factor \(r\) is “baked into” the weights.</p>
</div>
<div class="pseudocode">
# Goal: I = ∬_{x^2+y^2 ≤ R^2} f(x,y) dx dy
# Use polar change of variables with Jacobian r.

input: function f(x,y), radius R, integers n_r, n_theta

# 1) Angular nodes &amp; weights (periodic trapezoid)
for j in 0..n_theta-1:
  theta[j] = 2*pi*j / n_theta
  w_theta[j] = 2*pi / n_theta

# 2) Radial nodes &amp; weights: Gauss-Legendre on [0,R]
#    Map from [-1,1] to [0,R]; include Jacobian (R/2)
xi[], w[] = gauss_legendre(n_r)         # on [-1,1]
for k in 1..n_r:
  r[k] = 0.5*R*(1 + xi[k])
  w_r[k] = 0.5*R*w[k]                  # from mapping

# 3) Double sum (note the extra factor r[k] from Jacobian of polar)
I = 0
for j in 0..n_theta-1:
  for k in 1..n_r:
    x = r[k] * cos(theta[j])
    y = r[k] * sin(theta[j])
    I += w_theta[j] * w_r[k] * f(x, y) * r[k]

return I

# Tips:
# - Increase n_theta if f varies rapidly with angle.
# - Increase n_r if f varies rapidly with radius or near r=R.
# - For smooth periodic dependence on theta, trapezoid converges very fast.
</div>
<hr/>
<h4>Arbitrary shape Ω ⊂ ℝ² (triangulate &amp; sum)</h4>
<div class="layman">
<p><strong>Idea (tile with triangles):</strong> Any wiggly shape can be covered by tiny, non-overlapping triangles—like a mosaic. Triangles are great because they’re flat and simple. We compute the integral on each triangle (by mapping it to a standard “reference” triangle) and then add everything up.</p>
<ul>
<li><strong>Mesh</strong> your shape into triangles.</li>
<li><strong>Flatten</strong> each triangle onto a simple reference triangle.</li>
<li><strong>Sample &amp; sum</strong> with a small set of well-chosen points (Gaussian quadrature).</li>
</ul>
<p><em>Fun fact:</em> The determinant of the affine map’s Jacobian on a triangle is constant and equals <strong>2 × area of the triangle</strong>.</p>
</div>
<div class="formal">
<p>Let \(\Omega\subset\mathbb{R}^2\) be a polygonal (or meshed) domain, partitioned into disjoint triangles:
  \[
    \Omega = \bigcup_{e=1}^E T_e,\qquad T_e \cap T_{e'} = \varnothing \ (e\neq e').
  \]
  For each triangle \(T_e\) with vertices \(v_1, v_2, v_3\), define the affine map from the reference triangle
  \[
    T_{\text{ref}} = \{(\xi,\eta): \xi\ge 0,\ \eta\ge 0,\ \xi+\eta\le 1\}
  \]
  by
  \[
    \phi_e(\xi,\eta) = v_1 + \xi(v_2 - v_1) + \eta(v_3 - v_1).
  \]
  Its Jacobian matrix \(J_e = [v_2 - v_1 \ \ v_3 - v_1]\) is constant on \(T_e\), and
  \(\ | \det J_e | = 2\,|T_e|\) where \(|T_e|\) is the area of triangle \(T_e\).
  Then
  \[
    \int_{\Omega} f(x,y)\,dx\,dy
    =
    \sum_{e=1}^E \int_{T_{\text{ref}}} f\big(\phi_e(\xi,\eta)\big)\, |\det J_e| \, d\xi\,d\eta.
  \]</p>
</div>
<div class="equation">
<p><strong>Reference-triangle quadrature rules:</strong> Nodes \((\xi_q,\eta_q)\) and weights \(w_q\) are defined on \(T_{\text{ref}}\) whose area is \(1/2\). A few standard choices:</p>
<ul>
<li><em>Degree-1 (centroid):</em> One point at \((\tfrac{1}{3},\tfrac{1}{3})\) with weight \(w=\tfrac{1}{2}\).</li>
<li><em>Degree-2 (three-point):</em> Points \((\tfrac{1}{6},\tfrac{1}{6}),(\tfrac{2}{3},\tfrac{1}{6}),(\tfrac{1}{6},\tfrac{2}{3})\),
      each with weight \(w=\tfrac{1}{6}\) (weights sum to \(1/2\)).</li>
</ul>
<p><strong>Per-triangle approximation:</strong>
  \[
    I_e \approx |\det J_e| \sum_{q=1}^{N_q} w_q\, f\big(\phi_e(\xi_q,\eta_q)\big),
  \qquad
  I \approx \sum_{e=1}^E I_e.
  \]</p>
</div>
<div class="pseudocode">
# Goal: I = ∬_Ω f(x,y) dx dy by triangulation and Gaussian quadrature.

input: function f(x,y), triangle list {T_e}, quadrature rule {(xi_q, eta_q, w_q)} on T_ref

I = 0
for each triangle T_e with vertices v1=(x1,y1), v2=(x2,y2), v3=(x3,y3):
  # 1) Build affine map phi_e and Jacobian J_e
  e1 = v2 - v1
  e2 = v3 - v1
  J = [e1 | e2]                    # 2x2 matrix
  detJ = abs(det(J))               # constant on T_e

  # 2) Quadrature in reference coordinates
  I_e = 0
  for each (xi_q, eta_q, w_q):
    x_q = v1.x + xi_q*e1.x + eta_q*e2.x
    y_q = v1.y + xi_q*e1.y + eta_q*e2.y
    I_e += w_q * f(x_q, y_q)

  # 3) Scale by |detJ|
  I += detJ * I_e

return I

# Tips:
# - Use a finer mesh or higher-degree quadrature if f changes rapidly.
# - For curved boundaries, either refine triangles near the boundary or use curved elements.
# - The determinant satisfies detJ = 2 * area(T_e).
</div>
<hr/>
<h4>Boundary integral (Green/Gauss)</h4>
<div class="layman">
<p><strong>Idea (walk the fence, skip the lawn):</strong> Instead of summing over the whole area, sometimes you can walk only along the boundary and get the same answer. This is perfect when what you care about is a “flux” through the boundary or the domain is easier to describe by its edges/curve.</p>
<ul>
<li>Turn a 2D area integral into a 1D boundary integral (when applicable).</li>
<li>Great for polygons, splines, or piecewise smooth curves.</li>
<li>Integrate along edges with standard 1D quadrature.</li>
</ul>
<p><em>Fun fact:</em> The area of any simple polygon can be computed just by walking its boundary (the “shoelace formula”).</p>
</div>
<div class="formal">
<p><strong>Green’s theorem (two equivalent forms, CCW orientation):</strong></p>
<p><em>Circulation form:</em>
  \[
    \iint_{\Omega} \left(\frac{\partial Q}{\partial x} - \frac{\partial P}{\partial y}\right)\, dA
    \;=\;
    \oint_{\partial\Omega} P\,dx + Q\,dy.
  \]
  <em>Divergence / Gauss form in 2D:</em>
  \[
    \iint_{\Omega} \nabla\!\cdot\! \mathbf{F}\, dA \;=\; \oint_{\partial\Omega} \mathbf{F}\cdot \mathbf{n}\, ds.
  \]
  To convert a given area integrand \(f\):
  <ul>
<li>Choose \(P,Q\) so that \(\partial Q/\partial x - \partial P/\partial y = f\), then integrate \(P\,dx+Q\,dy\) along \(\partial\Omega\); <em>or</em></li>
<li>Choose \(\mathbf{F}\) so that \(\nabla\!\cdot\!\mathbf{F} = f\), then integrate \(\mathbf{F}\cdot\mathbf{n}\) along \(\partial\Omega\).</li>
</ul>
<em>Examples:</em>
  \[
    \text{Area}(\Omega)=\iint_\Omega 1\, dA
      = \oint_{\partial\Omega} \tfrac{1}{2}(x\,dy - y\,dx)
      = \iint_\Omega \nabla\!\cdot\!(x,0)\,dA
      = \oint_{\partial\Omega} (x,0)\cdot \mathbf{n}\, ds.
  \]
  \[
    \iint_\Omega x\, dA
      = \oint_{\partial\Omega} \tfrac{1}{2}x^2\, dy
      \quad\text{(take }P=0,\ Q=\tfrac{1}{2}x^2\text{)}.
  \]
  </p>
</div>
<div class="equation">
<p><strong>Parametric boundary &amp; 1D quadrature:</strong> Let a boundary segment be parameterized by \(\gamma(t)=(x(t),y(t))\), \(t\in[a,b]\).
  Then
  \[
    \int_{\gamma} P\,dx + Q\,dy
    = \int_a^b \big( P(\gamma(t))\, x'(t) + Q(\gamma(t))\, y'(t) \big)\, dt.
  \]
  For a straight segment from \(a=(x_0,y_0)\) to \(b=(x_1,y_1)\), use
  \(\gamma(t)=a+t(b-a)\), \(t\in[0,1]\), so \(x'(t)=x_1-x_0\), \(y'(t)=y_1-y_0\).
  Apply Gauss–Legendre in 1D:</p>
<p>
  \[
    \int_0^1 g(t)\,dt \approx \sum_{j=1}^{n} w_j\, g(t_j),
  \]
  hence on the segment
  \[
    \int_{\text{seg}} P\,dx + Q\,dy
    \approx
    \sum_{j=1}^n w_j \Big[
      P(\gamma(t_j))\, (x_1-x_0) + Q(\gamma(t_j))\, (y_1-y_0)
    \Big].
  \]
  Summing over all segments gives \(\oint_{\partial\Omega}\).</p>
<p><em>Polygon area (shoelace) as a special case:</em> For vertices \((x_i,y_i)\), \(i=1,\dots,N\), with \((x_{N+1},y_{N+1})=(x_1,y_1)\),
  \[
    \text{Area} = \frac{1}{2}\sum_{i=1}^{N}\big(x_i y_{i+1} - x_{i+1} y_i\big).
  \]</p>
</div>
<div class="pseudocode">
# Goal: Convert ∬_Ω f dA to a boundary integral via Green, then do 1D quadrature.

input: polygon/spline boundary segments {gamma_e(t) on [0,1]}, choice of (P,Q) with dQ/dx - dP/dy = f,
       1D Gauss-Legendre nodes {t_j, w_j}

I = 0
for each boundary segment gamma_e(t) = (x_e(t), y_e(t)), t in [0,1], oriented CCW:
  dxdt(t) = derivative of x_e at t
  dydt(t) = derivative of y_e at t

  I_e = 0
  for j in 1..n:
    tj = t_j
    xj = x_e(tj); yj = y_e(tj)
    I_e += w_j * ( P(xj,yj)*dxdt(tj) + Q(xj,yj)*dydt(tj) )

  I += I_e

return I

# Notes:
# - Ensure boundary orientation is counterclockwise for the sign to match Green's theorem.
# - For piecewise-linear edges, dx/dt and dy/dt are constants per edge.
# - If using the divergence form, parameterize n and use F·n instead.
</div>
<h2 id="section-9">Quadrature in the Wild: From Physics to Finance</h2>
<p>Quadrature is a universal tool for solving real-world problems across everything about math &amp; science. Let's take a world tour of integration in action.</p>
<h3 id="section-9-1">Physics: The Path Integral</h3>
<div class="formal">
            In quantum mechanics, the probability amplitude for a particle to move from point A to point B is given by the path integral, which sums over all possible paths:
            \[
            K(B, A) = \int \mathcal{D}x(t)  e^{\frac{i}{\hbar} S[x(t)]}
            \]
            where \(S[x(t)]\) is the classical action for each path. This infinite-dimensional integral is notoriously difficult to compute, but in practice, physicists discretize time and use numerical quadrature on a high-dimensional space (via lattice methods). For simple potentials, Gaussian quadrature on each time slice can be efficient.
        </div>
<div class="layman">
<strong>Think of it like this:</strong> Imagine every possible wiggly path a photon could take from your flashlight to the wall. The path integral adds up the "quantumness" of each wild route. Quadrature helps us approximate this sum of all realities.
        </div>
<h3 id="section-9-2">Chemistry: The Electronic Structure Problem</h3>
<div class="formal">
            The energy of a molecule is determined by solving the Schrödinger equation for electrons. This involves integrating the electron repulsion terms over molecular orbitals:
            \[
            (ij|kl) = \iint \frac{\phi_i(\mathbf{r}_1) \phi_j(\mathbf{r}_1) \phi_k(\mathbf{r}_2) \phi_l(\mathbf{r}_2)}{|\mathbf{r}_1 - \mathbf{r}_2|}  d\mathbf{r}_1 d\mathbf{r}_2
            \]
            These 6-dimensional integrals (for each pair of electrons) are computed using Gaussian quadrature with atomic-specific grids. The choice of grid points and weights is critical for accuracy in codes like Gaussian or PySCF.
        </div>
<div class="layman">
<strong>Think of it like this:</strong> Calculating how electrons—those moody particles—avoid each other in a molecule is like predicting the drama at a dance party. Quadrature is the social algorithm that figures out who stands where to minimize awkward collisions.
        </div>
<h3 id="section-9-3">Biology: Pharmacokinetics</h3>
<div class="formal">
            In drug modeling, the concentration of a drug in the body over time is described by integral equations. For example, the area under the curve (AUC) is a key metric:
            \[
            \text{AUC} = \int_0^{\infty} C(t)  dt
            \]
            where \(C(t)\) is the drug concentration. This integral is often computed using adaptive quadrature (like Gauss-Kronrod) from experimental data points, especially since \(C(t)\) is exponential and decays rapidly.
        </div>
<div class="layman">
<strong>Think of it like this:</strong> How much medicine is in your system over time? Quadrature measures the total "dose hours" by adding up the concentration minute by minute, like counting the total rainfall from a storm.
        </div>
<div class="formal">
            In drug modeling, the concentration of a drug in the body over time is described by integral equations. For example, the area under the curve (AUC) is a key metric:
            \[
            \text{AUC} = \int_0^{\infty} C(t)  dt
            \]
            where \(C(t)\) is the drug concentration. This integral is often computed using adaptive quadrature (like Gauss-Kronrod) from experimental data points, especially since \(C(t)\) is exponential and decays rapidly.
        </div>
<div class="layman">
<strong>Think of it like this:</strong> How much medicine is in your system over time? Quadrature measures the total "dose hours" by adding up the concentration minute by minute, like counting the total rainfall from a storm.
        </div>
<h3 id="section-9-4">Maps &amp; Geography: Geospatial Quadrature</h3>
<div class="formal">
        To integrate fields on Earth (e.g., rainfall, irradiance) over a region, use the sphere or ellipsoid surface element 
        \[
        dA = R^2 \sin\theta \, d\theta \, d\phi
        \]
        or solid angle \(\, d\Omega \,\). Great circle domains, spherical polygons, or equal‑area pixelizations (e.g., HEALPix) all boil down to “sum values × areas.”
        </div>
<div class="formal">
        # Average over region Γ on sphere (radius R):
        \[
        \langle F \rangle = \frac{1}{\text{Area}(\Gamma)} \iint_{\Gamma} F(\theta,\phi)\, dA,\quad dA = R^2 \sin\theta\, d\theta\, d\phi
        \]
        # Approaches:
        # (1) Equal-area grids (e.g., HEALPix) → sum F_i w_i
        # (2) Lebedev nodes → high-order angular quadrature on S^2
        # (3) Polygonal region: clip nodes to Γ or triangulate the spherical polygon

        Given sample nodes \((\theta_i,\phi_i)\) with weights \(w_i\):
        \[
        I \approx \sum_{i \in \Gamma} w_i \, F(\theta_i,\phi_i), \qquad
        \text{Area}(\Gamma) \approx \sum_{i \in \Gamma} w_i
        \]
        (if weights integrate to 1 over the sphere)
        </div>
<div class="layman">
<strong>Why the \(\sin\theta\)?</strong> Imagine Earth wearing “latitude belts.” Belts near the equator are longer than those near the poles; \(\sin\theta\) scales each belt’s width to get the right area.
        </div>
<p><strong>Polygon areas on ellipsoid:</strong> Use spherical-excess or ellipsoidal algorithms; for raster data, sum cell means × cell areas. Quadrature is just “weighted summation.”</p>
<h3 id="section-9-5">DSP: Audio</h3>
<p><strong>Energy, filters, and spectral features:</strong> integrals of \(|x(t)|^2\), filter kernels, or windowed transforms. Trapezoid on uniform samples is surprisingly powerful for periodic/FFT workflows.</p>
<div class="formal">
        # Short-time energy over a frame:
        \[
        E \approx \Delta t \sum_n |x[n]|^2
        \]
        # Filter output integral (continuous):
        \[
        y(t_0) = \int h(\tau)\, x(t_0 - \tau)\, d\tau \;\approx\; \sum_i w_i\, h(\tau_i)\, x(t_0 - \tau_i)
        \]
        </div>
<div class="layman">
<strong>Fun fact:</strong> Your ears handle ~\(10^{12}\) variation in sound power (≈120 dB). Energy integrals quantify “how loud” a clip is, not just how high the peaks are.
        </div>
<h3 id="section-9-6">Graphics: Rendering</h3>
<p><strong>Hemisphere sampling (lighting):</strong> integrate BRDF × incoming radiance over a hemisphere. Cosine‑weighted quadrature matches the physics of Lambertian surfaces.</p>
<div class="formal">
        # Outgoing radiance:
        \[
        L_o = \int_{\text{hemisphere}} f_r(\omega_i)\, L_i(\omega_i)\, \cos\theta_i\, d\omega_i
        \]
        Choose nodes \(\omega_i\) with weights \(w_i\) (cosine-weighted):
        \[
        L_o \approx \sum_i w_i\, f_r(\omega_i)\, L_i(\omega_i)\, \cos\theta_i
        \]
        </div>
<p><strong>Radiosity / form factors:</strong> double integrals over surfaces; discretize patches and apply Gauss rules per patch pair.</p>
<div class="layman">
<strong>Picture it:</strong> You’re sampling the dome of light above each pixel. More samples near the normal (cosine‑weighting) because those rays contribute most.
        </div>
<h3 id="section-9-7">Graphics: Quadrature for Animation and Quaternions</h3></g></g></svg></figure></figure></h4>

  In computer graphics and animation, <strong>quadrature</strong> (numerical integration) is essential for computing smooth motion, blending rotations, and simulating physics. When rotations are represented by <strong>quaternions</strong>, integration ensures continuous and stable interpolation over time.
<div class="formal">
<ul>
<li>
<strong>Quaternion Basics</strong>:  
      A quaternion \( q = w + xi + yj + zk \) represents a rotation in 3D space without gimbal lock.  
      Normalized quaternions satisfy:
      \[
        \|q\| = \sqrt{w^2 + x^2 + y^2 + z^2} = 1.
      \]
    </li>
<li>
<strong>Quaternion Differential Equation</strong>:  
      For angular velocity vector \(\boldsymbol{\omega}(t)\), the quaternion derivative is:
      \[
        \dot{q}(t) = \frac{1}{2}\,\Omega(\boldsymbol{\omega}(t))\,q(t),
      \]
      where
      \[
        \Omega(\boldsymbol{\omega}) =
        \begin{bmatrix}
          0 &amp; -\omega_x &amp; -\omega_y &amp; -\omega_z \\
          \omega_x &amp; 0 &amp; \omega_z &amp; -\omega_y \\
          \omega_y &amp; -\omega_z &amp; 0 &amp; \omega_x \\
          \omega_z &amp; \omega_y &amp; -\omega_x &amp; 0
        \end{bmatrix}.
      \]
    </li>
<li>
<strong>Numerical Integration for Animation</strong>:  
      To update orientation over time:
      \[
        q(t+\Delta t) \approx q(t) + \Delta t \cdot \dot{q}(t),
      \]
      then normalize \( q \) to maintain unit length. Higher-order quadrature (e.g., Runge–Kutta) improves accuracy:
      \[
        q_{n+1} = q_n + \frac{\Delta t}{6}(k_1 + 2k_2 + 2k_3 + k_4),
      \]
      where \(k_i\) are intermediate slopes from the quaternion ODE.
    </li>
<li>
<strong>Spherical Linear Interpolation (SLERP)</strong>:  
      For keyframe animation, interpolate between \(q_0\) and \(q_1\) on the unit sphere:
      \[
        \text{SLERP}(q_0,q_1;t) =
        \frac{\sin((1-t)\theta)}{\sin\theta}q_0 +
        \frac{\sin(t\theta)}{\sin\theta}q_1,\quad
        \theta = \arccos(q_0\cdot q_1).
      \]
      This ensures constant angular velocity and smooth motion.
    </li>
</ul>
<h5>Real-World Applications</h5>
<ul>
<li><strong>Character Animation:</strong> Smooth blending of skeletal rotations using quaternion integration.</li>
<li><strong>Camera Motion:</strong> Interpolating camera orientation for cinematic paths.</li>
<li><strong>Physics Engines:</strong> Integrating angular velocity to update rigid body orientation.</li>
<li><strong>VR/AR:</strong> Stable head-tracking and orientation prediction using quaternion ODE integration.</li>
</ul>
<h5>Pseudocode: Quaternion Integration with RK4</h5>
<div class="pseudocode">
// Inputs: q (unit quaternion), omega (angular velocity vector), dt (time step)
function integrate_quaternion(q, omega, dt):
    def dqdt(q, omega):
        Omega = [[0, -omega.x, -omega.y, -omega.z],
                 [omega.x, 0, omega.z, -omega.y],
                 [omega.y, -omega.z, 0, omega.x],
                 [omega.z, omega.y, -omega.x, 0]]
        return 0.5 * Omega * q

    k1 = dqdt(q, omega)
    k2 = dqdt(q + 0.5*dt*k1, omega)
    k3 = dqdt(q + 0.5*dt*k2, omega)
    k4 = dqdt(q + dt*k3, omega)

    q_new = q + (dt/6)*(k1 + 2*k2 + 2*k3 + k4)
    return normalize(q_new)
  </div>
</div>
<h3 id="section-9-8">Video Games: Physics</h3>
<p>Quadrature handles impulses, energies, occlusion integrals, and optimal control costs. Smooth gameplay often hides a lot of integrals under the hood.</p>
<div class="formal">
        # Controller cost:
        \[
        J = \int_0^T \big(q\, x(t)^2 + r\, u(t)^2\big)\, dt
        \;\approx\; \sum_k w_k \big(q\, x(t_k)^2 + r\, u(t_k)^2\big)
        \]
        </div>
<div class="layman">
<strong>Analogy:</strong> It’s like scoring how “wobbly” and “throttle‑heavy” your car was over a lap. Integrate wobble and throttle over time to tune a smoother ride.
        </div>
<h3 id="section-9-9">Real‑Estate: Architecture</h3>
<p>Daylight/insolation on façades: integrate the sun path and visibility over time and angles. Shadowing splits the domain → adaptive rules shine (pun intended).</p>
<div class="formal">
        # Annual insolation at a point:
        \[
        Q = \int_{\text{year}} \int_{\text{sun-visible}} I_{\text{sun}}(\alpha,\beta,t)\, \cos(\text{incidence})\, d\Omega\, dt
        \;\approx\; \sum_{t,\Omega} w_{t,\Omega}\, I_{\text{sun}}\, \cos(\text{incidence})
        \]
        </div>
<div class="layman">
<strong>Rule of thumb:</strong> A tiny change in panel tilt can integrate to big gains over a whole year. Quadrature helps find that sweet spot.
        </div>
<h3 id="section-9-10">Shipbuilding: Naval</h3>
<p>Hydrostatics and resistance: integrate pressure or sectional areas along the hull. Traditional “curves of areas” are made for Simpson/Boole’s rules.</p>
<div class="formal">
        # Displacement volume:
        \[
        V = \int_0^L A(x)\, dx \;\approx\; \text{Simpson}(A(x), x \in [0,L])
        \]
        </div>
<div class="layman">
<strong>Fun fact:</strong> Long before GPUs, shipwrights used Simpson’s rule on paper to estimate displacement from station drawings.
        </div>
<h3 id="section-9-11">Navigation / Logistics</h3>
<p>Fuel/energy along a route: integrate consumption rate over time/distance. If grades or winds change sharply, adaptive quadrature saves calls.</p>
<div class="formal">
        # Fuel:
        \[
        \text{Fuel} = \int \text{rate}(s(t), \text{grade}(t), \text{load})\, dt
        \;\approx\; \sum_k w_k\, \text{rate}(s_k,\text{grade}_k,\text{load})
        \]
        </div>
<div class="layman">
<strong>Real world:</strong> Two routes with the same length can burn very different fuel if one has hills. The integral remembers every slope.
        </div>
<h3 id="section-9-12">Remote Sensing: Crude Oil Detection</h3>
<p>Oil films change spectral reflectance and polarization. We detect via band‑integrated reflectance and angular models, then fuse evidence across pixels.</p>
<div class="formal">
        # Band-integrated reflectance in sensor band [λ1, λ2] with spectral response S(λ):
        \[
        R_{\text{band}} \;=\; \frac{\displaystyle \int_{\lambda_1}^{\lambda_2} R(\lambda)\, S(\lambda)\, d\lambda}
        {\displaystyle \int_{\lambda_1}^{\lambda_2} S(\lambda)\, d\lambda}
        \]

        # BRDF angular integration (sun-sensor geometry):
        \[
        L_{\text{TOA}} \;\approx\; \iint_{\Omega_{\text{surf}}} f_r(\lambda, \omega_i \rightarrow \omega_o)\, E_{\text{sun}}(\lambda,\omega_i)\, \cos\theta_i \, d\omega_i
        \]

        # Probabilistic detection (per pixel):
        \[
        \log \mathcal{L}(\text{oil} \mid \mathbf{R}) \;=\; \sum_b \log p\!\left( R_b \,\middle|\, \mu_b^{\text{oil}}, \sigma_b^{\text{oil}} \right)
        \]
        </div>
<div class="layman">
<strong>Why the rainbow sheen?</strong> Thin films create interference—different wavelengths amplify/cancel at different thicknesses, changing the integral across the band the satellite sees.
        </div>
<h3 id="section-9-13">Crystal Science: Diffraction</h3>
<p>Structure factors integrate electron density against complex exponentials. Powder diffraction averages orientations over the sphere—perfect for spherical quadrature.</p>
<div class="formal">
        # Structure factor at scattering vector q:
        \[
        F(\mathbf{q}) = \sum_{j=1}^{N} f_j(q)\, e^{\,i\, \mathbf{q}\cdot \mathbf{r}_j}
        \]

        # Debye scattering (pairwise distances r_{ij}):
        \[
        I(q) \propto \sum_{i=1}^{N} \sum_{j=1}^{N} f_i(q) f_j(q)\, \frac{\sin(q\, r_{ij})}{q\, r_{ij}}
        \]

        # Powder average (orientational integral on S^2):
        \[
        I_{\text{powder}}(q) = \frac{1}{4\pi} \int_{S^2} \big|F(R\,\mathbf{q})\big|^2 \, d\Omega
        \;\approx\; \sum_{\ell} w_\ell\, \big|F(R_\ell \mathbf{q})\big|^2
        \]
        (Use Lebedev nodes \(R_\ell\), weights \(w_\ell\))
        </div>
<div class="layman">
<strong>Think of it:</strong> X‑ray diffraction is the crystal’s “barcode.” The integral sums waves scattered by each atom; peaks appear where all waves add in sync.
        </div>
<h3 id="section-9-14">Lasers: Mode Overlap</h3>
<p>Coupling efficiency is a normalized overlap of modes. Gaussian/Hermite rules excel for analytic beams; FE rules shine in complex waveguides.</p>
<div class="formal">
        \[
        \eta = \frac{\big|\iint E_1(x,y)\, E_2^*(x,y)\, dx\, dy\big|^2}
        {\big(\iint |E_1|^2\big)\,\big(\iint |E_2|^2\big)}
        \;\approx\; \sum_e \sum_q w_{e,q}\, (E_1\, E_2^*)
        \]
        </div>
<div class="layman">
<strong>Laser handshake:</strong> Modes couple best when their “shapes” match. The integral is a compatibility score between the beams.
        </div>
<h3 id="section-9-15">Astrophysics</h3>
<p>Distances, band fluxes, and lensing—all are integrals. Smooth 1D rules often suffice; angular pieces use spherical quadrature.</p>
<div class="formal">
        # Luminosity distance (flat ΛCDM; generalize with curvature if needed):
        \[
        D_L(z) = (1+z)\, \frac{c}{H_0} \int_0^{z} \frac{dz'}{E(z')}, \quad
        E(z) = \sqrt{\Omega_m(1+z)^3 + \Omega_\Lambda + \Omega_r(1+z)^4}
        \]

        # Band-integrated flux through a filter T(ν):
        \[
        F_{\text{band}} = \frac{\displaystyle \int f_\nu(\nu)\, T(\nu)\, d\nu}{\displaystyle \int T(\nu)\, d\nu}
        \]

        # (Sketch) Weak lensing potential (2D convolution integral):
        \[
        \psi(\boldsymbol{\theta}) = \frac{1}{\pi} \iint \kappa(\boldsymbol{\theta}')\,
        \ln \big|\boldsymbol{\theta} - \boldsymbol{\theta}'\big| \, d^2\theta'
        \]
        </div>
<div class="layman">
<strong>Cosmic road trip:</strong> \(D_L\) integrates how the expansion stretches space between us and a galaxy. Filters then integrate a galaxy’s light through the telescope’s “color glasses.”
        </div>
<h3 id="section-9-16">Finance: Local Volatility and Beyond</h3>
<p>In finance, quadrature powers some of the most sophisticated models. Let's dive deeper.</p>
<h4>Local Volatility (Dupire's Formula)</h4>
<div class="formal">
            Dupire's formula calculates the local volatility surface from market option prices:
            \[
            \sigma_{\text{local}}(S, t)^2 = \frac{\frac{\partial C}{\partial T} + (r - q)S \frac{\partial C}{\partial S} + q C}{\frac{1}{2} S^2 \frac{\partial^2 C}{\partial S^2}}
            \]
            The derivatives \(\frac{\partial C}{\partial T}\), \(\frac{\partial C}{\partial S}\), and \(\frac{\partial^2 C}{\partial S^2}\) are computed from smoothed option price data using numerical differentiation. Quadrature is used implicitly when integrating the local volatility PDE to price options.
        </div>
<div class="layman">
<strong>Think of it like this:</strong> Local volatility is the "mood swings" of the market at every possible stock price and time. Quadrature helps us deduce these mood swings from the option prices traders are paying.
        </div>
<h4>SABR Model</h4>
<div class="formal">
            The SABR model (Stochastic Alpha Beta Rho) describes the dynamics of forward rates with stochastic volatility:
            \[
            dF = \alpha F^\beta dW_1, \quad d\alpha = \nu \alpha dW_2, \quad \mathbb{E}[dW_1 dW_2] = \rho dt
            \]
            Option prices under SABR are computed using asymptotic expansions or by numerical integration of the probability density. For example, the option price can be written as:
            \[
            C(K) = \int_{0}^{\infty} \int_{0}^{\infty} (F - K)^+ p(F, \alpha)  dF d\alpha
            \]
            where \(p(F, \alpha)\) is the joint density. This 2D integral is efficiently computed with Gaussian quadrature or sparse grids.
        </div>
<div class="layman">
<strong>Think of it like this:</strong> SABR is like modeling a car's speed (the forward rate) and the driver's mood (volatility) simultaneously. Quadrature adds up all the possible outcomes of speed and mood swings to price the option.
        </div>
<h4>Heston Model</h4>
<div class="formal">
            The Heston model we've seen earlier has a closed-form characteristic function, allowing option prices to be computed via Fourier quadrature (Carr-Madan). However, the integral can be oscillatory, and adaptive quadrature is often used to handle the oscillations efficiently.
        </div>
<div class="layman">
<strong>Think of it like this:</strong> The Heston model is the market's heartbeat—sometimes steady, sometimes erratic. Quadrature listens to the heartbeat's frequency components to price options.
        </div>
<h4>Stochastic Local Volatility (SLV)</h4>
<div class="formal">
            SLV models combine local volatility and stochastic volatility to capture the best of both worlds. The option price is given by:
            \[
            C = e^{-rT} \int_{0}^{\infty} \int_{0}^{\infty} (S - K)^+ p(S, v)  dS dv
            \]
            where \(p(S, v)\) is the joint density of stock price and volatility. This 2D integral is computed using quadrature on a grid, often with adaptive methods in the volatility dimension.
        </div>
<div class="layman">
<strong>Think of it like this:</strong> SLV is like having a weather model that accounts for both local humidity (local vol) and global storm systems (stochastic vol). Quadrature is the supercomputer that adds up all the weather scenarios.
        </div>
<h2 id="section-10">FFT: The Quadrature in Disguise</h2>
<p>The Fast Fourier Transform (FFT) is not just a fast algorithm; it's a powerful quadrature rule in disguise. Let's uncover the connection.</p>
<h3 id="section-10-1">Chebyshev Nodes and Clenshaw-Curtis</h3>
<div class="formal">
            Recall that Clenshaw-Curtis quadrature uses Chebyshev nodes:
            \[
            x_k = \cos\left(\frac{k\pi}{n}\right), \quad k = 0, 1, \dots, n
            \]
            These nodes are the roots of Chebyshev polynomials and are clustered near the endpoints, which helps capture endpoint behavior. The weights for Clenshaw-Curtis can be computed via the FFT because the discrete cosine transform (DCT) is intimately related to the Chebyshev series.
        </div>
<div class="layman">
<strong>Think of it like this:</strong> Chebyshev nodes are like placing more sensors near the edges of a drum—where the skin is tightest and most sensitive. The FFT is the quick way to calculate the drum's sound from those sensors.
        </div>
<h3 id="section-10-2">FFT as a Quadrature Rule</h3>
<div class="formal">
            The FFT itself can be viewed as a quadrature rule for integrating periodic functions. Consider integrating a periodic function \(f(x)\) over \([0, 2\pi]\):
            \[
            I = \int_{0}^{2\pi} f(x)  dx
            \]
            If we sample \(f(x)\) at \(n\) equally spaced points \(x_j = 2\pi j / n\), the trapezoidal rule gives:
            \[
            I \approx \frac{2\pi}{n} \sum_{j=0}^{n-1} f(x_j)
            \]
            But for periodic functions, the trapezoidal rule is exponentially accurate! The FFT can be used to compute this sum quickly, especially if \(f(x)\) is smooth and periodic.
        </div>
<div class="layman">
<strong>Think of it like this:</strong> The FFT is like a revolving door that smoothly integrates people flowing in and out of a building. For periodic events (like rush hour), it's incredibly efficient.
        </div>
<h3 id="section-10-3">Convolution: The Integral that FFT Loves</h3>
<div class="formal">
            Convolution is a fundamental operation in math and engineering:
            \[
            (f * g)(t) = \int_{-\infty}^{\infty} f(\tau) g(t - \tau)  d\tau
            \]
            This integral appears everywhere: in signal processing (filtering), probability (sum of random variables), and finance (option pricing with stochastic processes). The convolution theorem states that the Fourier transform of a convolution is the product of the Fourier transforms:
            \[
            \mathcal{F}\{f * g\} = \mathcal{F}\{f\} \cdot \mathcal{F}\{g\}
            \]
            So, to compute convolution, we can FFT both functions, multiply, and then inverse FFT. This is often faster than direct numerical integration.
        </div>
<div class="layman">
<strong>Think of it like this:</strong> Convolution is like blending two smoothies together. The FFT method is like using a super-blender that instantly combines their flavors, rather than stirring slowly with a spoon (quadrature).
        </div>
<h3 id="section-10-4">Quadrature for Convolution</h3>
<div class="formal">
            But what if we want to use quadrature directly for convolution? We can discretize the integral:
            \[
            (f * g)(t) \approx \sum_{i} w_i f(\tau_i) g(t - \tau_i)
            \]
            This is a quadrature rule! However, for each \(t\), we need to evaluate the sum, which is expensive. The FFT method computes this for all \(t\) simultaneously in \(O(n \log n)\) time, whereas direct quadrature would be \(O(n^2)\).
        </div>
<div class="layman">
<strong>Think of it like this:</strong> Direct quadrature for convolution is like hand-mixing every possible combination of ingredients. The FFT is like using a mixer with many blades—it does all the combinations at once.
        </div>
<h2 id="section-11">Time is Money: Quadrature for Bermudan Options</h2>
<p>Bermudan options can be exercised at specific dates before expiration. Pricing them requires solving a dynamic programming problem, and quadrature plays a key role.</p>
<h3 id="section-11-1">The Dynamic Programming Equation</h3>
<div class="formal">
            The value of a Bermudan option at time \(t_i\) is:
            \[
            V(S, t_i) = \max\left( \text{payoff}(S), e^{-r \Delta t} \mathbb{E}[V(S_{t_{i+1}}, t_{i+1}) | S_{t_i} = S] \right)
            \]
            The conditional expectation is an integral:
            \[
            \mathbb{E}[V(S_{t_{i+1}}) | S] = \int_{0}^{\infty} V(S', t_{i+1}) p(S' | S)  dS'
            \]
            where \(p(S' | S)\) is the transition density. This integral is computed using quadrature at each time step.
        </div>
<div class="layman">
<strong>Think of it like this:</strong> Pricing a Bermudan option is like deciding whether to take a gift now or wait for a potentially better gift later. Quadrature helps us calculate the value of waiting by adding up all future possibilities.
        </div>
<h3 id="section-11-2">Quadrature Methods</h3>
<div class="formal">
            We can use Gaussian quadrature with points tailored to the transition density. For example, if the density is log-normal, Gauss-Laguerre quadrature is efficient. The process:
            <ol>
<li>Discretize the asset price space into quadrature points \(\{S_j\}\) at each exercise date.</li>
<li>At the final date, set \(V(S, T) = \text{payoff}(S)\).</li>
<li>Move backwards in time: for each date, compute the continuation value at each \(S_j\) using quadrature:
                    \[
                    C(S_j) = e^{-r \Delta t} \sum_{k} w_k V(S_k, t_{i+1}) p(S_k | S_j)
                    \]
                </li>
<li>Set \(V(S_j, t_i) = \max(\text{payoff}(S_j), C(S_j))\).</li>
</ol>
            This is called the quadrature method for Bermudan options.
        </div>
<div class="layman">
<strong>Think of it like this:</strong> It's like solving a maze backwards—starting from the end and working back to the beginning, using quadrature to shine a light on the best path at each step.
        </div>
<h3 id="section-11-3">FFT Acceleration</h3>
<div class="formal">
            If the transition density has a closed-form characteristic function, the expectation can be written as a convolution. Then, FFT can be used to compute the continuation value for all asset prices simultaneously. This is faster than quadrature for large grids.
        </div>
<div class="layman">
<strong>Think of it like this:</strong> FFT for Bermudan options is like using a megaphone to broadcast the value of waiting to all asset prices at once, rather than whispering to each one individually.
        </div>
<h2 id="section-12">The Grand Reduction: Taming a High-Dimensional Integral</h2>
<p>Let's conjure a practical example from finance: pricing a basket option on multiple assets under a multivariate Black-Scholes model. We'll start with a high-dimensional integral and reduce it step by step.</p>
<h3 id="section-12-1">The Problem</h3>
<div class="formal">
            Consider a basket of \(d\) assets with prices \(S_1, S_2, \dots, S_d\) following correlated geometric Brownian motion. The basket call option payoff at time \(T\) is:
            \[
            \text{payoff} = \max\left( \sum_{i=1}^d w_i S_i(T) - K, 0 \right)
            \]
            The risk-neutral price is:
            \[
            C = e^{-rT} \int_{\mathbb{R}^d} \max\left( \sum_{i=1}^d w_i S_i(0) e^{(r - \frac{1}{2}\sigma_i^2)T + \sigma_i \sqrt{T} z_i} - K, 0 \right) \phi_d(\mathbf{z}; \Sigma)  d\mathbf{z}
            \]
            where \(\phi_d(\mathbf{z}; \Sigma)\) is the multivariate normal density with correlation matrix \(\Sigma\). This is a \(d\)-dimensional integral.
        </div>
<h3 id="section-12-2">Step 1: Change of Variables - Cholesky Decomposition</h3>
<div class="formal">
            We decompose \(\Sigma = L L^\top\), where \(L\) is lower triangular. Then, we set \(\mathbf{z} = L \mathbf{y}\), where \(\mathbf{y} \sim \mathcal{N}(0, I)\). The integral becomes:
            \[
            C = e^{-rT} \int_{\mathbb{R}^d} \max\left( \sum_{i=1}^d w_i S_i(0) e^{(r - \frac{1}{2}\sigma_i^2)T + \sigma_i \sqrt{T} (L \mathbf{y})_i} - K, 0 \right) \phi(y_1) \phi(y_2) \cdots \phi(y_d)  d\mathbf{y}
            \]
            Now the integrand is a product of independent normal densities.
        </div>
<div class="layman">
<strong>Think of it like this:</strong> We're untangling the correlated assets into independent factors, like separating intertwined threads.
        </div>
<h3 id="section-12-3">Step 2: Dimension Reduction via Conditioning</h3>
<div class="formal">
            If the basket is large, we can use a conditioning variable. Let \(X = \sum_{i=1}^d w_i S_i(T)\). We can write:
            \[
            C = e^{-rT} \mathbb{E}[\max(X - K, 0)] = e^{-rT} \int_{0}^{\infty} \mathbb{P}(X &gt; k)  dk
            \]
            But \(\mathbb{P}(X &gt; k)\) is still hard to compute. Alternatively, we can use a comonotonic approximation or principal component analysis (PCA) to reduce the dimension.
        </div>
<h3 id="section-12-4">Step 3: PCA Reduction</h3>
<div class="formal">
            Perform PCA on the correlation matrix \(\Sigma\). The first few principal components explain most of the variance. Let \(P\) be the matrix of eigenvectors. Then, \(\mathbf{z} = P \mathbf{u}\), where \(\mathbf{u}\) are the principal components. We can truncate the sum after \(m &lt; d\) components:
            \[
            z_i \approx \sum_{j=1}^m P_{ij} u_j
            \]
            The integral becomes \(m\)-dimensional. For example, if \(m=2\), we have:
            \[
            C \approx e^{-rT} \int_{\mathbb{R}^2} \max\left( \sum_{i=1}^d w_i S_i(0) e^{(r - \frac{1}{2}\sigma_i^2)T + \sigma_i \sqrt{T} \sum_{j=1}^2 P_{ij} u_j} - K, 0 \right) \phi(u_1) \phi(u_2)  du_1 du_2
            \]
        </div>
<div class="layman">
<strong>Think of it like this:</strong> PCA is like finding the main actors in a play—the few that drive the plot. We focus on them and ignore the extras.
        </div>
<h3 id="section-12-5">Step 4: Numerical Integration</h3>
<div class="formal">
            Now we have a 2D integral. We can compute it using Gaussian quadrature:
            \[
            C \approx e^{-rT} \sum_{k=1}^{n_1} \sum_{l=1}^{n_2} w_k w_l  \max\left( \sum_{i=1}^d w_i S_i(0) e^{(r - \frac{1}{2}\sigma_i^2)T + \sigma_i \sqrt{T} (P_{i1} u_k + P_{i2} u_l)} - K, 0 \right)
            \]
            where \(u_k\) and \(u_l\) are Gauss-Hermite nodes, and \(w_k, w_l\) are the corresponding weights.
        </div>
<h3 id="section-12-6">Step 5: Validation with Monte Carlo</h3>
<div class="formal">
            To validate, we set up a Monte Carlo simulation:
            <ol>
<li>Generate \(N\) samples of \(\mathbf{y} \sim \mathcal{N}(0, I_d)\).</li>
<li>Transform to \(\mathbf{z} = L \mathbf{y}\).</li>
<li>Compute \(S_i(T) = S_i(0) e^{(r - \frac{1}{2}\sigma_i^2)T + \sigma_i \sqrt{T} z_i}\) for each asset.</li>
<li>Compute the basket value \(X = \sum_{i} w_i S_i(T)\).</li>
<li>Average the payoff: \(C \approx e^{-rT} \frac{1}{N} \sum \max(X - K, 0)\).</li>
</ol>
            We use variance reduction techniques like antithetic sampling to improve accuracy.
        </div>
<div class="layman">
<strong>Think of it like this:</strong> Monte Carlo is the party where we invite millions of random scenarios to see how they behave. Quadrature is the careful interview of a few representative scenarios.
        </div>
<h3 id="section-12-7">Step 6: Compare Results</h3>
<div class="formal">
            For a basket with \(d=10\) assets, we might find that PCA with \(m=2\) components captures 95% of the variance. The quadrature method with \(20 \times 20 = 400\) points might be faster and more accurate than Monte Carlo with \(100,000\) paths.
        </div>
<div class="key-point">
<strong>Key Insight:</strong> Dimension reduction is like compressing a high-resolution image into a smaller file without losing important details. Quadrature then works efficiently on the compressed version.
        </div>
<!-- ... (The existing Conclusion and References sections follow here) ... -->
<h2 id="section-13">Conclusion</h2>
<p>
			  Quadrature methods aren’t just a set of dusty formulas from a numerical analysis textbook — they’re a precision‑engineered toolkit for turning hairy integrals into neat, well‑behaved numbers. In financial mathematics, they let you move seamlessly from the humble trapezoidal rule to the Rolls‑Royce of Gaussian quadrature, choosing the right ride for the terrain between speed and accuracy.
			</p>
<p>
			  In option pricing, quadrature can be the Goldilocks method: not as assumption‑bound as closed‑form solutions, not as variance‑hungry as Monte Carlo. Whether you’re bending the problem into a strike‑aware integral, flipping it into Fourier space, or re‑casting it as a stop‑loss premium, quadrature has a way of finding the sweet spot — integrating form and function, if you will.
			</p>
<div class="key-point">
<strong>Practical Implementation Tips:</strong>
<ul>
<li>Test convergence with increasing node counts — don’t just take it on faith.</li>
<li>Choose transforms to expose natural weights — let the integrand show you where it wants to be sampled.</li>
<li>Prefer nested rules for adaptivity — reuse points, save cycles, keep your grid from going off on a tangent.</li>
<li>Vectorize integrand evaluations — because looping over points one‑by‑one is so last century.</li>
<li>For per‑strike precision, use quadrature; for whole surfaces, use FFT — know when to sum it up and when to transform.</li>
</ul>
</div>
<div class="layman">
<strong>Remember:</strong> Quadrature is like hiring a team of elite surveyors who know exactly where to stand to measure the landscape. Monte Carlo is like sending a million tourists with tape measures and hoping the average works out. Both have their place, but when you need to “integrate” into the market quickly, quadrature can help you avoid going off on a random walk. Oh here's a joke - “Why did the quadrature rule break up with Monte Carlo?
Because it wanted something more deterministic in its life!”
			</div>
<p>
			  So, as you head off to tackle your next pricing problem, keep your wits sharp and your weights positive. Don’t be afraid to change variables if the view looks better from another coordinate system. And if anyone tells you integration is boring, just smile and say: “I’ve got <em>bounds</em> on that opinion.” After all, in the grand sum of things, it’s not just about getting the area under the curve — it’s about making every point count.
			</p>
<div class="footer">
<h3 id="section-13-1">References &amp; Further Reading</h3>
<ol>
<li>Carr, P. &amp; Madan, D. (1999). Option valuation using the fast Fourier transform. <em>Journal of Computational Finance</em>.</li>
<li>Press, W.H., Teukolsky, S.A., Vetterling, W.T., Flannery, B.P. <em>Numerical Recipes: The Art of Scientific Computing</em> (3rd ed.). Cambridge University Press. (The definitive handbook for numerical algorithms).</li>
<li>Golub, G. H., &amp; Welsch, J. H. (1969). Calculation of Gauss quadrature rules. <em>Mathematics of Computation</em>, 23(106), 221–230. (The seminal Golub-Welsch paper).</li>
<li>Trefethen, L. N. (2008). Is Gauss quadrature better than Clenshaw–Curtis? <em>SIAM Review</em>, 50(1), 67–87. (A fantastic and surprising comparison).</li>
<li>Mori, M. (1974). Quadrature formulas obtained by variable transformation. <em>Publications of the Research Institute for Mathematical Sciences</em>, 9(3), 121–130. (The original Japanese publication on tanh-sinh quadrature).</li>
<li>Deelstra, G., Liinev, J., &amp; Vanmaele, M. (2004). Pricing of arithmetic basket options by conditioning. <em>Insurance: Mathematics and Economics</em>, 34(1), 55–77.</li>
<li>Heston, S. L. (1993). A closed-form solution for options with stochastic volatility with applications to bond and currency options. <em>The Review of Financial Studies</em>, 6(2), 327–343.</li>
<li>Hagan, P. S., Kumar, D., Lesniewski, A. S., &amp; Woodward, D. E. (2002). Managing smile risk. <em>Wilmott Magazine</em>, 1, 84–108. (The SABR model paper).</li>
<!-- NEW REFERENCES -->
<li>Heinrich, S. (2001). Quantum integration in Sobolev classes. <em>Journal of Complexity</em>, 17(1), 1-16. (Theoretical foundations of quantum numerical integration).</li>
<li>Rebentrost, P., Gupt, B., &amp; Bromley, T. R. (2018). Quantum computational finance: Monte Carlo pricing of financial derivatives. <em>Physical Review A</em>, 98(2), 022321. (Application of amplitude estimation to finance).</li>
<li>Montanaro, A. (2015). Quantum speedup of Monte Carlo methods. <em>Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences</em>, 471(2181), 20150301. (A key paper on the quadratic speedup).</li>
<li>Sanders, J., &amp; Kandrot, E. (2010). <em>CUDA by Example: An Introduction to General-Purpose GPU Programming</em>. Addison-Wesley Professional. (The beginner-friendly bible for CUDA).</li>
<li>Kirk, D. B., &amp; Hwu, W. W. (2016). <em>Programming Massively Parallel Processors: A Hands-on Approach</em> (3rd ed.). Morgan Kaufmann. (The advanced, comprehensive guide to GPU computing).</li>
<li>NVIDIA Corporation. (2024). <em>CUDA C++ Programming Guide</em>. <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/" target="_blank">https://docs.nvidia.com/cuda/cuda-c-programming-guide/</a> (The ultimate official reference).</li>
<li>Gropp, W., Lusk, E., &amp; Skjellum, A. (2014). <em>Using MPI: Portable Parallel Programming with the Message-Passing Interface</em> (3rd ed.). MIT Press. (The standard textbook for MPI).</li>
<li>Message Passing Interface Forum. (2015). <em>MPI: A Message-Passing Interface Standard Version 3.1</em>. <a href="https://www.mpi-forum.org/docs/mpi-3.1/mpi31-report.pdf" target="_blank">https://www.mpi-forum.org/docs/mpi-3.1/mpi31-report.pdf</a> (The formal specification).</li>
<li>Giles, M. B. (2015). Multilevel Monte Carlo methods. <em>Acta Numerica</em>, 24, 259-328. (Not directly quadrature, but a crucial modern MC method for validation).</li>
<li>Gander, W., &amp; Gautschi, W. (2000). Adaptive Quadrature—Revisited. <em>BIT Numerical Mathematics</em>, 40(1), 84–101. (An excellent review of robust adaptive methods).</li>
<li>Boyd, J. P. (2001). <em>Chebyshev and Fourier Spectral Methods</em> (2nd ed.). Dover. (The comprehensive work linking polynomials, FFT, and quadrature).</li>
</ol>
<p>© Gene Boo. This educational material is provided for informational purposes only. The author is neither a mathematician nor a physicist, but merely a risk practitioner, who is also a hobbyist coder and collector of numerical methods.</p>
</div>
</section></div>
</body>
</html>